{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework (deadline 25.11.2022 11:59:59)\n",
    "Write solutions for the homework exercises in this notebook. Once the work is done download the notebook file (`File > Download .ipynb`) rename it properly so it follows a template `HW1_<SURNAME>_<NAME>.ipynb` and send the file to me. **Please also attach all files you will be asked to produce in this homework**. My email address is as follows: \n",
    "\n",
    "* <m.biesaga@uw.edu.pl>\n",
    "\n",
    "Remember that you can contact me via email if you have any problems. Moreover, you can also visit me in the ISS on the fourth floor (room 415). Usually, I am there from 11ish but please let me know in advance if you are coming because I might be busy. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 (5 points)\n",
    "\n",
    "Connect to Twitter filtered-stream during a live event of your choice (the easiest would be a sporting event because they usually have a specific hashtag) and collect 100 tweets that are not in English and are not retweets. Among the tweets find a user with the most followers and download 300 tweets from their timeline. Save the tweets in a JSON line file, however, please write out only the following fields:\n",
    "\n",
    "* `retweet count`\n",
    "* `like count`\n",
    "* `quote count`\n",
    "* `reply count`\n",
    "* `id`\n",
    "* `context_annotations`\n",
    "* `lang`\n",
    "* `text`\n",
    "* `created_at`\n",
    "* `screen_name`\n",
    "\n",
    "**HINT**: The JSON line file should contain a list of dictionaries looking more or less like this:\n",
    "\n",
    "```python\n",
    "{\n",
    "  'retweet_count' : 2,\n",
    "  'like_count' : 15,\n",
    "  'quote_count' : 0,\n",
    "  'reply_count' : 21,\n",
    "  'id' : 13290821347,\n",
    "  'context_annotations' : \n",
    "    [\n",
    "      {\n",
    "        'entity': {'id': '854692455005921281',\n",
    "        'name': 'Science',\n",
    "        'description': 'Science'}\n",
    "      }\n",
    "    ]\n",
    "\n",
    "  'lang' : 'pl',\n",
    "  'text' : 'POLSKA GUROM! #Ortografiada'\n",
    "  'created_at' : '2022-11-11 11:11:11'\n",
    "  'username' : \"manolito_gafotas\"\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## If you are using Google Colab otherwise comment it\n",
    "!pip install twarc\n",
    "\n",
    "## Here paste your Bearer Token but please remove it before sending me the\n",
    "## solution\n",
    "BEARER_TOKEN = ''\n",
    "\n",
    "## Load module\n",
    "from twarc import Twarc2, expansions\n",
    "import json\n",
    "\n",
    "## Connect to Twitter API\n",
    "client = Twarc2(bearer_token=BEARER_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check the existing rules. \n",
    "existing_rules = client.get_stream_rules()\n",
    "print(existing_rules)\n",
    "\n",
    "## Check whether there are any rules.\n",
    "if 'data' in existing_rules and len(existing_rules['data']) > 0:\n",
    "\t## Return a list of rules ids\n",
    "    rule_ids = [ rule['id'] for rule in existing_rules['data'] ]\n",
    "\t## Remove existing rules\n",
    "    client.delete_stream_rule_ids(rule_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add new rules\n",
    "new_rules = [\n",
    "    { \"value\": \"Qatar2022 -lang:en -is:retweet\", \"tag\": \"Qatar2022 tweets not in English\" }\n",
    "]\n",
    "added_rules = client.add_stream_rules(rules=new_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create an empty list\n",
    "list_tweets = []\n",
    "\n",
    "## Connect to the filtered stream\n",
    "for count, result in enumerate(client.stream()):\n",
    "    ## Use expansions.flatten() to merge the data about a single\n",
    "    ## tweet\n",
    "    tweet = expansions.flatten(result)\n",
    "    ## Append the tweet to the list\n",
    "    list_tweets.append(tweet[0])\n",
    "    ## Break the look after reaching 102 tweets\n",
    "    if count > 98:\n",
    "        break\n",
    "\n",
    "## Delete the rules once you have collected the desired number of Tweets\n",
    "rule_ids = [rule['id'] for rule in added_rules['data']]\n",
    "client.delete_stream_rule_ids(rule_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define variables that will be populated with\n",
    "## the data about the most followed user\n",
    "the_most_followed_user = ''\n",
    "followers_count = 0\n",
    "\n",
    "## Iterate over all gathered tweets\n",
    "for item in list_tweets:\n",
    "    ## Check if the author has more followers than the previous one\n",
    "    if item['author']['public_metrics']['followers_count'] > followers_count:\n",
    "        ## If yes update the followers count\n",
    "        followers_count = item['author']['public_metrics']['followers_count']\n",
    "        ## If yes update the user name\n",
    "        the_most_followed_user = item['author']['username']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Connect to the timeline of Nature \n",
    "user_timeline = client.timeline(user=the_most_followed_user, max_results = 100)\n",
    "\n",
    "## Open connection to the file\n",
    "with open('timeline.jl', 'w') as file:\n",
    "  ## Twitter API will return the data paginated. It means\n",
    "  ## that it will be grouped in batches of 100 tweets.\n",
    "  ## Therefore, we first iterate over pages (batches of\n",
    "  ## tweets).\n",
    "  for count, page in enumerate(user_timeline):\n",
    "      ## Go over only first 3 batches of tweets\n",
    "      if count == 3:\n",
    "        break\n",
    "      ## Use expansions.flatten() to merge information \n",
    "      ## about tweets\n",
    "      result = expansions.flatten(page)\n",
    "      ## Iterate over this 100 tweets\n",
    "      for tweet in result:\n",
    "        ## Create an empty dictionary\n",
    "        output = {}\t\n",
    "        ## Assign various metrics to relevant keys in the output dictionary\n",
    "        output['retweet count'] = tweet['public_metrics']['retweet_count']\n",
    "        output['like count'] = tweet['public_metrics']['like_count']\n",
    "        output['quote count'] = tweet['public_metrics']['quote_count']\n",
    "        output['reply count'] = tweet['public_metrics']['reply_count']\n",
    "        output['id'] = tweet['id']\n",
    "        output['lang'] = tweet['lang']\n",
    "        output['text'] = tweet['text']\n",
    "        output['created_at'] = tweet['created_at']\n",
    "        output['screen_name'] = tweet['author']['username']\n",
    "        ## Write out the resutls\n",
    "        file.write(json.dumps(output) + '\\n')\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('ids')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2f6b087e3237558552441618582c1cfff7dc162872ae65c00a668bb71df085d3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
