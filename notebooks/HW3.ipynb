{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework (deadline 9.02.2024 23:59:59)\n",
    "\n",
    "Write solutions for the homework exercises in this notebook. Once the work is done download the notebook file (`File > Download .ipynb`) rename it properly so it follows a template `HW3_<SURNAME>_<NAME>.ipynb` and upload it to the [Google Classroom](https://classroom.google.com/c/NjI5NzI5ODQxNDIw/a/NjU3NTM4NjU0NDM4/details).\n",
    "\n",
    "Remember that you can contact me via email if you have any problems. Moreover, you can also visit me in the ISS on the fourth floor (room 415). Usually, I am there from 11ish but please let me know in advance if you are coming because I might be busy. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 (5 points)\n",
    "\n",
    "Compute sentiment for all the articles you gathered under HW2 (if you did not get 15 points you can use one of my solutions to create a correct JSON line file). Find the most positive article and the most negative one (we are interested only in the content of the article). Write out these two articles (with all the fields) in a file.\n",
    "\n",
    "You should use `VADER` to compute sentiment. For the given text, it returns the proportion of negative, positive, and neutral words as well as the compound score ranging from -1 (most negative) to 1 (most positive). However, the compound score for long texts tends to approximate maximum or minimum value regardless of the actual emotional intensity. To solve this issue please compute a corrected sentiment index that takes into account the length of the text:\n",
    "\n",
    "$$ sentiment = compound \\times (1 - neutral) $$\n",
    "\n",
    "The output JSON line file should look like the following (it should have just two lines).\n",
    "\n",
    "```python\n",
    "{ \n",
    "\t\"title\" : \"Why are Namedays better than Birthdays?\",\n",
    "    \"author\" : \"M. Biesaga\",\n",
    "    \"date\" : \"06.12.2019\",\n",
    "    \"lead\" : \"Scientists from one of the best Universities in the U.S. proved that the discussion about birthdays and namedays is finally over.\",\n",
    "    \"content\" : \"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer eget sapien nisi. In placerat nisl felis, vel porttitor odio aliquam quis. Nulla a facilisis arcu. Suspendisse potenti. Interdum et malesuada fames ac ante ipsum primis in faucibus. Duis et semper urna. Mauris sit amet enim ex. Integer eget ultricies enim, at tristique sem. In eu eros nisl. Nulla vitae pretium risus, quis vestibulum dui. Nullam vitae dapibus quam. Maecenas commodo dictum ex, id vestibulum ex volutpat interdum. Cras tempor diam non urna auctor, vitae dignissim tortor tristique. Nunc consectetur mauris non lorem luctus aliquam. Mauris vitae ligula orci.\",\n",
    "\t\"source\" : \"Journal of Scientific Science\",\n",
    "\t\"fb\" : { \n",
    "\t\t        \"likes\" : 112,\n",
    "\t            \"shares\" : 2,\n",
    "\t\t\t    \"comments\" : 43\n",
    "\t},\n",
    "\t\"length\" : 98,\n",
    "\t'sentiment' : .99 \n",
    "}\n",
    "{ \n",
    "\t\"title\" : \"Why automn is the worst part of the year?\",\n",
    "    \"author\" : \"M. Biesaga\",\n",
    "    \"date\" : \"06.12.2019\",\n",
    "    \"lead\" : \"Because it is bad, dark, and gloomy.\",\n",
    "    \"content\" : \"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer eget sapien nisi. In placerat nisl felis, vel porttitor odio aliquam quis. Nulla a facilisis arcu. Suspendisse potenti. Interdum et malesuada fames ac ante ipsum primis in faucibus. Duis et semper urna. Mauris sit amet enim ex. Integer eget ultricies enim, at tristique sem. In eu eros nisl. Nulla vitae pretium risus, quis vestibulum dui. Nullam vitae dapibus quam. Maecenas commodo dictum ex, id vestibulum ex volutpat interdum. Cras tempor diam non urna auctor, vitae dignissim tortor tristique. Nunc consectetur mauris non lorem luctus aliquam. Mauris vitae ligula orci.\",\n",
    "\t\"source\" : \"Applied Opinions on Weather\",\n",
    "\t\"fb\" : { \n",
    "\t\t        \"likes\" : 21,\n",
    "\t            \"shares\" : 3,\n",
    "\t\t\t    \"comments\" : 7\n",
    "\t},\n",
    "\t\"length\" : 98,\n",
    "\t'sentiment' : -.99 \n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import modules\n",
    "import json\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "## Download the VADER dictionary\n",
    "nltk.download(\"vader_lexicon\")\n",
    "\n",
    "## Initialize an instance of a sentiment analyzer\n",
    "vader = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initilize max and min sentiment dictionaries\n",
    "max_sentiment = {}\n",
    "min_sentiment = {}\n",
    "\n",
    "## Iterate over all the articles\n",
    "for line in open(\"HW2_Biesaga_Mikolaj.jsonl\", \"r\"):\n",
    "    ## Convert a string into a dictionary\n",
    "    temp_line = json.loads(line)\n",
    "    ## Compute sentiment\n",
    "    sentiment = vader.polarity_scores(temp_line[\"content\"])\n",
    "    ## Compute corrected sentiment\n",
    "    temp_line[\"sentiment\"] = sentiment[\"compound\"] * (1 - sentiment[\"neu\"])\n",
    "    ## Test whether a given article has a higher sentiment than any other\n",
    "    if temp_line[\"sentiment\"] > max_sentiment.get(\"sentiment\", 0):\n",
    "        max_sentiment = temp_line\n",
    "    ## Test whether a given article has a lower sentiment than any other\n",
    "    if temp_line[\"sentiment\"] < min_sentiment.get(\"sentiment\", 0):\n",
    "        min_sentiment = temp_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write out results to a file\n",
    "with open(\"HW3_1_Biesaga_Mikolaj.jsonl\", \"w\") as file:\n",
    "    file.write(json.dumps(min_sentiment) + \"\\n\")\n",
    "    file.write(json.dumps(max_sentiment) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 (5 points)\n",
    "\n",
    "Compute the average corrected sentiment of the sentence in the articles from Task 1. Find the article with the smallest variance of the corrected sentence sentiment and the one with the largest. Write them both in a JSON line file.\n",
    "\n",
    "The output of the JSON line file should look like the following\n",
    "\n",
    "```python\n",
    "{ \n",
    "\t\"title\" : \"Why are Namedays better than Birthdays?\",\n",
    "    \"author\" : \"M. Biesaga\",\n",
    "    \"date\" : \"06.12.2019\",\n",
    "    \"lead\" : \"Scientists from one of the best Universities in the U.S. proved that the discussion about birthdays and namedays is finally over.\",\n",
    "    \"content\" : \"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer eget sapien nisi. In placerat nisl felis, vel porttitor odio aliquam quis. Nulla a facilisis arcu. Suspendisse potenti. Interdum et malesuada fames ac ante ipsum primis in faucibus. Duis et semper urna. Mauris sit amet enim ex. Integer eget ultricies enim, at tristique sem. In eu eros nisl. Nulla vitae pretium risus, quis vestibulum dui. Nullam vitae dapibus quam. Maecenas commodo dictum ex, id vestibulum ex volutpat interdum. Cras tempor diam non urna auctor, vitae dignissim tortor tristique. Nunc consectetur mauris non lorem luctus aliquam. Mauris vitae ligula orci.\",\n",
    "\t\"source\" : \"Journal of Scientific Science\",\n",
    "\t\"fb\" : { \n",
    "\t\t        \"likes\" : 112,\n",
    "\t            \"shares\" : 2,\n",
    "\t\t\t    \"comments\" : 43\n",
    "\t},\n",
    "\t\"length\" : 98,\n",
    "\t'mena_sentiment' : 0,\n",
    "\t'var_sentiment' : .02 \n",
    "}\n",
    "{ \n",
    "\t\"title\" : \"Why automn is the worst part of the year?\",\n",
    "    \"author\" : \"M. Biesaga\",\n",
    "    \"date\" : \"06.12.2019\",\n",
    "    \"lead\" : \"Because it is bad, dark, and gloomy.\",\n",
    "    \"content\" : \"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer eget sapien nisi. In placerat nisl felis, vel porttitor odio aliquam quis. Nulla a facilisis arcu. Suspendisse potenti. Interdum et malesuada fames ac ante ipsum primis in faucibus. Duis et semper urna. Mauris sit amet enim ex. Integer eget ultricies enim, at tristique sem. In eu eros nisl. Nulla vitae pretium risus, quis vestibulum dui. Nullam vitae dapibus quam. Maecenas commodo dictum ex, id vestibulum ex volutpat interdum. Cras tempor diam non urna auctor, vitae dignissim tortor tristique. Nunc consectetur mauris non lorem luctus aliquam. Mauris vitae ligula orci.\",\n",
    "\t\"source\" : \"Applied Opinions on Weather\",\n",
    "\t\"fb\" : { \n",
    "\t\t        \"likes\" : 21,\n",
    "\t            \"shares\" : 3,\n",
    "\t\t\t    \"comments\" : 7\n",
    "\t},\n",
    "\t\"length\" : 98,\n",
    "\t'mean_sentiemtn' : 0,\n",
    "\t'var_sentiment' : .2 \n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "## Download the VADER dictionary\n",
    "nltk.download(\"vader_lexicon\")\n",
    "\n",
    "## Initialize an instance of a sentiment analyzer\n",
    "vader = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initilize min and max sentiment dictionaries\n",
    "maxvar_sentiment = {}\n",
    "minvar_sentiment = {}\n",
    "## Iterate over all the articles\n",
    "for line in open(\"HW2_Biesaga_Mikolaj.jsonl\", \"r\"):\n",
    "    ## Convert a string into a dictionary\n",
    "    temp_line = json.loads(line)\n",
    "    ## Compute sentiment for every sentence in the given text\n",
    "    sentiment = [\n",
    "        vader.polarity_scores(sent) for sent in sent_tokenize(temp_line[\"content\"])\n",
    "    ]\n",
    "    ## Compute a vector of corrected sentiment\n",
    "    sentiment = [sent[\"compound\"] * (1 - sent[\"neu\"]) for sent in sentiment]\n",
    "    ## Compute the average sentence sentiment\n",
    "    temp_line[\"sent_mean\"] = sum(sentiment) / len(sentiment)\n",
    "    ## Compute the variance\n",
    "    temp_line[\"sent_var\"] = sum(\n",
    "        (sent - temp_line[\"sent_mean\"]) ** 2 for sent in sentiment\n",
    "    ) / len(sentiment)\n",
    "    ## Test whether a given article has a higher variance than any other\n",
    "    if temp_line[\"sent_var\"] > maxvar_sentiment.get(\"sent_var\", 0):\n",
    "        maxvar_sentiment = temp_line\n",
    "    ## Test whether a given article has a lower variance than any other\n",
    "    if temp_line[\"sent_var\"] < minvar_sentiment.get(\"sent_var\", 1):\n",
    "        minvar_sentiment = temp_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minvar_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write out to a file\n",
    "with open(\"HW3_2_Biesaga_Mikolaj.jsonl\", \"w\") as file:\n",
    "    file.write(json.dumps(minvar_sentiment) + \"\\n\")\n",
    "    file.write(json.dumps(maxvar_sentiment) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 (10 points)\n",
    "\n",
    "Use the corpus which is available on the [Google Disk](https://classroom.google.com/c/NjI5NzI5ODQxNDIw/a/NjU3NTM4NjU0NDM4/details). It contains 1000 receipts for different meals. Your task is to find a receipt whose content is the most similar to the text of the [\"Weird Al\" Yankovich's](https://en.wikipedia.org/wiki/%22Weird_Al%22_Yankovic) song [\"Eat it\"](https://www.youtube.com/watch?v=ZcJjMnHoIBI). The text of the song you will find [here](https://classroom.google.com/c/NjI5NzI5ODQxNDIw/a/NjU3NTM4NjU0NDM4/details). Use cosine similarity as a measure of similarity (review [N6](https://github.com/MikoBie/ids/blob/main/notebooks/N6.ipynb))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import JSON module\n",
    "import json\n",
    "\n",
    "## Import defaultdict from collections\n",
    "from collections import defaultdict\n",
    "\n",
    "## Import NLTK module\n",
    "import nltk\n",
    "\n",
    "## Import stop words\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "## Import function to tokenize text\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "## Import function to lemmatize text\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "## Import gensim\n",
    "from gensim import corpora, models, similarities\n",
    "\n",
    "## Download stopwords list\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "\n",
    "## Assign the list of English stop words to stop_words\n",
    "stop_words = stopwords.words(\"english\")\n",
    "\n",
    "## Assign WordNetLemmatizer to lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read in the text of the song\n",
    "with open(\"eatIt.txt\", \"r\") as file:\n",
    "    song = file.read()\n",
    "\n",
    "## Read in the corpus\n",
    "with open(\"reciepts.jsonl\", \"r\") as file:\n",
    "    corpus = [json.loads(line) for line in file]\n",
    "\n",
    "## Extract the content from each reciept\n",
    "text_corpus = [line[\"content\"] for line in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenize every single text. Meanwhile lemmatize and remove tokens\n",
    "## that are shorter than 2 and are in stop_words list\n",
    "texts = [\n",
    "    [\n",
    "        lemmatizer.lemmatize(token)\n",
    "        for token in word_tokenize(doc.lower())\n",
    "        if lemmatizer.lemmatize(token) not in stop_words\n",
    "        and len(lemmatizer.lemmatize(token)) > 1\n",
    "    ]\n",
    "    for doc in text_corpus\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create an empty fancy mapping.\n",
    "frequency = defaultdict(int)\n",
    "\n",
    "## Count the number of words all texts\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove tokens that appear only once in the whole corpus\n",
    "processed_corpus = [[token for token in text if frequency[token] > 1] for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a new object called Dictionary\n",
    "dictionary = corpora.Dictionary(processed_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(text) for text in processed_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train the model. In other words, compute\n",
    "## the tf-idf statistic for each term.\n",
    "tfidf = models.TfidfModel(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute cosine similarity measure. Baisically, it is a cosine between\n",
    "## two vectors.\n",
    "index = similarities.SparseMatrixSimilarity(\n",
    "    tfidf[bow_corpus], num_features=len(dictionary)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_document = [lemmatizer.lemmatize(token).lower() for token in word_tokenize(song)]\n",
    "\n",
    "## Convert the query document to a Bag of words representation\n",
    "query_bow = dictionary.doc2bow(query_document)\n",
    "\n",
    "## Compute the similarities between a query_document and corpus documents\n",
    "sims = index[tfidf[query_bow]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print out the results in the descending order\n",
    "for document_number, score in sorted(enumerate(sims), key=lambda x: x[1], reverse=True):\n",
    "    print(corpus[document_number][\"content\"], score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 (10 points)\n",
    "\n",
    "Use the corpus which is available on the [Google Disk](https://classroom.google.com/c/NjI5NzI5ODQxNDIw/a/NjU3NTM4NjU0NDM4/details) and compute on it LDA. It contains 1000 receipts for different meals. Your task is to gather together similar receipts and name the categories (topics). In other words, we would like to reduce the data somehow using LDA -- know what kind of receipts we have here, for example, vegetarian meals, desserts, etc. Therefore, search for a model that best fits the data according to the coherence measure. You should check the parameter space between 20 and 40 topics. Name the topics to the best of your knowledge (based on the probability of words but also you can look into receipts assigned to the given topic).\n",
    "\n",
    "Your solution should include the code from [N7](https://github.com/MikoBie/ids/blob/main/notebooks/N7.ipynb) and a description of the interpretation of the results:\n",
    "\n",
    "1. the selected number of topics;\n",
    "2. the names of the topics.\n",
    "\n",
    "For example, I chose the solution with 21 topics because it had the lowest coherence measure.\n",
    "\n",
    "`Topic 1` -- Desserts\n",
    "\n",
    "`Topic 2` -- Vegetarian Food\n",
    "\n",
    "`Topic 3` -- Meals with almond milk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('ids')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "2f6b087e3237558552441618582c1cfff7dc162872ae65c00a668bb71df085d3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
