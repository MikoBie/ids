{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework (deadline 31.01.2025 23:59:59)\n",
    "\n",
    "Write solutions for the homework exercises in this notebook. Once the work is done download the notebook file (`File > Download .ipynb`) rename it properly so it follows a template `HW3_<SURNAME>_<NAME>.ipynb` and upload it to the [Google Classroom](https://classroom.google.com/c/NzIwNDg4NDAyMTA2/a/NzIwNDg4NDAyMTQ5/details).\n",
    "\n",
    "Remember that you can contact me via email if you have any problems. Moreover, you can also visit me in the ISS on the fourth floor (room 415). Usually, I am there from 11ish but please let me know in advance if you are coming because I might be busy. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 (5 points)\n",
    "\n",
    "Compute sentiment for all the articles you gathered under HW2 (if you did not get 15 points you can use one of my solutions to create a correct JSON line file). Find the most positive article and the most negative one (we are interested only in the content of the article). Write out these two articles (with all the fields) in a file.\n",
    "\n",
    "You should use `VADER` to compute sentiment. For the given text, it returns the proportion of negative, positive, and neutral words as well as the compound score ranging from -1 (most negative) to 1 (most positive). However, the compound score for long texts tends to approximate maximum or minimum value regardless of the actual emotional intensity. To solve this issue please compute a corrected sentiment index that takes into account the length of the text:\n",
    "\n",
    "$$ sentiment = compound \\times (1 - neutral) $$\n",
    "\n",
    "The JSON lines file should look more or less like the following.\n",
    "```python\n",
    "{\"title\":\"Why are Namedays better than Birthdays?\",\"author\":\"M. Biesaga\",\"date\":\"06.12.2019\",\"lead\":\"Scientists from one of the best Universities in the U.S. proved that the discussion about birthdays and namedays is finally over.\",\"content\":\"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer eget sapien nisi. In placerat nisl felis, vel porttitor odio aliquam quis. Nulla a facilisis arcu. Suspendisse potenti. Interdum et malesuada fames ac ante ipsum primis in faucibus. Duis et semper urna. Mauris sit amet enim ex. Integer eget ultricies enim, at tristique sem. In eu eros nisl. Nulla vitae pretium risus, quis vestibulum dui. Nullam vitae dapibus quam. Maecenas commodo dictum ex, id vestibulum ex volutpat interdum. Cras tempor diam non urna auctor, vitae dignissim tortor tristique. Nunc consectetur mauris non lorem luctus aliquam. Mauris vitae ligula orci.\",\"source\":\"Journal of Scientific Science\",\"likes\":112,\"shares\":2,\"comments\":43,\"length\":98,\"sentiment\":.99}\n",
    "{\"title\":\"Why automn is the worst part of the year?\",\"author\":\"M. Biesaga\",\"date\":\"06.12.2019\",\"lead\":\"Because it is bad, dark, and gloomy.\",\"content\":\"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer eget sapien nisi. In placerat nisl felis, vel porttitor odio aliquam quis. Nulla a facilisis arcu. Suspendisse potenti. Interdum et malesuada fames ac ante ipsum primis in faucibus. Duis et semper urna. Mauris sit amet enim ex. Integer eget ultricies enim, at tristique sem. In eu eros nisl. Nulla vitae pretium risus, quis vestibulum dui. Nullam vitae dapibus quam. Maecenas commodo dictum ex, id vestibulum ex volutpat interdum. Cras tempor diam non urna auctor, vitae dignissim tortor tristique. Nunc consectetur mauris non lorem luctus aliquam. Mauris vitae ligula orci.\",\"source\":\"Applied Opinions on Weather\",\"likes\":21,\"shares\":3,\"comments\":7,\"length\":98,'sentiment':-.99}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import modules\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "## Download the VADER dictionary\n",
    "nltk.download(\"vader_lexicon\")\n",
    "\n",
    "## Initialize an instance of a sentiment analyzer\n",
    "vader = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 (5 points)\n",
    "\n",
    "Compute the average corrected sentiment of the sentence in the articles from Task 1. Find the article with the smallest variance of the corrected sentence sentiment and the one with the largest. Write them both in a JSON line file.\n",
    "\n",
    "The output of the JSON line file should look like the following.\n",
    "\n",
    "```python\n",
    "{ \"title\":\"Why are Namedays better than Birthdays?\",\"author\":\"M. Biesaga\",\"date\":\"06.12.2019\",\"lead\":\"Scientists from one of the best Universities in the U.S. proved that the discussion about birthdays and namedays is finally over.\",\"content\":\"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer eget sapien nisi. In placerat nisl felis, vel porttitor odio aliquam quis. Nulla a facilisis arcu. Suspendisse potenti. Interdum et malesuada fames ac ante ipsum primis in faucibus. Duis et semper urna. Mauris sit amet enim ex. Integer eget ultricies enim, at tristique sem. In eu eros nisl. Nulla vitae pretium risus, quis vestibulum dui. Nullam vitae dapibus quam. Maecenas commodo dictum ex, id vestibulum ex volutpat interdum. Cras tempor diam non urna auctor, vitae dignissim tortor tristique. Nunc consectetur mauris non lorem luctus aliquam. Mauris vitae ligula orci.\",\"source\":\"Journal of Scientific Science\",\"likes\":112,\"shares\":2,\"comments\":43,\"length\":98,'mena_sentiment':0,'var_sentiment':.02}\n",
    "{\"title\":\"Why automn is the worst part of the year?\",\"author\":\"M. Biesaga\",\"date\":\"06.12.2019\",\"lead\":\"Because it is bad, dark, and gloomy.\",\"content\":\"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer eget sapien nisi. In placerat nisl felis, vel porttitor odio aliquam quis. Nulla a facilisis arcu. Suspendisse potenti. Interdum et malesuada fames ac ante ipsum primis in faucibus. Duis et semper urna. Mauris sit amet enim ex. Integer eget ultricies enim, at tristique sem. In eu eros nisl. Nulla vitae pretium risus, quis vestibulum dui. Nullam vitae dapibus quam. Maecenas commodo dictum ex, id vestibulum ex volutpat interdum. Cras tempor diam non urna auctor, vitae dignissim tortor tristique. Nunc consectetur mauris non lorem luctus aliquam. Mauris vitae ligula orci.\",\"source\":\"Applied Opinions on Weather\",\"likes\":21,\"shares\":3,\"comments\":7,\"length\":98,'mean_sentiemtn':0,'var_sentiment':.2}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "## Download the VADER dictionary\n",
    "nltk.download(\"vader_lexicon\")\n",
    "\n",
    "## Initialize an instance of a sentiment analyzer\n",
    "vader = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 (10 points)\n",
    "\n",
    "Use the corpus which is available on the [Google Disk](https://classroom.google.com/c/NjI5NzI5ODQxNDIw/a/NjU3NTM4NjU0NDM4/details). It contains 1000 recipes for different meals. Your task is to find a recipe which content is the most similar to the text of the [\"Weird Al\" Yankovich's](https://en.wikipedia.org/wiki/%22Weird_Al%22_Yankovic) song [\"Eat it\"](https://www.youtube.com/watch?v=ZcJjMnHoIBI). The text of the song you will find [here](https://classroom.google.com/c/NzIwNDg4NDAyMTA2/a/NzIwNDg4NDAyMTQ5/details). Use cosine similarity as a measure of similarity (review [N6](https://github.com/MikoBie/ids/blob/main/notebooks/N6.ipynb))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import JSON module\n",
    "\n",
    "## Import defaultdict from collections\n",
    "\n",
    "## Import NLTK module\n",
    "import nltk\n",
    "\n",
    "## Import stop words\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "## Import function to tokenize text\n",
    "\n",
    "## Import function to lemmatize text\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "## Import gensim\n",
    "\n",
    "## Download stopwords list\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "\n",
    "## Assign the list of English stop words to stop_words\n",
    "stop_words = stopwords.words(\"english\")\n",
    "\n",
    "## Assign WordNetLemmatizer to lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 (10 points)\n",
    "\n",
    "Use the corpus which is available on the [Google Disk](https://classroom.google.com/c/NzIwNDg4NDAyMTA2/a/NzIwNDg4NDAyMTQ5/details) and compute on it LDA. It contains 1000 recipes for different meals. Your task is to gather together similar recipes and name the categories (topics). In other words, we would like to reduce the data somehow using LDA -- know what kind of recipes we have here, for example, vegetarian meals, desserts, etc. Therefore, search for a model that best fits the data according to the coherence measure. You should check the parameter space between 20 and 40 topics. Name the topics to the best of your knowledge (based on the probability of words but also you can look into recipes assigned to the given topic).\n",
    "\n",
    "Your solution should include the code from [N7](https://github.com/MikoBie/ids/blob/main/notebooks/N7.ipynb) and a description of the interpretation of the results:\n",
    "\n",
    "1. the selected number of topics;\n",
    "2. the names of the topics.\n",
    "\n",
    "For example, I chose the solution with 21 topics because it had the lowest coherence measure.\n",
    "\n",
    "`Topic 1` -- Desserts\n",
    "\n",
    "`Topic 2` -- Vegetarian Food\n",
    "\n",
    "`Topic 3` -- Meals with almond milk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ids",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
