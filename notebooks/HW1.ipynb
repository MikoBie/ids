{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework (deadline 18.11.2022 13:44:59)\n",
    "Write solutions for the homework exercises in this notebook. Once the work is done download the notebook file (`File > Download .ipynb`) rename it properly so it follows a template `HW1_<SURNAME>_<NAME>.ipynb` and send the file to me. My email address is as follows: \n",
    "\n",
    "* <m.biesaga@uw.edu.pl>\n",
    "\n",
    "Remember that you can contact me via email if you have any problems. Moreover, you can also visit me in the ISS on the fourth floor (room 415). Usually, I am there from 11ish but please let me know in advance if you are coming because I might be busy. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 (5 points)\n",
    "\n",
    "Read about the `pageviews` method (`prop=pageviews`) in the `query endpoint` ([docpage](https://en.wikipedia.org/w/api.php?action=help&modules=query%2Bpageviews)). Use this method to extract page views data for the pages from the exercise we did during the class (if you want you can sample 10 new pages with the `list=random` method) for the last 60 days. The results will be broken down by single days, so you have to aggregate the results (sum) so they give the total page views count for the entire period of 60 days. Remember that to select pages by page ids you pass `pageids=<id 1>|<id 2>|...|<id n>`. We did a very similar thing when we extracted article content through the `cirrusdoc` method in the Wikipedia API. Your final output should be a `dict` object that maps page ids to pageviews (total number of pageviews over 60 days). It should look something like this:\n",
    "\n",
    "```python\n",
    "results = {\n",
    "    # page_id: pageviews\n",
    "    153253: 10204,\n",
    "    423423: 101,\n",
    "    11012:  12,\n",
    "    42435:  546,\n",
    "    # and so on\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import module requests\n",
    "import requests\n",
    "\n",
    "## Some page ids\n",
    "page_ids = [\n",
    "    19969580,\n",
    "    39982842,\n",
    "    25699035,\n",
    "    52642931,\n",
    "    53055349,\n",
    "    24133565,\n",
    "    1164662,\n",
    "    40656459,\n",
    "    12533026,\n",
    "    47110862\n",
    "]\n",
    "\n",
    "## API URL\n",
    "BASE_URL = 'https://en.wikipedia.org/w/api.php'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the payload\n",
    "params = {\n",
    "    'action': 'query',\n",
    "    'prop': 'pageviews',\n",
    "    'pageids': '|'.join(str(pid) for pid in page_ids),\n",
    "    'pvidays': 60,\n",
    "    'format': 'json'\n",
    "}\n",
    "## Send the request\n",
    "response = requests.get(BASE_URL, params=params)\n",
    "## Extract a dictionary with pages\n",
    "data = response.json()['query']['pages']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## My friends solution with the filter function and get method\n",
    "PVS = { v['title']: sum(filter(None, v.get('pageviews', {}).values())) for v in data.values() }\n",
    "## My solutions with two list comprehensions\n",
    "PVM = { v['title'] : sum([ item for item in v['pageviews'].values() if item is not None ]) for v in data.values() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a new dictionary\n",
    "PV = {}\n",
    "\n",
    "## Iterate over all dictionaries\n",
    "for item in data.values():\n",
    "    ## Create a list of values filtering out None\n",
    "    temp_list = filter(None, item.get('pageviews', {}).values())\n",
    "    ## Add a new key - value pair\n",
    "    PV[item['title']] = sum(temp_list)\n",
    "\n",
    "PV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PVS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taks 2 (5 + 2 points)\n",
    "In this task, you can score either 5 points or 7. The only difference is in the pages you will download. To score 5 points you just need to download the content of 20 random pages from Wikipedia (please review the [N1](https://github.com/MikoBie/ids/blob/main/notebooks/N1.ipynb) in which we downloaded the content of 10 random pages). To have the chance to score 7 points you need to download the content of 10 pages that have in the title `Olivia` and 10 pages that have `Noah` (those are the most popular names in the UK in 2021). \n",
    "\n",
    "**Hint for 7 points**: you might find this [`pssearch`](https://en.wikipedia.org/w/api.php?action=help&modules=query%2Bprefixsearch) method interesting.\n",
    "\n",
    "When you have the content of these 20 articles for each one of them compute a distribution of the following possessive pronouns: `her`, `his`, and `their`. In other words, you should end up with the list looking more or less like this one (but has 20 elements):\n",
    "\n",
    "```python\n",
    "[{'his': 79, 'her': 212, 'their': 14},\n",
    " {'his': 36, 'her': 147, 'their': 20},\n",
    " {'his': 17, 'her': 80, 'their': 6},\n",
    " {'his': 8, 'her': 80, 'their': 9},\n",
    " {'his': 14, 'her': 66, 'their': 2},\n",
    " {'his': 12, 'her': 188, 'their': 16},\n",
    " {'his': 3, 'her': 156, 'their': 13},\n",
    " {'his': 33, 'her': 126, 'their': 33},\n",
    " {'his': 10, 'her': 113, 'their': 8},\n",
    " {'his': 21, 'her': 4, 'their': 33}]\n",
    "```\n",
    "**Hint**: Remember that sometimes in articles pronouns starts with the capital letter (ignore the cases like `hers` and `theirs`). Moreover, review the [notebook number 5](https://github.com/MikoBie/ppss/blob/main/notebooks/N5.ipynb) about the lists from May 19th, 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import module requests\n",
    "import requests as rq\n",
    "URL = 'https://en.wikipedia.org/w/api.php'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_ids(keywords, pslimit = 10, form = 'json'):\n",
    "    \"\"\"\n",
    "    Searches for page ids and titles based on given words. It returns a list\n",
    "    of page ids.\n",
    "\n",
    "\tArgs:\n",
    "\t\tkeywords (str): a string with keywords to search. Keywords should be\n",
    "\t\tseparated by commas.\n",
    "        pslimit (int): number of ids that should be returned. By default, it\n",
    "        takes the value of 10.\n",
    "        format (str): a string that indicates the format in which the response\n",
    "        should be returned. By default, it takes the value of json.\n",
    "        \n",
    "    Returns:\n",
    "        (list) : a list with page ids.\n",
    "    \"\"\"\n",
    "    BASE_URL = 'https://en.wikipedia.org/w/api.php'\n",
    "    list_keywords = keywords.split(',')\n",
    "    output = []\n",
    "    for keyword in list_keywords:\n",
    "        keyword = keyword.strip()\n",
    "        payload = { 'action' : 'query',\n",
    "                    'list' : 'prefixsearch',\n",
    "                    'pssearch' : keyword,\n",
    "                    'format' : form,\n",
    "                    'pslimit' : pslimit\n",
    "\t\t}\n",
    "        response = rq.get(BASE_URL, payload)\n",
    "        data = response.json()\n",
    "        output.extend( [ item['pageid'] for item in data['query']['prefixsearch'] ] )\n",
    "    return output\n",
    "        \n",
    "def count_pronouns(s, l = ['her', 'his', 'their']):\n",
    "    \"\"\"\n",
    "    Takes a string and counts possesive pronouns that have been passed in a list. \n",
    "    It returns a dictionary with a frequency of of a given pronouns.\n",
    "\n",
    "    Args:\n",
    "        s (str): a string\n",
    "        l (list): a list of words that should be counted. By default it is a list of \n",
    "        the following possesive pronouns.\n",
    "    \n",
    "    Returns:\n",
    "        (dict): a dictionary with frequency of words listed in l.\n",
    "    \"\"\"\n",
    "    s = s.lower().split()\n",
    "    return { item : s.count(item) for item in l }\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the list of page ids for names Olivia and Noah\n",
    "page_ids_string = page_ids(keywords = 'Olivia, Noah', pslimit=10)\n",
    "## Convert the list into a string suitable for passing as an\n",
    "## argument for Wikipedia API call\n",
    "page_ids_string = '|'.join(str(item) for item in page_ids_string)\n",
    "## Print out the string\n",
    "page_ids_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the payload\n",
    "payload = { 'action' : 'query',\n",
    "            'prop' : 'cirrusdoc',\n",
    "            'pageids' : page_ids_string,\n",
    "            'format' : 'json'\n",
    "}\n",
    "## Send the request\n",
    "response = rq.get(URL, payload)\n",
    "## Extract the dictionary from the response\n",
    "data = response.json()\n",
    "## Extract a dictionary with the content of pages\n",
    "pages = data['query']['pages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a dictionary in which keys denotate the title of the page and values the \n",
    "## its content\n",
    "articles = { p['title'] : p['cirrusdoc'][0]['source']['text'] for p in pages.values() }\n",
    "\n",
    "## Create a list of dictionaries with frequencies of possesive pronouns\n",
    "pronouns_list = [ count_pronouns(item) for item in articles.values() ]\n",
    "\n",
    "## Create a dictionary with frequencies for each page\n",
    "pronouns_dict = { key : count_pronouns(value) for key, value in articles.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print out the list\n",
    "pronouns_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pring out the dictionary\n",
    "pronouns_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('ids')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "2f6b087e3237558552441618582c1cfff7dc162872ae65c00a668bb71df085d3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
