{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MediaCloud\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"png/mediacloud.png\" /></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is MediaCloud?\n",
    "\n",
    "[Media Cloud](https://www.mediacloud.org) is an open-source media research project, enabling the study of news and information flow globally. The project is administered as a consortium collaboration between the Media Ecosystems Analysis Group, the University of Massachusetts Amherst, and Northeastern University. It was originally incubated at Harvard University and the Massachusetts Institute of Technology.\n",
    "\n",
    "In simple terms, it is a platform that **collects** and analyzes news articles from various sources, allowing researchers to study media trends, narratives, and the spread of information over time. It provides tools for data collection, text analysis, and visualization, making it a valuable resource for journalists, academics, and anyone interested in understanding the dynamics of media coverage.\n",
    "\n",
    "However, Media Cloud is **not** a repository of news articles. Instead, it provides **metadata** and **text analysis** of articles collected from various sources. The actual articles themselves are not stored within Media Cloud; rather, the platform focuses on analyzing the content and patterns within the media landscape.\n",
    "\n",
    "## Media Cloud API\n",
    "\n",
    "You can interact with Media Cloud either through their web interface or programmatically using their API. The API allows you to query the Media Cloud database, retrieve metadata about articles, and perform various analyses. In general, there are two main components of the Media Cloud API:\n",
    "\n",
    "1. Media Cloud API: Search Against Media Cloud's new Online News Archive, with access to 200 million+ stories and growing every day. Search against hundreds of sources and collections we have developed.\n",
    "2. Wayback Machine: Search against the Wayback Machineâ€™s database through an API we developed to be able to search against the large number of sources and collections we have developed. Search is against the title text.\n",
    "\n",
    "In this notebook, we will focus on the Media Cloud API. However, first, we will start by briefly exploring the web interface.\n",
    "\n",
    "## Setting Up an Account\n",
    "\n",
    "To use the Media Cloud API, you need to set up an account and obtain an API key. Follow these steps:\n",
    "\n",
    "1. Visit the [Media Cloud website](https://www.mediacloud.org) and sign up for an account.\n",
    "2. Once you are logged in, navigate to your profile settings and press Request API Access (see the image below).\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"png/mediacloud_api.png\" /></div>\n",
    "\n",
    "3. After requesting access, you will receive an email with a link to confirm your API access. Click the link to confirm.\n",
    "4. Once confirmed, you can find your API key in your profile settings (see the image below).\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"png/mediacloud_api_key.png\" /></div>\n",
    "\n",
    "In the picture above, I hid some information because those are the credentials (API Key) you will use to tell Media Cloud who you are. You should never share them with anyone, even your spouse or a firefighter! That is because they serve to identify you. If someone maluses them, it will be on you. \n",
    "\n",
    "## Storing your credentials\n",
    "\n",
    "There are multiple ways to store your credentials and passwords safely. We don't want them to be corrupted, right? However, it is one thing to store them [safely](https://youtu.be/MnjQV--o1-0?si=hIlgl9sCyt4JhVUd) and the other to have [strong passwords](https://youtu.be/mQ36sUT77qI?si=hxRw4O4UxKM_WUPy). We all know that we should use strong passwords, but do we really know why? The picture below shows how fast one can crack your password depending on its complexity.\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"png/password_table_2023.jpg\"/></div>\n",
    "\n",
    "Anyhow, the lesson we should take from the graph above is twofold:\n",
    "\n",
    "1. Use strong passwords.\n",
    "2. Use password managers to propose strong passwords and store them.\n",
    "\n",
    "If, for any reason, you are still reluctant to trust password managers, at least create complex passwords by mixing nonsense words (it is the only place where making spelling errors helps) and special characters, for example:\n",
    "\n",
    ">`$eating#keyborads-1ncreases_staminA`\n",
    "\n",
    "In our case, we have already generated passwords and credentials that look pretty strong. How are we going to store them?\n",
    "\n",
    "### Environmental variables\n",
    "\n",
    "As you probably rightly suspect, in our case, we will need our credentials to connect to the API. We don't really want to store them in the notebook because we want to be able to share the notebook (you want to share it with me, and I want to share it with you). We don't want to copy and paste them every time we want to use the notebook, cause it would be very inefficient. Also, it will be quite easy to forget about it. What are we going to do then?\n",
    "\n",
    "We are going to use something called environmental variables. In other words, we are going to define some variables either on our computer or in the Colab that will be stored there. In the Notebook, we will just retrieve them by their names. For this purpose, we need to press the key on the left-hand side tab. We need to define the 5 variables:\n",
    "\n",
    "* `API_KEY` -- this is our API Key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load module\n",
    "from google.colab import userdata\n",
    "\n",
    "## Retrive our environmental variables and assing them to names.\n",
    "API_KEY = userdata.get(\"API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Media Cloud Module\n",
    "\n",
    "So, when we finally do have our credentials in the Notebook, what are we going to do next? We need to pass it somehow through a request to the Media Cloud API, right? Intuitively, we would do it through a payload and `request` module, right? Yes, this is a good intuition, but fortunately, we don't really have to do it this way. That is because most social media have so-called wrappers. Those are modules that allow us to connect to the API and send requests. We could still do it through our web browser, but the URL would be much more complicated.\n",
    "\n",
    "That is why, in the case of Media Cloud, we will use the module. It will serve us to connect and get data from Media Cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Install mediacloud module\n",
    "!pip install mediacloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import modules\n",
    "import mediacloud.api\n",
    "import datetime as dt\n",
    "\n",
    "## Connect to Media Cloud\n",
    "search_api = mediacloud.api.SearchApi(API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a bit underwhelming because nothing was printed. To check whether everything worked well, we can just execute the following. It will return information about our user account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print user's information\n",
    "search_api.user_profile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important information from this dictionary is about the quota. This tells us how many requests we made and how much we have left this week. \n",
    "\n",
    "## Counts\n",
    "\n",
    "Anyway, once we have established a connection to Media Cloud, let's now try to get some data. We will start with searching for the number of articles that contained the name of sensational [Caitlin Clark](https://en.wikipedia.org/wiki/Caitlin_Clark) -- number 1 in the 2024 WNBA Draft. There are multiple ways in which we can construct the query string. They are described in the [documentation](https://www.mediacloud.org/documentation/query-guide). Here, we will use the simplest -- we will just type her name and surname in quotation marks, which will guarantee that we hit only articles with the `\"Caitlin Clark\"` string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The method search_api.story_count() returns a dictionary with the number of stories that matched the query.\n",
    "clark_all = search_api.story_count(\n",
    "    query=\"'Caitlin Clark'\", start_date=dt.date(2025, 1, 1), end_date=dt.date.today()\n",
    ")\n",
    "clark_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is more, we can specify a resource to search in. For example, the New York Times. However, to do that, we need to know its media ID. We can find it through the Media Cloud web interface or through a different API endpoint. However, let's, for now, assume we will use the web interface for this. At the end of the notebook, we will come back to investigating available sources through the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We can add a list of sources.\n",
    "clark_all_ny = search_api.story_count(\n",
    "    \"'Caitlin Clark'\",\n",
    "    start_date=dt.date(2025, 1, 1),\n",
    "    end_date=dt.date.today(),\n",
    "    source_ids=[1],\n",
    ")\n",
    "clark_all_ny"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, most of the time, the count sum for the whole search is kind of useless. Probably, it would be much better to aggregate daily. Say no more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data aggregated daily.\n",
    "clark_ny = search_api.story_count_over_time(\n",
    "    \"'Caitlin Clark'\",\n",
    "    start_date=dt.date(2025, 1, 1),\n",
    "    end_date=dt.date.today(),\n",
    "    source_ids=[1],\n",
    ")\n",
    "clark_ny"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Find a day this year when Caitlin Clark was mentioned the most in The  New York Times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Simple solution\n",
    "\n",
    "## The dictionary against we make the first comparison\n",
    "output = {\"count\": 0}\n",
    "\n",
    "## Iterate over all dictionaries\n",
    "for item in clark_ny:\n",
    "    ## Test whether the currect item is bigger than the last biggest value\n",
    "    if item[\"count\"] > output[\"count\"]:\n",
    "        ## Overwrite the biggest value object\n",
    "        output = item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## One line solution\n",
    "output = max(clark_ny, key=lambda x: x[\"count\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can also search through a collection of sources. Media Cloud has a set of collections of sources that are specific to a given country. Similarly to the source id, we need to know the id of a given collection. There are two ways of learning it, either through the web interface or through the API. Again, we will talk about the API pathway later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example of getting data from a collection of sources.\n",
    "clark_collection = search_api.story_count_over_time(\n",
    "    \"'Caitlin Clark'\",\n",
    "    start_date=dt.date(2025, 1, 1),\n",
    "    end_date=dt.date.today(),\n",
    "    collection_ids=[34412234],\n",
    ")\n",
    "clark_collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now see how it changed over time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import functions for plotting.\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "## Prepare data.\n",
    "x = [item[\"date\"] for item in clark_ny]\n",
    "y_count = [item[\"count\"] for item in clark_ny]\n",
    "y_ratio = [item[\"ratio\"] for item in clark_ny]\n",
    "\n",
    "## Define plot.\n",
    "fig, axs = plt.subplots(figsize=(9, 5), nrows=2)\n",
    "axs[0].plot(x, y_count, color=\"#C8102E\")\n",
    "axs[0].set_ylabel(\"Count\")\n",
    "axs[1].plot(x, y_ratio, color=\"#041E42\")\n",
    "axs[1].set_ylabel(\"Ratio\")\n",
    "## Edit xticks so they show the names of the months only.\n",
    "for ax in axs:\n",
    "    ax.xaxis.set_major_locator(mdates.MonthLocator())\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%b\"))\n",
    "## Add title.\n",
    "plt.suptitle(\n",
    "    f\"Dynamics of the number articles on Caitlin Clark in NY Times in 2025\\n(N = {clark_all_ny['relevant']})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can search for the sources with the most articles with a given phrase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clark_sources = search_api.sources(\n",
    "    \"'Caitlin Clark'\", start_date=dt.date(2025, 1, 1), end_date=dt.date.today()\n",
    ")\n",
    "clark_sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Words\n",
    "\n",
    "The most analytical thing we can extract from the Media Cloud is the most popular words that were used in the articles on the given topic. Unfortunately, this is only estimated on the sample of a maximum $5000$ articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get 100 top words from the articles.\n",
    "clark_words = search_api.words(\n",
    "    \"'Caitlin Clark'\",\n",
    "    start_date=dt.date(2025, 1, 1),\n",
    "    end_date=dt.date.today(),\n",
    "    collection_ids=[34412234],\n",
    ")\n",
    "clark_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Create a dictionary in which keys will represent terms and values will be term ratios. Filter out words: `\"Caitlin\"` and `\"Clark\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE\n",
    "## Empty dictioanry to store the data\n",
    "words = {}\n",
    "\n",
    "## Iterate over all elements of the list\n",
    "for item in clark_words:\n",
    "    ## Test whether term is not caitlin and not clark <= Gemini was right!\n",
    "    if \"caitlin\" not in item[\"term\"] and \"clark\" not in item[\"term\"]:\n",
    "        ## Create key value pairing\n",
    "        words[item[\"term\"]] = item[\"term_count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now draw a word cloud. This is one of the most useless graphs (I think it is even worse than a pie chart). However, sometimes it looks nice. And people do use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import modules\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Define the plot.\n",
    "wordcloud = WordCloud(\n",
    "    width=800, height=400, background_color=\"white\"\n",
    ").generate_from_frequencies(words)\n",
    "\n",
    "## Define the size of the plot.\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Caitlin Clark Word Cloud\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lists\n",
    "\n",
    "Finally, what is probably the most interesting and useful are lists of metadata. Using Media Cloud, you can find metadata about the articles on a given topic. In simple terms, you can get the titles of all articles on `\"Caitlin Clark\"` from 2025. The simplest way of getting the list of metadata is by using the `search_api.story_sample()` method. However, as the name suggests, it can only provide us with a sample of metadata. The maximum limit is $1250$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's get the list of metadata\n",
    "clark_meta_sample = search_api.story_sample(\n",
    "    \"'Caitlin Clark'\",\n",
    "    start_date=dt.date(2025, 1, 1),\n",
    "    end_date=dt.date.today(),\n",
    "    limit=500,\n",
    "    collection_ids=[34412234],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's see the example of the result\n",
    "clark_meta_sample[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the dates are not strings. If we want to later write it out to JSON lines files. We will get the `\"TypeError\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "## Serialize a datetime object\n",
    "json.dumps(clark_meta_sample[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fix it, we need to convert all datetime objects into strings first. How to do it? It is fairly simple. Each datatime object has a method defined that allows for doing exactly what it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define a datetime object\n",
    "dt_example = dt.date.today()\n",
    "\n",
    "## Convert a datetime object into a string of pattern\n",
    "## \"day-month-year\".\n",
    "dt_str = dt_example.strftime(\"%d-%m-%Y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, to be able to write out to the file datetime objects, we need to first convert each one of them to a string. Only then will the `json.dumps()` work as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Find the ten most popular sources in a sample of 500 and 1250 articles. What is the proportion of the articles in the sample come from these sources? Is the most popular source in the 500 sample also the most popular in the 1250 sample?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get a list of 500 links\n",
    "clark_500_sample = search_api.story_sample(\n",
    "    \"'Caitlin Clark'\",\n",
    "    start_date=dt.date(2025, 1, 1),\n",
    "    end_date=dt.date.today(),\n",
    "    limit=500,\n",
    "    collection_ids=[34412234],\n",
    ")\n",
    "\n",
    "## Get a list of 1250 links\n",
    "clark_1250_sample = search_api.story_sample(\n",
    "    \"'Caitlin Clark'\",\n",
    "    start_date=dt.date(2025, 1, 1),\n",
    "    end_date=dt.date.today(),\n",
    "    limit=1250,\n",
    "    collection_ids=[34412234],\n",
    ")\n",
    "\n",
    "## Compute the proportion for 500 links\n",
    "dct_500 = {}\n",
    "for item in clark_500_sample:\n",
    "    if item[\"media_name\"] not in dct_500:\n",
    "        dct_500[item[\"media_name\"]] = 1 / 500\n",
    "    else:\n",
    "        dct_500[item[\"media_name\"]] += 1 / 500\n",
    "\n",
    "## Compute the proportion for 1250 links\n",
    "dct_1250 = {}\n",
    "for item in clark_1250_sample:\n",
    "    if item[\"media_name\"] not in dct_1250:\n",
    "        dct_1250[item[\"media_name\"]] = 1 / 1250\n",
    "    else:\n",
    "        dct_1250[item[\"media_name\"]] += 1 / 1250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(dct_500.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(dct_1250.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also a method that returns metadata for all the articles. However, it takes much longer. It gets 1000 articles every 30 seconds. That is because the API will send us all the articles in batches of 1000. Moreover, there is a rate limit at this endpoint. It means that we can send only 2 requests per minute to it. But how to do it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import module to control time and for a progress bar\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "## Output list\n",
    "clark_meta_all = []\n",
    "\n",
    "## The id of the next batch of data\n",
    "pagination_token = None\n",
    "\n",
    "## Control variable\n",
    "more_stories = True\n",
    "\n",
    "## How much data is there to collect\n",
    "clark_all = search_api.story_count(\n",
    "    \"'Caitlin Clark'\",\n",
    "    start_date=dt.date(2025, 1, 1),\n",
    "    end_date=dt.date.today(),\n",
    "    collection_ids=[34412234],\n",
    ")\n",
    "\n",
    "## Add progress bar\n",
    "with tqdm(total=clark_all[\"relevant\"]) as pbar:\n",
    "    while more_stories:\n",
    "        ## Wait to make another request\n",
    "        time.sleep(31)\n",
    "\n",
    "        ## search_api.story_list() returns a two elements long tuple. The\n",
    "        ## first element is a list of dictionaries. The second element is\n",
    "        ## the id of the next batch of data\n",
    "        page, pagination_token = search_api.story_list(\n",
    "            \"'Caitlin Clark'\",\n",
    "            start_date=dt.date(2025, 1, 1),\n",
    "            end_date=dt.date.today(),\n",
    "            collection_ids=[34412234],\n",
    "            pagination_token=pagination_token,\n",
    "        )\n",
    "\n",
    "        ## Update the list\n",
    "        clark_meta_all += page\n",
    "\n",
    "        ## Update the progress bar\n",
    "        pbar.update(1000)\n",
    "\n",
    "        ## Update the control variable\n",
    "        more_stories = pagination_token is not None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching for sources\n",
    "\n",
    "As we discussed, it is possible to search for sources through the API. However, in my opinion, it is a bit inefficient. It is much easier to do it through a web browser. That is also because searching through the API requires using a different endpoint, `DirectoryAPI`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First, we need to establish a connection with the DirectoryApi\n",
    "directory_api = mediacloud.api.DirectoryApi(API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is just enough to provide the name of the source we are interested in, and MediaCloud will return all the sources that match the given query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## If for some reason we are interested in the sources which have in the name the word Gazeta\n",
    "gazetas = directory_api.source_list(name=\"gazeta\")\n",
    "gazetas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowing the `id`, we can get through the API the description of the source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We can extract information about the source also by providing the id of the source\n",
    "gazetajnk = directory_api.source(gazetas[\"results\"][-1][\"id\"])\n",
    "gazetajnk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also search for the collection by its name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is going to return all the collection about Poland\n",
    "collection_poland = directory_api.collection_list(name=\"poland\")\n",
    "collection_poland"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, to list all the sources from the collection it is enough to do the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ID of Polish National collection\n",
    "POLAND = 34412416\n",
    "## If we set the limit to None it should return all sources\n",
    "sources = directory_api.source_list(collection_id=POLAND, limit=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ids",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
