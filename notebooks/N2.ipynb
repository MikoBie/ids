{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit\n",
    "<div style=\"text-align:center\"><img src=\"png/reddit_log.png\" /></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is reddit?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, Reddit is a big fat old forum. Below I copied some basic information from their [help page](https://www.reddithelp.com/hc/en-us/articles/204511479-What-is-Reddit/). They give the following answer to the question of what Reddit is:\n",
    "> * Reddit is a source for what's new and popular on the Internet.\n",
    "> * Users like you provide all of the content and decide, through voting, what's good and what's junk.\n",
    "> * Reddit is made up of many individual communities, also known as subreddits. Each community has its own page, subject matter, users, and moderators.\n",
    "> * Users post stories, links, and media to these communities, and other users vote and comment on the posts.\n",
    "> * Through voting, users determine what posts rise to the top of community pages and, by extension, the public home page of the site.\n",
    "> * Links that receive community approval bubble up towards #1, so the front page is constantly in motion and (hopefully) filled with fresh, interesting links.\n",
    "\n",
    "~~Personally, I *do not have* an account on Reddit and probably not planning to have one, but~~ if you want to understand better what kind of data you can extract from there I would recommend setting up an account. As far as I understand Reddit is a big old internet forum (similar to 4chan or Polish Wykop) in which users post or comment on different information. Actually, every user can perform four types of actions:\n",
    "\n",
    "1. Create a subreddit. Basically, it is a subforum on a given topic in which a group of users discusses it.\n",
    "2. Write a post (submission) in a given subreddit.\n",
    "3. Write a comment on a given post.\n",
    "4. Rate a given comment or post.\n",
    "\n",
    "For these actions, people earn **karma**.\n",
    "\n",
    "### What is karma?\n",
    "\n",
    "Again, according to [Reddit's help page](https://www.reddithelp.com/hc/en-us/articles/204511829-What-is-karma-) karma is:\n",
    "\n",
    ">A user's **karma** reflects how much a user has contributed to the Reddit community by an approximate indication of the total votes a user has earned on their submissions (\"post karma\") and comments (\"comment karma\"). When posts or comments get upvoted, that user gains some karma. You can see how much karma a user has on their profile page.\n",
    ">\n",
    ">Karma is only approximate: there is not a 1:1 relationship with votes. Your post karma will always be significantly lower than the total number of votes you receive on your links. Comment karma is closer to a 1:1 relationship but is still only approximate.\n",
    "\n",
    "Therefore, there are two important pieces of information here. First, users differ in karma points which are based on their activity and the popularity of the content they created. This information might be useful when/if we learn how to get information on users. Second, posts (submissions) or comments might be either upvoted or downvoted. This is important because as far as I understand the comments or posts (submissions) with the highest score are exposed on the front page of Reddit and might have a bigger impact on the users not necessarily only the given subreddit. Also, comments with a high score are displayed higher under the submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reddit API\n",
    "In general, we now should know what API is and what Reddit is. So it is the right time to [talk about practice](https://www.youtube.com/watch?v=_UMIcM66S1M), i.e. where to find Reddit's API. This question is more complex than it might seem. There are two ways to access Reddit's data through the API:\n",
    "\n",
    "1. **Official Reddit API.** In most cases the best way to access data from a webpage (social media platform) that has an API is to use the official one. You might find documentation on Reddit [here](https://www.reddit.com/dev/api). This webpage is not particularly beautiful but rarely documentation is. At first glance, you probably would be overwhelmed with the amount of information you find there. However, for now, you only need to know that you are not going to use the official Reddit API because it is inconvenient. It requires authentication (having a developer account). Anyhow, if you decide to perform a more detailed analysis of Reddit you probably should read the official documentation and visit these two pages: [Reddit's Archived GitHub repository](https://github.com/reddit-archive/reddit/wiki/API) and [Documentation on Reddit's API Python Wrapper](https://praw.readthedocs.io/en/latest/). This is a lot of reading and understanding, however, there is no other way unless...\n",
    "2. **Pushshift Reddit's API.** There is a Reddit user Jason Baumgartner who for unclear reasons (at least for me they are unclear but I was not particularly motivated to look it up) decided to dump the whole Reddit. Moreover, he created an API to access the data he collects. On [this](https://pushshift.io) much nicer webpage (but not very up-to-date) you will find documentation on his API.\n",
    "\n",
    "In our case, we will use **Pushshift Reddit API**. It is much easier to use and for our purposes, it will be more than enough. Though it does not allow for collecting exactly the same data as when using the **Official Reddit's API**, however, it has the huge advantage of not requesting authentication. When we are using Pushshift we need to remember a few things:\n",
    "\n",
    "1. It is less reliable than the official API because it is run by a single person (half-truth).\n",
    "2. It does not offer the same functionality as the official API.\n",
    "3. It is likely to introduce some kind of authentication in the future.\n",
    "4. It streams the data live from Reddit hence post scores are not that reliable (in theory it does it twice in 24h window, however, sometimes there are delays).\n",
    "\n",
    "## API and where to find it?\n",
    "\n",
    "So in simple terms, an API is an interface using which you send a specific message (request) and get something back (response). In the case of **Pushshift**, it lives under the following [url](https://api.pushshift.io). However, if you click on it a blank page will open. For some reason, it works like this but the more common practice is to use the API address to put the documentation there ([Wikipedia](https://en.wikipedia.org/w/api.php) does exactly that). You can find documentation of Pushshift API [here](https://pushshift.io/api-parameters/). Before you move any further you should start reading it. Why? Because you need to know what the API can offer you. In other words what kind of data you might access?\n",
    "\n",
    "Using Pushshift API you might access either submissions or comments even though in the [docummantation](https://pushshift.io/api-parameters/) they state something different. Therefore, it is better to visit their [GitHub repository](https://github.com/pushshift/api). To access submissions or comments we will use something which is called endpoints. If an API is an interface endpoint is a communication channel. In Pushshift there are two (if you click on any of the links you should see the last 25 comments or last 25 submissions):\n",
    "\n",
    "* [https://api.pushshift.io/reddit/submission/search](https://api.pushshift.io/reddit/submission/search)\n",
    "* [https://api.pushshift.io/reddit/comment/search](https://api.pushshift.io/reddit/comment/search)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.\n",
    "First things first. Likewise in _R_, we will start with loading libraries. In terms of _Python_, they are called modules. Below I load three modules that we will use in this script. Because we are using Google Colab we do not have to install them. If you were using _Python_ on your personal computer you would have to install them first. It is the same in _R_, where you have to install the package only once, and afterward, you might use it till the world's end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as rq ## this is a module to send requests\n",
    "import json ## this is a module to process json\n",
    "import time ## this is a module we will need to understand time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.\n",
    "Let's define our two endpoints as `strings`. It makes everything more convenient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Comments endpoint \n",
    "url_comments = 'https://api.pushshift.io/reddit/search/comment/'\n",
    "\n",
    "## Submissions endpoint\n",
    "url_submissions = 'https://api.pushshift.io/reddit/search/submission/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.\n",
    "When we clicked the link of each endpoint we got a random 25 comments or submissions, but when we looked at the documentation we saw that there were plenty of different parameters we could specify. I would recommend visiting this website where the [documentation](https://github.com/pushshift/api) is presented more compactly and understandably. Below I use only two parameters but you might want to specify more. However, you need to be careful with after and before because they are in what might seem like a strange format. It is called [epoch time](https://en.wikipedia.org/wiki/Unix_time). It counts seconds from January 1st, 1970, the beginning of Unix Epoch, till [03:14:07 UTC on 19 January 2038](https://en.wikipedia.org/wiki/Year_2038_problem). To convert this strange format to more human-readable we will use two functions from the `time` module: \n",
    "\n",
    "* `time.strptime()` is a function to convert a string into a date format based on a specific pattern. In other words, we tell _Python_ that `'30.08.2011 11:05:02'` is not a normal string but a date object of the following pattern `'%d.%m.%Y %H:%M:%S'`;\n",
    "* `time.mktime()` is a function to convert a normal date into epoch format, for example, it converts '`30.08.2011 11:05:02`' into the following number `1314695102`.\n",
    "\n",
    "The usage of both of them would look something like this:\n",
    "```python\n",
    "date_time = '30.08.2011 11:05:02'\n",
    "pattern = '%d.%m.%Y %H:%M:%S'\n",
    "after_time = str(int(time.mktime(time.strptime(date_time, pattern))))\n",
    "```\n",
    "At the first glance, it might look a bit complex but when you think about it it is not. I mean I do not expect you to know it but to understand it on a very general level. The only thing you will have to change below data_time to the dates of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_time = '30.08.2011 11:05:02'\n",
    "pattern = '%d.%m.%Y %H:%M:%S'\n",
    "after_time = str(int(time.mktime(time.strptime(date_time, pattern))))\n",
    "\n",
    "payload = { 'subreddit' : 'climate',\n",
    "            'after' : after_time }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.\n",
    "So when we know the URL of the endpoint and options we want to pass we should send the request to this URL. We will use the `get` function from the `requests` module. However, unlike in _R_, we need to tell _Python_ from which module that function is. Therefore, we will use `rq.get()`.\n",
    "This function will take as the first argument the endpoint URL and as the second argument specific options, we want to pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's send the request and save the response as the response object\n",
    "response = rq.get(url_submissions, params = payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check what we got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we just execute the chunk above we will get only a mysterious and enigmatic code 200. This is good information. It means that we got a valid response from the server. There are multiple different codes we could get when we send the request to the server but you should be aware of two: [5xx](https://github.com/500) and [4xx](https://www.pixar.com/404). In general, the former means that there is an issue on the server side and the latter that the resource you are looking for [does not exist](https://github.com/404)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5.\n",
    "Ok, but how to extract from this response object dome data? It is easier than it might look like but what we need to do is to use a method text on the object. I am not going too much into detail but in _Python_ objects might have methods (functions) that might be run on them. It is a bit like the internal ability of the object that is executed on the object in question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, now our `response.text` object for _Python_ is just a not really interesting long string. To be able to process it further (mainly save it into a data file) we need to transform it into a _Python_ representation of JSON. We will do it by using function `json.loads()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.loads(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if you look closely at the object above you will realize it is a single curly bracket object. It has one key -- data and one value which is a list. You can either believe me or we can just check it in _python_ by using the method `dct.keys()` on the `json.loads(response.text)` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.loads(response.text).keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extract and assign the value from this key we will execute the following code which is a bit similar to what you are used to in _R_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.loads(response.text)['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6. \n",
    "Let's save the file now to the JSON line file on our computer. It is not as complicated as it might seem. We are simply opening a file and then writing every single line from our data object into this file. When saving the file we convert this strange date format to a more useful one: `'Y-m-d H:M:S'`. To do so first we use the function `time.localtime()` that converts Epoch time to date format and afterward use time.strftime we convert it to the desired format of `'Y-m-d H:M:s'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Open the file in write mode\n",
    "with open('climate.jl', 'w') as file:\n",
    "    ## Iterate every all elements\n",
    "    for line in data:\n",
    "        ## Convert Epoch time to human-readable format\n",
    "        line['created_utc'] = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(line['created_utc']))\n",
    "        ## Check whether the field author_created_utc exists\n",
    "        if 'author_created_utc' in line.keys():\n",
    "            ## Convert Epoch time to human-readable format\n",
    "            line['author_created_utc'] = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(line['author_created_utc']))\n",
    "        ## Dump the line to the file\n",
    "        file.write(json.dumps(line) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Read into _Python_ the file you just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Open the file in read mode\n",
    "with open('climate.jl', 'r') as file:\n",
    "\t## Create a new list with dictionaries as elements\n",
    "    df = [ json.loads(item) for item in file.readlines() ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments\n",
    "It is a bit more tricky to access the comments than submissions but at the end of the day not that hard. First, you need to decide what you would want to access. You have multiple options:\n",
    "\n",
    "* get all comments from the specific subreddit - it is relatively the easiest, however, it will take days to collect all comments or at least hours for big subreddits. You need to specify the `subreddit` field and nothing else (however, it is wise to change the default number of collected items to the maximum - `'size' : 100`). Therefore, the payload should look something like that:\n",
    "```python\n",
    "payload = { 'subreddit' : 'todayilearned',\n",
    "            'size' : 100 }\n",
    "```\n",
    "* get all comments from under the specific submission. You need to specify the `link_id` field and nothing else (however, it is wise to change the default number of collected items to the maximum - `'size' : 100`). The important note is that as a `link_id` you should put an `id` of the submission. Therefore, the payload should look something like that:\n",
    "```python\n",
    "payload = { 'link_id' : '8vjr2l',\n",
    "            'size' : 100 }\n",
    "```\n",
    "* get all comments from under the comment. You need to specify the `parent_id` field and nothing else (however, it is wise to change the default number of collected items to the maximum - `'size' : 100`). The important note is that as a `parent_id` you should put an `id` of the comment. Therefore, the payload should look something like that:\n",
    "```python\n",
    "payload = { 'parent_id' : 'e1nuxpc',\n",
    "            'size' : 100 }\n",
    "```\n",
    "Obviously, you can specify also other fields to narrow the search, however, I guess for our purposes it will not be necessary.\n",
    "\n",
    "Actually, the procedure of scraping the comments is very similar to the submission one. The only difference is the `payload` and `source_url`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the payload\n",
    "date_time = '30.08.2022 11:05:02'\n",
    "pattern = '%d.%m.%Y %H:%M:%S'\n",
    "after_time = str(int(time.mktime(time.strptime(date_time, pattern))))\n",
    "\n",
    "source_url = 'https://api.pushshift.io/reddit/search/comment/'\n",
    "payload = { 'subreddit': 'todayilearned',\n",
    "            'after' : after_time}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Send the request\n",
    "response = rq.get(source_url, params = payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract the data to a list of dictionaries\n",
    "data = json.loads(response.text)['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Write out the data in a JSON line file. Make sure that all the dates are in a human-readable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Open the file in write mode\n",
    "with open('comments.jl', 'w') as file:\n",
    "    ## Iterate every all elements\n",
    "    for line in data:\n",
    "        ## Convert Epoch time to human-readable format\n",
    "        line['created_utc'] = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(line['created_utc']))\n",
    "        line['retrieved_utc'] = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(line['retrieved_utc']))\n",
    "        ## Dump the line to the file\n",
    "        file.write(json.dumps(line) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('ids')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "2f6b087e3237558552441618582c1cfff7dc162872ae65c00a668bb71df085d3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
