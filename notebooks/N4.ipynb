{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscraping\n",
    "\n",
    "![Webscraping](png/webscraping.jpg)\n",
    "\n",
    "The general idea behind webscraping is quite simple. Instead of copying the website yourself you employ a computer program to do so. Although it is that simple you need to remember that the robot (the web spider) does not see the webpage exactly as we do in a web browser, but looks only at the HTML code. I talked a bit about it in the presentation, but now we will dig into it even more. Let's first click at [this link](https://wikileaks.org/dealmaker/Al-Yousef/) that leads to the Wikileaks article ~~and lear more about shady interests between French state-owned company and the United Arab Emirates~~. On this fairly \n",
    "\n",
    "So this is the way we normally see the webpages, but as I wrote above it is not a very useful way of looking at webpages from the webscraping point of view. Let's now look at what is behind the nicely looking webpage. To see the HTML source code you simply need to press `ctrl + shift + I` (or in Safari on Mac `cmd + option + I`). What you see is the HTML code of this particular WikiLeaks article. This is what your program will see. So let's talk now a bit about what is there.\n",
    "\n",
    "## HTML (HypterText Markdown Language)\n",
    "\n",
    "HTML is a programming language in which most of the websites you browse on the Internet are written. The HTML code describes the structure of the webpage and contains multiple elements, which are represented as tags. So, for the time being, let's move away from WikiLeaks and concentrate on the easy example of a HTML code which creates a simple webpage (if you do not believe me you can just copy the code below and save it in the notepad with extension HTML).\n",
    "\n",
    "```html\n",
    "    <!DOCTYPE html>\n",
    "    <html>\n",
    "        <head>\n",
    "            <title>\n",
    "                Justyna Kowalczyk fandom\n",
    "            </title>\n",
    "        </head>\n",
    "        <body>\n",
    "            <h1>\n",
    "                Why Justyna Kowalczyk is the best?\n",
    "            </h1>\n",
    "            <p>\n",
    "                Because she is just <b>the best</b> cross-county skier in the history of the sport. \n",
    "                You can learn more about her amazing achivements visiting her Wikipedia webpage:\n",
    "                https://pl.wikipedia.org/wiki/Justyna_Kowalczyk.\n",
    "            </p>\n",
    "        </body>\n",
    "    </html>\n",
    "```\n",
    "\n",
    "### What are tags?\n",
    "\n",
    "Tags are used to mark up the start of an HTML element and they are enclosed in **angle brackets**. Above we have a few tags but the most important is the root tag ```<html>```. Inside this tag, between ```<html>``` and ```</html>``` all other elements live. In the example above we have following tags:\n",
    "\n",
    "* ```<head></head>``` - element contains meta information about the document\n",
    "* ```<title></title>``` - element specifies a title for the document\n",
    "* ```<body></body>``` - element contains the visible page content\n",
    "* ```<h1></h1>``` - element defines a large heading\n",
    "* ```<p></p>``` - element defines a paragraph\n",
    "* ```<b></b>``` - element defines a boldface\n",
    "\n",
    "It might not be clear from the very first glance but HTML code has a structure of a tree. Hopefully, it will be more clear on this scheme:\n",
    "\n",
    "<center><img src=\"png/dom.png\" width = 600/></center>\n",
    "\n",
    "However, apart from obvious reasons this website has a major drawback. The address to Justyna Kowalczyk Wikipedia webpage is not a link. So this website does not really take the advantage of one of the most basic futures of HTML -- hypertext links. For that we need attributes of tags.\n",
    "\n",
    "## What are attributes?\n",
    "\n",
    "Attributes provide a piece of extra information about an element and the are alwyas specified in the opening tag of the element. They coem in `item-value` pairs. For example, in our website about Justyna Kowalczyk (<3) instead of writing the address of ther Wikipedia page in a plain text we would tag it in the following way:\n",
    "\n",
    "```html\n",
    "<a href=https://pl.wikipedia.org/wiki/Justyna_Kowalczyk> Wikipedia webpage <\\a>\n",
    "```\n",
    "This way we would get a clickable link. In HTML there are many attributes for specific tags, but since we are going to only webscrap data not create webpages, let's focus on the useful ones from our perspective.\n",
    "\n",
    "* `href` - sepifies the URL (web address) for a link\n",
    "* `src` - specifies the URL (web address) for an image\n",
    "* `id` - specifies a unique id for an element\n",
    "* `class` - specifies a class of an element\n",
    "\n",
    "### Division tag\n",
    "\n",
    "Let's now move back to the [Wikileaks](https://wikileaks.org/Al-Yousef/) webpage and open its source code. At first glance, it looks how it should. There are ```<html>```, ```<head>```, and ```<body>``` tags. However, instead of getting straight into paragraphs and titles in the ```<body>``` tag we have some strange ```<div>``` tags. They serve as containers of other HTML elements and as the name suggests divide the HTML document into meaningful sections. If we look at the WikiLeaks webpage there is a bunch of different ```<div>``` tags inside the ```<body>``` tag (there are two other tags there as well, but we will ignore them for time being). In most of the web browser, you can either hover over the tag to highlight the specific part of the webpage or hover over the part of the webpage to highlight the specific container. Either way, you need to know in which container the data we are interested in is stored so you can direct your robot (webspider) to navigate to this specific tag and extract the text from it.\n",
    "\n",
    "## CSS selectors\n",
    "\n",
    "The core idea in webscraping is to use the fact that every website is an HTML document and every HTML document is very nicely structured (it has a from of a tree). As a result, it is (almost) always possible to identify an element by providing a path leading to it from the root of the document. In practical context, we usually only have to specify a partial path.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSS selectors\n",
    "\n",
    "The core idea in webscraping is to use the fact that every website is an HTML document and every HTML document is very nicely structured (it has a form of a tree).\n",
    "As a result, it is (almost) always possible to identify an element uniquely by providing a path leading to it from the root of the document. In practical context, we usually only have to specify a partial path.\n",
    "\n",
    "<center><img src=\"png/tree.png\" width = 750/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us assume that we want to extract the title (Justyna Kowalczyk) data from the simple document above. To do this, we have to specify a unique path going from the root of the document tree (`<html>` tag) to the `<title>` tag. Technically speaking, there are many ways to do this. One of the most convenient is to use so-called CSS selectors. CSS selectors are super easy as they are defined just as simple strings in which each word (separated with space) corresponds to a tag at a given level of the tree, counting from the root. So in our case, the appropriate CSS selector is the following:\n",
    "\n",
    "```python\n",
    "\"html head title\"\n",
    "```\n",
    "\n",
    "Note that we do not have to use tag braces (`<` and `>`). In general, we can also omit some levels of the tree and write a more general selector. Such a selector can give us multiple tags if there are multiple matches. In our case we can really simplify the selector to the following form:\n",
    "\n",
    "```python\n",
    "\"title\"\n",
    "```\n",
    "\n",
    "Why? Because there is only a single `<title>` tag in our document, so our selector in simplified still uniquely determines the part of the webpage we want to extract data from. However, what will a selector like this do? Will it return a single element of the webpage or multiple elements?\n",
    "\n",
    "```python\n",
    "\"body div\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced selectors: classes and ids\n",
    "\n",
    "Usually referring to elements of a webpage only by using generic tag names is not enough to get what we want. That is why we have to include some additional data in our selectors. Most often we extend selectors by adding information on classes and ids attached to HTML tags. Classes and ids are used in web development to provide finely-grained control over aesthetics and layout of a webpage as they allow to address different parts with greater precision. Similarly in the context of webscraping they can be used to write more precise selectors. Consider the example below.\n",
    "\n",
    "```html\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "  <head>\n",
    "      <title>A website with CSS classes and ids</title>       \n",
    "  </head>\n",
    "  <body>\n",
    "    <div class=\"outer\">\n",
    "      <div class=\"inner\" id=\"first-one\">\n",
    "        Example I\n",
    "      </div>\n",
    "      <div class=\"inner active\">\n",
    "        Example II\n",
    "      </div>\n",
    "      <div class=\"inner\">\n",
    "        Last example\n",
    "      </div>\n",
    "    </div>\n",
    "  </body>\n",
    "</html>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data provided in the `class` and `id` attributes are just a sequence of labels (separated with spaces) that can be used to identify particular elements of the HTML document. The only, but very important, difference between a `class` and an `id` is that the same `class` can be assigned to multiple elements (so classes are in principle used to identify groups of elements that are supposed to be in some sense equivalent) and any `id` can be assigned to only one unique element (so ids are used to identify specific, distinguished elements).\n",
    "\n",
    "Classes and ids can be specified in a CSS selector correspondingly by appending their names to a tag name after a dot (`.`) or a hash (`#`).\n",
    "\n",
    "```python\n",
    "\"div.outer\"           # selects all <div> elements with \"outer\"\n",
    "\"div.inner.active\"    # selects all <div> elements with both \"inner\" and \"active\" class\n",
    "\n",
    "\"div#first-one\"        # selects the unique <div> with id \"first-one\"\n",
    "\"div#first-one.inner\"  # selects the unique <div> with id \"first-one\" and class \"inner\" (this does not work on Google Colab due to a bug)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now try to extract data from particular inner `<div>` containers. In order to do this properly, we will have to use CSS selectors with classes and ids.\n",
    "\n",
    "A selector such as\n",
    "\n",
    "```python\n",
    "\"body div.inner\"\n",
    "```\n",
    "\n",
    "will not do because it will match and return multiple (exactly three) `<div>` elements with class `\"inner\"`. However, if we want to get the second `<div>`, then we can do this because it is uniquely specified if we also include the `\"active\"` class. So we can rewrite the selector like this:\n",
    "\n",
    "```python\n",
    "\"body div.inner.active\"\n",
    "```\n",
    "\n",
    "Similarly, we can address the first inner `<div>` like this:\n",
    "\n",
    "```python\n",
    "\"body div#first-one\"\n",
    "```\n",
    "\n",
    "In fact, we can even simplify this selector, because we know that HTML ids are always unique. Can you find the simples (shortest) selector?\n",
    "\n",
    "However, the last `<div>` is problematic, since we can not write a selector that would select only it. In order to do so, we will have to use additional functionalities provided by our Python tools for webscraping.\n",
    "\n",
    "NOTE. In general, it is possible to write a selector for this problem, but it is quite technically involved, so we will not do this here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "<!DOCTYPE html>\n",
       "\n",
       "<html>\n",
       "<head>\n",
       "<title>A website with CSS classes and ids</title>\n",
       "</head>\n",
       "<body>\n",
       "<div class=\"outer\">\n",
       "<div class=\"inner\" id=\"first-one\">\n",
       "        Example I\n",
       "      </div>\n",
       "<div class=\"inner active\">\n",
       "        Example II\n",
       "      </div>\n",
       "<div class=\"inner\">\n",
       "        Last example\n",
       "      </div>\n",
       "</div>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup    # This is the Python package that provides nice tools for parsing HTML documents\n",
    "\n",
    "html_string = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "  <head>\n",
    "      <title>A website with CSS classes and ids</title>       \n",
    "  </head>\n",
    "  <body>\n",
    "    <div class=\"outer\">\n",
    "      <div class=\"inner\" id=\"first-one\">\n",
    "        Example I\n",
    "      </div>\n",
    "      <div class=\"inner active\">\n",
    "        Example II\n",
    "      </div>\n",
    "      <div class=\"inner\">\n",
    "        Last example\n",
    "      </div>\n",
    "    </div>\n",
    "  </body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# Here we create an object representing our HTML document\n",
    "# We will use it to easily work with the data it contains\n",
    "html = BeautifulSoup(html_string, 'html.parser')\n",
    "html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n        Example I\\n      '"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Extract the first div\n",
    "html.select_one(\"body div#first-one\").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Example I'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## It is better to strip the text in order to get rid of the whitespace\n",
    "html.select_one(\"body div#first-one\").text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Example II'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Extract the middle div\n",
    "html.select_one(\"body div.inner.active\").text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, things are not as nice when we try to extract the last `<div>` because we can not write a unique selector for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<div class=\"inner\" id=\"first-one\">\n",
       "        Example I\n",
       "      </div>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html.select_one(\"body div.inner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we did not get an error, but only the first matched item. If we want to get all the matching elements we can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<div class=\"inner\" id=\"first-one\">\n",
       "         Example I\n",
       "       </div>,\n",
       " <div class=\"inner active\">\n",
       "         Example II\n",
       "       </div>,\n",
       " <div class=\"inner\">\n",
       "         Last example\n",
       "       </div>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html.select(\"body div.inner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This actually makes it very simple for us to just extract the element we want with simple indexing because we got a list of matched elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<div class=\"inner\">\n",
       "        Last example\n",
       "      </div>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html.select(\"body div.inner\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Last example'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And we can extract text from it in the usual way\n",
    "html.select(\"body div.inner\")[-1].text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: scraping Wikileaks\n",
    "\n",
    "Wikileaks page has a relatively simple structure so it constitutes a very good playground for us to try to really understand webscraping in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we have to use the `requests` package to download the actual HTML document representing the website we are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"https://wikileaks.org/dealmaker/Al-Yousef/\")\n",
    "html = BeautifulSoup(response.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to examine the structure of the website and figure out what kind of selector do we need. Once we have it, we can use it to extract the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today WikiLeaks publishes a secret document from the International Chamber of Commerce (ICC) International Court of Arbitration, pertaining to a dispute over commission payment in relation to a $3.6 billion arms deal between French state-owned company GIAT Industries SA (now Nexter Systems) and the United Arab Emirates (UAE). The agreement was for the sale of 388 Leclerc combat tanks, 46 armoured vehicles, 2 training tanks, spare parts and ammunition. It was signed in 1993 and scheduled to be completed in 2008.\n",
      "The case brought before the ICC arbitration tribunal was a claim from Abbas Ibrahim Yousef Al Yousef, a UAE businessman, that GIAT had not honoured a contract to pay him a 6,5% commission on the deal or almost $235 million total. GIAT stopped paying after sending Al Yousef over $195 million through his company Kenoza Consulting & Management Inc., which was registered in the British Virgin Islands. Al Yousef demanded the nearly $40 million that remained outstanding.\n",
      "GIAT's lawyers maintained that they had to stop payments as they became illegal when the OECD Anti-Corruption Convention was transposed into French law in the year 2000. They claimed \"Kenoza intended to commit and indeed committed corruption acts\". Al Yousef firmly denied that any part of the commission had been used to bribe UAE officials or used in any corrupt acts. As GIAT did not produce any evidence for the claim, the ICC Tribunal did not rule on the issue but noted that \"...if the excessive nature of the compensation for the Claimants service must be taken as evidence of a corrupt purpose of the Agency Agreement, this purpose must have been known and intended by both Parties to the agreement\".\n",
      "The Tribunal did investigate what services Al Yousef provided to justify the excessive commission. Despite claims to the contrary, the Tribunal found that Al Yousef did not play an important role in the development of the Leclerc tank. The tanks were fitted with German engines, which created an obstacle as this would violate laws forbidding German arms sales to the Middle East. Al Yousef claimed he had successfully lobbied German authorities to obtain a waiver from these laws in \"...a process which involved decision makers at the highest levels, both in France and Germany\". During a witness statement, Al Yousef could not remember the names of any German officials and told the Tribunal he had used lobbyists instead of meeting with German authorities directly.\n",
      "Surprisingly, Al Yousef told the Tribunal that had he been on a retainer, he would have asked GIAT to pay him a million dollars a month as a consultant. That would have brought him $51 million to $60 million rather than nearly $235 million. As a result, the Tribunal concluded that \"...the contractual commission rates is far above anything that could be justified (...). The remuneration is excessive by the standard which Mr Al Yousef himself set and by any standard which was raised in the arbitration\". His claims were dismissed and Al Yousef was ordered to pay the entire cost of arbitration by the Tribunal ($550 000) plus a portion of GIAT’s legal costs (€115 000).\n",
      "Joseph A. Farrell contributed to this article.\n"
     ]
    }
   ],
   "source": [
    "for p in html.select('div.content div.release div.leak-content p'):\n",
    "    text = p.text\n",
    "    if text:\n",
    "        print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Ia.\n",
    "\n",
    "The task is to extract data about products from a simple mockup of an online shop. For every record create a dictionary of the following form:\n",
    "\n",
    "```python\n",
    "record = {\n",
    "    \"product\": <name of the product>,\n",
    "    \"price\": <product price>,\n",
    "    \"description\": <description>,\n",
    "    # Optionally, you can also try to extract\n",
    "    \"reviews\": <number of reviews>,\n",
    "    \"rating\": <rating score / number of stars>\n",
    "}\n",
    "```\n",
    "\n",
    "In other words, the task is to first create a selector that extracts product records. Then, loop over the records and extract data to create a list of records defined as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "response = requests.get(\"https://webscraper.io/test-sites/e-commerce/allinone/computers/tablets\")\n",
    "html = BeautifulSoup(response.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------- #\n",
    "# Solution we developed in the class #\n",
    "# ---------------------------------- #\n",
    "thumbnails = html.select('div.container.test-site div.thumbnail')\n",
    "# Show structure of the first one\n",
    "thumbnails[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the list in which we will store the dictionaries with product info\n",
    "records = []\n",
    "\n",
    "for thumb in thumbnails:\n",
    "    title = thumb.select_one('div.caption h4 a.title').text.strip()\n",
    "    price = thumb.select_one('div.caption h4.price').text.strip()\n",
    "    description = thumb.select_one('div.caption p.description').text.strip()\n",
    "    # Here I solve the optional part and extract data about rating and reviews\n",
    "    reviews = thumb.select_one('div.ratings p.pull-right').text.strip()\n",
    "    # To get rating I count the number of stars a product has\n",
    "    rating = len(thumb.select('div.ratings p span.glyphicon-star'))\n",
    "    record = {\n",
    "        'title': title,\n",
    "        'price': price,\n",
    "        'description': description,\n",
    "        'reviews': reviews,\n",
    "        'rating': rating\n",
    "    }\n",
    "    records.append(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can take a look and see what we created\n",
    "records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Ib.\n",
    "\n",
    "The task is to extract data from **the first table** in a simple mockup of a webpage. For every row in the table create a dictionary of the following form:\n",
    "\n",
    "```python\n",
    "record = {\n",
    "    \"first_name\": <first name>,\n",
    "    \"last_name\": <last_name>,\n",
    "    \"username\": <username>\n",
    "}\n",
    "```\n",
    "\n",
    "First, you may try to extract data from all tables (or some subset). And then try to figure out what you can do to limit your results only to the first table.\n",
    "Remember that you can both modify your CSS selector or first extract al data and store it in a list (i.e. with `.select()` method)\n",
    "and then extract the part you need from the list with Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "response = requests.get(\"https://webscraper.io/test-sites/tables\")\n",
    "html = BeautifulSoup(response.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('ids')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "2f6b087e3237558552441618582c1cfff7dc162872ae65c00a668bb71df085d3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
