{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscraping\n",
    "\n",
    "![Webscraping](png/webscraping.jpg)\n",
    "\n",
    "The general idea behind webscraping is quite simple. Instead of copying the website yourself, you employ a computer program to do so. Although it is that simple you need to remember that the robot (the web spider) does not see the webpage exactly as we do in a web browser, but looks only at the HTML code. I talked a bit about it in the presentation, but now we will dig into it even more. Let's first click at [this link](https://wikileaks.org/dealmaker/Al-Yousef/) that leads to the Wikileaks article ~~and learn more about shady interests between French state-owned company and the United Arab Emirates~~. In this fairly simple (but real-life) example, we will see how to get the text of the article without copying and pasting it.\n",
    "\n",
    "So this is the way we normally see the webpages, but as I wrote above it is not a very useful way of looking at webpages from the webscraping point of view. Let's now look at what is behind the nicely looking webpage. To see the HTML source code you simply need to press `ctrl + shift + I` (or in Safari on Mac `cmd + option + I`). What you see is the HTML code of this particular WikiLeaks article. This is what your program will see. So let's talk now a bit about what is there.\n",
    "\n",
    "## HTML (HypterText Markdown Language)\n",
    "\n",
    "HTML is a programming language in which most of the websites you browse on the Internet are written. The HTML code describes the structure of the webpage and contains multiple elements, which are represented as tags. So, for the time being, let's move away from WikiLeaks and concentrate on the easy example of HTML code that creates a simple webpage (if you do not believe me you can just copy the code below and save it in the notepad with the HTML extension).\n",
    "\n",
    "```html\n",
    "    <!DOCTYPE html>\n",
    "    <html>\n",
    "        <head>\n",
    "            <title>\n",
    "                Justyna Kowalczyk fandom\n",
    "            </title>\n",
    "        </head>\n",
    "        <body>\n",
    "            <h1>\n",
    "                Why Justyna Kowalczyk is the best?\n",
    "            </h1>\n",
    "            <p>\n",
    "                Because she is just <b>the best</b> cross-country skier in the history of the sport. \n",
    "                You can learn more about her amazing achievements visiting her Wikipedia webpage:\n",
    "                https://pl.wikipedia.org/wiki/Justyna_Kowalczyk.\n",
    "            </p>\n",
    "        </body>\n",
    "    </html>\n",
    "```\n",
    "\n",
    "### What are tags?\n",
    "\n",
    "Tags are used to mark up the start of an HTML element and they are enclosed in **angle brackets**. Above we have a few tags but the most important is the root tag ```<html>```. Inside this tag, between ```<html>``` and ```</html>``` all other elements live. In the example above we have the following tags:\n",
    "\n",
    "* ```<head></head>``` - element contains meta-information about the document\n",
    "* ```<title></title>``` - element specifies a title for the document\n",
    "* ```<body></body>``` - element contains the visible page content\n",
    "* ```<h1></h1>``` - element defines a large heading\n",
    "* ```<p></p>``` - element defines a paragraph\n",
    "* ```<b></b>``` - element defines a boldface\n",
    "\n",
    "It might not be clear from the very first glance but HTML code has a structure of a tree. Hopefully, it will be more clear on this scheme:\n",
    "\n",
    "<center><img src=\"png/dom.png\" width = 600/></center>\n",
    "\n",
    "However, apart from obvious reasons this website has a major drawback. The address to Justyna Kowalczyk's Wikipedia webpage is not a link. So this website does not really take the advantage of one of the most basic features of HTML -- hypertext links. For that, we need attributes of tags.\n",
    "\n",
    "## What are attributes?\n",
    "\n",
    "Attributes provide a piece of extra information about an element and are always specified in the opening tag of the element. They come in `item-value` pairs. For example, on our website about Justyna Kowalczyk (<3) instead of writing the address of her Wikipedia page in plain text we would tag it in the following way:\n",
    "\n",
    "```html\n",
    "<a href=https://pl.wikipedia.org/wiki/Justyna_Kowalczyk> her Wikipedia webpage <\\a>\n",
    "```\n",
    "This way we would get a clickable link. In HTML there are many attributes for specific tags, but since we are going to only webscrap data not create webpages, let's focus on the useful ones from our perspective.\n",
    "\n",
    "* `href` - specifies the URL (web address) for a link\n",
    "* `src` - specifies the URL (web address) for an image\n",
    "* `id` - specifies a unique id for an element\n",
    "* `class` - specifies a class of an element\n",
    "\n",
    "### Division tag\n",
    "\n",
    "Let's now move back to the [Wikileaks](https://wikileaks.org/Al-Yousef/) webpage and open its source code. At first glance, it looks how it should. There are ```<html>```, ```<head>```, and ```<body>``` tags. However, instead of getting straight into paragraphs and titles in the ```<body>``` tag, we have some strange ```<div>``` tags. They serve as containers of other HTML elements and as the name suggests divide the HTML document into meaningful sections. If we look at the WikiLeaks webpage there is a bunch of different ```<div>``` tags inside the ```<body>``` tag (there are two other tags there as well, but we will ignore them for time being). In most web browsers, you can either hover over the tag to highlight the specific part of the webpage or hover over the part of the webpage to highlight the specific container. Either way, you need to know in which container the data we are interested in is stored so you can direct your robot (webspider) to navigate to this specific tag and extract the text from it.\n",
    "\n",
    "## CSS selectors\n",
    "\n",
    "The core idea in webscraping is to use the fact that every website is an HTML document and every HTML document is very nicely structured (it has the form of a tree). As a result, it is (almost) always possible to identify an element by providing a path leading to it from the root of the document. In a practical context, we usually only have to specify a partial path.\n",
    "\n",
    "<center><img src=\"png/tree.png\" width = 750/></center>\n",
    "\n",
    "\n",
    "Let us assume that we want to extract the title (Justyna Kowalczyk) data from the simple document above. To do this, we have to specify a unique path going from the root of the document tree (`<html>` tag) to the `<title>` tag. Technically speaking, there are many ways to do this. One of the most convenient is to use so-called CSS selectors. CSS selectors are super easy as they are defined just as simple strings in which each word (separated with space) corresponds to a tag at a given level of the tree, counting from the root. So in our case, the appropriate CSS selector is the following:\n",
    "\n",
    "```python\n",
    "\"html head title\"\n",
    "```\n",
    "\n",
    "Note that we do not have to use tag braces (`<` and `>`). In general, we can also omit some levels of the tree and write a more general selector. Such a selector can give us multiple tags if there are multiple matches. In our case we can really simplify the selector to the following form:\n",
    "\n",
    "```python\n",
    "\"title\"\n",
    "```\n",
    "\n",
    "Why? Because there is only a single `<title>` tag in our document, so our selector in simplified still uniquely determines the part of the webpage we want to extract data from. However, what will a selector like this do? Will it return a single element of the webpage or multiple elements?\n",
    "\n",
    "```python\n",
    "\"body div\"\n",
    "```\n",
    "\n",
    "\n",
    "### Advanced selectors: classes and ids\n",
    "\n",
    "Usually referring to elements of a webpage only by using generic tag names is not enough to get what we want. That is why we have to include some additional data in our selectors. Most often we extend selectors by adding information on classes and ids attached to HTML tags. Classes and ids are used in web development to provide finely-grained control over the aesthetics and layout of a webpage as they allow to address different parts with greater precision. Similarly in the context of webscraping they can be used to write more precise selectors. Consider the example below.\n",
    "\n",
    "```html\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "  <head>\n",
    "      <title>A website with CSS classes and ids</title>       \n",
    "  </head>\n",
    "  <body>\n",
    "    <div class=\"outer\">\n",
    "      <div class=\"inner\" id=\"first-one\">\n",
    "        Example I\n",
    "      </div>\n",
    "      <div class=\"inner active\">\n",
    "        Example II\n",
    "      </div>\n",
    "      <div class=\"inner\">\n",
    "        Last example\n",
    "      </div>\n",
    "    </div>\n",
    "  </body>\n",
    "</html>\n",
    "```\n",
    "\n",
    "\n",
    "Data provided in the `class` and `id` attributes are just a sequence of labels (separated with spaces) that can be used to identify particular elements of the HTML document. The only, but very important difference between a `class` and an `id` is that the same `class` can be assigned to multiple elements (so classes are in principle used to identify groups of elements that are supposed to be in some sense equivalent) and any `id` can be assigned to only one unique element (so ids are used to identify specific, distinguished elements).\n",
    "\n",
    "Classes and ids can be specified in a CSS selector correspondingly by appending their names to a tag name after a dot (`.`) or a hash (`#`).\n",
    "\n",
    "```python\n",
    "\"div.outer\"           # selects all <div> elements with \"outer\"\n",
    "\"div.inner.active\"    # selects all <div> elements with both \"inner\" and \"active\" class\n",
    "\n",
    "\"div#first-one\"        # selects the unique <div> with id \"first-one\"\n",
    "\"div#first-one.inner\"  # selects the unique <div> with id \"first-one\" and class \"inner\" (this does not work on Google Colab due to a bug)\n",
    "```\n",
    "\n",
    "\n",
    "Let us now try to extract data from particular inner `<div>` containers. In order to do this properly, we will have to use CSS selectors with classes and ids.\n",
    "\n",
    "A selector such as\n",
    "\n",
    "```python\n",
    "\"body div.inner\"\n",
    "```\n",
    "\n",
    "will not do because it will match and return multiple (exactly three) `<div>` elements with class `\"inner\"`. However, if we want to get the second `<div>`, then we can do this because it is uniquely specified if we also include the `\"active\"` class. So we can rewrite the selector like this:\n",
    "\n",
    "```python\n",
    "\"body div.inner.active\"\n",
    "```\n",
    "\n",
    "Similarly, we can address the first inner `<div>` like this:\n",
    "\n",
    "```python\n",
    "\"body div#first-one\"\n",
    "```\n",
    "\n",
    "In fact, we can even simplify this selector, because we know that HTML ids are always unique. Can you find the simplest (shortest) selector?\n",
    "\n",
    "However, the last `<div>` is problematic, since we can not write a selector that would select only it. In order to do so, we will have to use additional functionalities provided by our Python tools for webscraping.\n",
    "\n",
    "**NOTE:** In general, it is possible to write a selector for this problem, but it is quite technically involved, so we will not do this here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup    ## This is the Python package that provides nice tools for parsing HTML documents\n",
    "\n",
    "html_string = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "  <head>\n",
    "      <title>A website with CSS classes and ids</title>       \n",
    "  </head>\n",
    "  <body>\n",
    "    <div class=\"outer\">\n",
    "      <div class=\"inner\" id=\"first-one\">\n",
    "        Example I\n",
    "      </div>\n",
    "      <div class=\"inner active\">\n",
    "        Example II\n",
    "      </div>\n",
    "      <div class=\"inner\">\n",
    "        Last example\n",
    "      </div>\n",
    "    </div>\n",
    "  </body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "## Here we create an object representing our HTML document\n",
    "## We will use it to easily work with the data it contains\n",
    "html = BeautifulSoup(html_string, 'html.parser')\n",
    "html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract the first div\n",
    "html.select_one(\"body div#first-one\").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## It is better to strip the text in order to get rid of the whitespace\n",
    "html.select_one(\"body div#first-one\").text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract the middle div\n",
    "html.select_one(\"body div.inner.active\").text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, things are not as nice when we try to extract the last `<div>` because we can not write a unique selector for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html.select_one(\"body div.inner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we did not get an error, but only the first matched item. If we want to get all the matching elements we can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html.select(\"body div.inner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This actually makes it very simple for us to just extract the element we want with simple indexing because we got a list of matched elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html.select(\"body div.inner\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## And we can extract text from it in the usual way\n",
    "html.select(\"body div.inner\")[-1].text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: scraping Wikileaks\n",
    "\n",
    "The Wikileaks page has a relatively simple structure so it constitutes a very good playground for us to try to really understand webscraping in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import modules\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we have to use the `requests` package to download the actual HTML document representing the website we are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Send a request\n",
    "response = requests.get(\"https://wikileaks.org/dealmaker/Al-Yousef/\")\n",
    "\n",
    "## Convert a string into html object\n",
    "html = BeautifulSoup(response.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to examine the structure of the website and figure out what kind of selector do we need. Once we have it, we can use it to extract the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create an empty list\n",
    "article = []\n",
    "\n",
    "## Iterate over all paragraphs\n",
    "for item in html.select('div.leak-content p'):\n",
    "    ## Append the content of the paragraph to the\n",
    "    ## list\n",
    "    article.append(item.text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print out the results\n",
    "article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "The task is to extract data about products from a simple mockup of an online shop. For every record create a dictionary of the following form:\n",
    "\n",
    "```python\n",
    "record = {\n",
    "    \"product\": <name of the product>,\n",
    "    \"price\": <product price>,\n",
    "    \"description\": <description>,\n",
    "    # Optionally, you can also try to extract\n",
    "    \"reviews\": <number of reviews>,\n",
    "    \"rating\": <rating score / number of stars>\n",
    "}\n",
    "```\n",
    "\n",
    "In other words, the task is to first create a selector that extracts product records. Then, loop over the records and extract data to create a list of records defined as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import modules\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "## Send the request\n",
    "response = requests.get(\"https://webscraper.io/test-sites/e-commerce/allinone/computers/tablets\")\n",
    "\n",
    "## Parse the context of the response into HTML\n",
    "html = BeautifulSoup(response.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE\n",
    "## Create a list of thumbnails\n",
    "thumbnails = html.select('div.container.test-site div.thumbnail')\n",
    "## See the structure of the first thumbnail\n",
    "thumbnails[0]\n",
    "\n",
    "## Create an empty list\n",
    "output = []\n",
    "\n",
    "## Iterate over all thumbnails\n",
    "for thumbnail in thumbnails:\n",
    "    ## Create an empty dictionary\n",
    "    temp_dict = {}\n",
    "    ## Extract the name of the item\n",
    "    temp_dict['name'] = thumbnail.select_one('a.title').text.strip()\n",
    "    ## Extract the price of the item\n",
    "    temp_dict['price'] = thumbnail.select_one('div.caption h4.pull-right.price').text.strip()\n",
    "    ## Extract the description of the item\n",
    "    temp_dict['description'] = thumbnail.select_one('div.caption p.description').text.strip()\n",
    "    ## Extract the number of stars. I count the length of the list in which each element is a star.\n",
    "    temp_dict['ratings'] = len(thumbnail.select('div.ratings p span.glyphicon.glyphicon-star'))\n",
    "    ## Extract the number of stars. I use the name of the attribute to select the p tag. Also,\n",
    "    ## I use the get method to extract its value\n",
    "    temp_dict['ratings2'] = thumbnail.select_one('div.ratings p[data-rating]').get('data-rating')\n",
    "    ## Extract the number of reviews. I am interested only in a number, not the 'reviews' word.\n",
    "    ## Therefore I split the string by space and take only the first element.\n",
    "    temp_dict['reviews'] = thumbnail.select_one('div.ratings p.review-count').text.strip().split()[0]\n",
    "    output.append(temp_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "The task is to extract data from **the first table** in a simple mockup of a webpage. For every row in the table create a dictionary of the following form:\n",
    "\n",
    "```python\n",
    "record = {\n",
    "    \"first_name\": <first name>,\n",
    "    \"last_name\": <last_name>,\n",
    "    \"username\": <username>\n",
    "}\n",
    "```\n",
    "\n",
    "First, you may try to extract data from all tables (or some subset). And then try to figure out what you can do to limit your results only to the first table.\n",
    "Remember that you can both modify your CSS selector or first extract all data and store it in a list (i.e. with `.select()` method)\n",
    "and then extract the part you need from the list with Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import modules\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "## Send the request\n",
    "response = requests.get(\"https://webscraper.io/test-sites/tables\")\n",
    "\n",
    "## Parse the content of the response into HTML\n",
    "html = BeautifulSoup(response.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE\n",
    "## Extract the HTML code for the first table\n",
    "table1 = html.select_one('div.container table.table.table-bordered')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create an empty list\n",
    "output = []\n",
    "## Iterate over rows of the first table\n",
    "for row in table1.select('tbody tr'):\n",
    "\t## Create an empty dictionary\n",
    "    temp_dict = {}\n",
    "\t## Extract the element of the second column\n",
    "    temp_dict['name'] = row.select('td')[1].text.strip()\n",
    "\t## Extract the element of the third column\n",
    "    temp_dict['surname'] = row.select('td')[2].text.strip()\n",
    "\t## Extract the element of the fourth column\n",
    "    temp_dict['username'] = row.select('td')[3].text.strip()\n",
    "    output.append(temp_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('ids')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "2f6b087e3237558552441618582c1cfff7dc162872ae65c00a668bb71df085d3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
