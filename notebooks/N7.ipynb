{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA (Latent Dirichlet Allocation) Model\n",
    "\n",
    "In simple terms Latent Dirichlet Allocation models are a way of automatically discovering topics in the given corpus. For example, imagine you have the following set of sentences (in real-world we would operate on documents that contain multiple sentences).\n",
    "\n",
    "* I like to eat broccoli and bananas.\n",
    "* I ate a banana and spinach smoothie for breakfast.\n",
    "* Chinchillas and kittens are cute.\n",
    "* My sister adopted a kitten yesterday.\n",
    "* Look at this cute hamster munching on a piece of broccoli.\n",
    "\n",
    "The LDA model returns more or less the following information about the probability to belong each sentence to a topic.\n",
    "\n",
    "* **Sentences 1 and 2**: 100% Topic A\n",
    "* **Sentences 3 and 4**: 100% Topic B\n",
    "* **Sentence 5**: 60% Topic A, 40% Topic B\n",
    "\n",
    "And the most representative words for a topic.\n",
    "* **Topic A**: 30% broccoli, 15% bananas, 10% breakfast, 10% munching, etc. (at which point, you could interpret topic A to be about food)\n",
    "* **Topic B**: 20% chinchillas, 20% kittens, 20% cute, 15% hamster, etc. (at which point, you could interpret topic B to be about cute animals)\n",
    "\n",
    "### This is fun but how does it happen?\n",
    "\n",
    "First, LDA makes assumptions about how the documents are created. It sees each document as a mixutere of topics that spit out words with certain probabilities. Therefore, it assumes that each document is created in the following fashion.\n",
    "\n",
    "1. Decide on the number of words `N` the document will have.\n",
    "2. Choose a topic mixture for the document. For example, assuming that we have the two food and cute animal topics above, you might choose the document to consist of $\\frac{1}{3}$ food and $\\frac{2}{3}$ cute animals.\n",
    "3. Generate each word $w_i$ in the document by:\n",
    "\t- First picking a topic (according to the multinomial distribution that you sampled above; for example, you might pick the food topic with 1/3 probability and the cute animals topic with 2/3 probability).\n",
    "\t- Using the topic to generate the word itself (according to the topic’s multinomial distribution). For example, if we selected the food topic, we might generate the word “broccoli” with 30% probability, “bananas” with 15% probability, and so on.\n",
    "\n",
    "Assuming this generative model for a collection of documents, LDA then tries to backtrack from the documents to find a set of topics that are likely to have generated the collection.\n",
    "\n",
    "### Example\n",
    "\n",
    "According to the above process, when generating some particular document D, you might\n",
    "\n",
    "1. Pick 5 to be the number of words in D.\n",
    "2. Decide that D will be 1/2 about food and 1/2 about cute animals.\n",
    "3. Pick the first word to come from the food topic, which then gives you the word `\"broccoli\"`.\n",
    "4. Pick the second word to come from the cute animals topic, which gives you `\"panda\"`.\n",
    "5. Pick the third word to come from the cute animals topic, giving you `\"adorable\"`.\n",
    "6. Pick the fourth word to come from the food topic, giving you `\"cherries\"`.\n",
    "7. Pick the fifth word to come from the food topic, giving you `\"eating\"`.\n",
    "\n",
    "So the document generated under the LDA model will be `\"broccoli panda adorable cherries eating\"` (note that LDA is a bag-of-words model).\n",
    "\n",
    "### Learning\n",
    "\n",
    "This is all very good, let's assume that we have generated a set of similar documents. You’ve chosen some fixed number of `K` topics to discover, and want to use LDA to learn the topic representation of each document and the words associated to each topic. How do you do this? One way (known as collapsed [Gibbs sampling](https://en.wikipedia.org/wiki/Gibbs_sampling)) is the following:\n",
    "\n",
    "Go through each document, and randomly assign each word in the document to one of the K topics. Notice that this random assignment already gives you both topic representations of all the documents and word distributions of all the topics (albeit not very good ones).\n",
    "\n",
    "So how to improve this topics?\n",
    "\n",
    "Go through each word `w` in `d` and for each topic `t`, compute two things: \n",
    "\n",
    "1) $p(topic_t | document_d)$ -- the proportion of words in document `d` that are currently assigned to topic `t`. \n",
    "2) $p(word_w | topic_t)$ -- the proportion of assignments to topic `t` over all documents that come from this word `w`. \n",
    "\n",
    "Reassign `w` a new topic, where we choose topic `t` with probability $p(topic_t | document_d) \\times p(word_w | topic_t)$ (according to our generative model, this is essentially the probability that topic `t` generated word `w`, so it makes sense that we resample the current word’s topic with this probability). In other words, in this step, we’re assuming that all topic assignments except for the current word in question are correct, and then updating the assignment of the current word using our model of how documents are generated.\n",
    "\n",
    "After repeating the previous step a large number of times, you’ll eventually reach a roughly steady state where your assignments are pretty good. So use these assignments to estimate the topic mixtures of each document (by counting the proportion of words assigned to each topic within that document) and the words associated to each topic (by counting the proportion of words assigned to each topic overall).\n",
    "\n",
    "\n",
    "### Real-world example\n",
    "\n",
    "This is all very good but how to do it in practice. The good news is that it is quite easy in _Python_. What we need is a corpus of texts. Let's use the same corpus we used in the article on [The effect of the pandemic on European narratives on smart cities and surveillance](https://doi.org/10.1177/00420980221138317). You can download it from this link from the [Google Drive](https://classroom.google.com/c/NjI5NzI5ODQxNDIw/m/NjU3NTM3NTU4MDg0/details)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import module for tokenization and lemmatization\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "## Import module for LDA\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "## Import the module for JSON handling\n",
    "import json\n",
    "## Import the module for path handling\n",
    "import os\n",
    "## Import module for creating a table\n",
    "import pandas as pd\n",
    "## Import module for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "## Import module for handling standard output\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCorpus:\n",
    "    \"\"\"\n",
    "    A class that represents a corpus and has usefull methods defined.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, path, key='content'):\n",
    "        \"\"\"\n",
    "        Reads from a JSON line file. Tokenizes and lemmatizes\n",
    "        the text under key. It writes out the new JSON line\n",
    "        file with a new field -- tokens.\n",
    "        Args:\n",
    "            path (str): a path to a JSON line.\n",
    "            key (str): a key with the content to lemmatize.\n",
    "        \"\"\"\n",
    "        self._path_original = path\n",
    "        self._key = key\n",
    "        self._dictionary = None\n",
    "        self._path = path.replace('.', '_NLP.')\n",
    "        with open(self._path, 'w') as file:\n",
    "            n = 1\n",
    "            for line in open(self._path_original, 'r'):\n",
    "                temp_dict = json.loads(line)\n",
    "                text_nlp = nlp(temp_dict[self._key])\n",
    "                temp_dict['tokens'] = []\n",
    "                for token in text_nlp:\n",
    "                    is_stop = token.is_stop or token.is_punct or token.is_space \\\n",
    "                        or token.is_bracket or token.is_currency or token.is_digit \\\n",
    "                            or token.is_quote or len(token) < 2\n",
    "                    if is_stop:\n",
    "                        continue\n",
    "                    else:\n",
    "                        temp_dict['tokens'].append(token.lemma_.lower())\n",
    "                file.write( json.dumps(temp_dict) + '\\n')\n",
    "                sys.stdout.write(f'\\rLine {n} processed')\n",
    "                n += 1\n",
    "                sys.stdout.flush()\n",
    "\n",
    "        \n",
    "    def set_dictionary(self, dictionary):\n",
    "        \"\"\"\n",
    "        Assigns a gensim.corpora.dictionary.Dictioanry object\n",
    "        to self._dictionary.\n",
    "\n",
    "        Args:\n",
    "            dictionary (gensim.corpora.dictionary.Dictionary): a dictionary\n",
    "            that stores the frequencies of unique tokens in the corpus.\n",
    "        \"\"\"\n",
    "        self._dictionary = dictionary\n",
    "\n",
    "    def get_tokens(self):\n",
    "        \"\"\"\n",
    "        It takes the path to a JSON line file with comments from Reddit and\n",
    "        returns a generator that yields tokens for each comment.\n",
    "\n",
    "        Yields:\n",
    "            list : list of tokens for a comment from Reddit. \n",
    "        \"\"\"\n",
    "        for doc in open(self._path, 'r'):\n",
    "            temp = json.loads(doc)\n",
    "            yield temp['tokens']\n",
    "    \n",
    "    def get_bow(self):\n",
    "        \"\"\"\n",
    "        It takes a dictionary with frequencies of unique tokens in the corpus\n",
    "        and for each list of tokens returns a list of tuples that denote the \n",
    "        id of a given token and its frequency in a given document.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: if the dictionary was not assigned to self._dictionary.\n",
    "\n",
    "        Yields:\n",
    "            list : a list of tuples that denote the id of a given token and its\n",
    "            frequency in a given document.\n",
    "        \"\"\"\n",
    "        if self._dictionary:\n",
    "            for doc in self.get_tokens():\n",
    "                yield self._dictionary.doc2bow(doc)\n",
    "        else:\n",
    "            raise ValueError('Dictionary has the value of None')\n",
    "    \n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Yields:\n",
    "            list : a list of tuples that denote the id of a given token and\n",
    "            its frequency in a given document.\n",
    "        \"\"\"\n",
    "        for doc in self.get_bow():\n",
    "            yield doc\n",
    "\n",
    "    def get_topics(self, model):\n",
    "        \"\"\"\n",
    "        It takes a model and returns a generator that yields a mapping for each\n",
    "        comment from Reddit. Among other keys it returns the most probable topic\n",
    "        based on the LDA model provided and its probability.\n",
    "\n",
    "        Args:\n",
    "            model (gensim.models.ldamodel.LdaModel): Latent Dirchlet Allocation\n",
    "            model.\n",
    "\n",
    "        Yields:\n",
    "            dict : a mapping for each comment from Reddit. Among other keys it\n",
    "            returns the most prpobable topic based on the LDA model provided and\n",
    "            its probability. \n",
    "        \"\"\"\n",
    "        for doc in open(self._path, 'r'):\n",
    "            temp = json.loads(doc)\n",
    "            topics = model.get_document_topics(self._dictionary.doc2bow(temp['tokens']))\n",
    "            topic, prob = sorted( topics, key = lambda x: x[1], reverse=True )[0]\n",
    "            temp['topic'] = topic + 1\n",
    "            temp['topic_prob'] = prob\n",
    "            yield temp\n",
    "\n",
    "                \n",
    "class MyModel(LdaModel):\n",
    "    \"\"\"\n",
    "    Subclass of gensim.models.LdaModel.\n",
    "    \"\"\"\n",
    "    def get_coherence(self, corpus):\n",
    "        \"\"\"\n",
    "        Returns the average coherence measure for the given model.\n",
    "\n",
    "        Args:\n",
    "            corpus (MyCorpus): A corpus on which the model is computed. \n",
    "\n",
    "        Returns:\n",
    "            float: the average coherence measure for the given model.\n",
    "        \"\"\"\n",
    "        top_topics = self.top_topics(corpus)\n",
    "        return sum([t[1] for t in top_topics]) / len(top_topics)\n",
    "    \n",
    "    def get_top_tokens(self, corpus):\n",
    "        \"\"\"\n",
    "        Returns a list of dictionaries that depict the most probable\n",
    "        tokens for each topic.\n",
    "\n",
    "        Args:\n",
    "            corpus (MyCorpus): A corpus on which the model was computed.\n",
    "\n",
    "        Returns:\n",
    "            list: list of dicitionaries that depict the most probable \n",
    "            tokens fro each topic.\n",
    "        \"\"\"\n",
    "        top_tokens = self.top_topics(corpus)\n",
    "        return [ { key : value for value, key in t[0] } for t in top_tokens ]\n",
    "\n",
    "    \n",
    "    \n",
    "        \n",
    "def run_lda_models(corpus, dictionary, min_topics, max_topics, step = 1, **kwargs):\n",
    "    \"\"\"\n",
    "    Computes a sequence of lda models for a given corpus and dictionary. It prints\n",
    "    the coherence measure and number of topics to the screen. It writes out the\n",
    "    model to disk.\n",
    "\n",
    "    Args:\n",
    "        corpus (MyModel): A stream of document vectors or sparse matrix of shape (num_documents, num_terms).\n",
    "        dictionary (dict): a mapping that assigns id to unique tokens from the corpus.\n",
    "        min_topics (int): the smallest number of topics to compute.\n",
    "        max_topics (int): the highest number of topics to compute.\n",
    "        step (int, optional): the size of the break inbetween computed models. Defaults to 1.\n",
    "    \"\"\"\n",
    "    name = input(\"Please provide the name of the model\\n\")\n",
    "    temp = dictionary[0]\n",
    "    id2word = dictionary.id2token\n",
    "    if not os.path.exists('models'):\n",
    "        os.mkdir('models')\n",
    "    if not os.path.exists('png'):\n",
    "        os.mkdir('png')\n",
    "    for num_topic in range(min_topics, max_topics+1, step):\n",
    "        model = MyModel( corpus = corpus,\n",
    "                         id2word=id2word,\n",
    "                         alpha = 'asymmetric',\n",
    "                         eta = 'auto',\n",
    "                         iterations = 500,\n",
    "                         passes = 20,\n",
    "                         eval_every=None,\n",
    "                         num_topics=num_topic,\n",
    "                         random_state=1044,\n",
    "                         per_word_topics=True)\n",
    "        temp_dict = {}\n",
    "        temp_dict['name'] = name\n",
    "        temp_dict['num_topics'] =  num_topic\n",
    "        temp_dict['coherence'] = model.get_coherence(corpus = corpus)\n",
    "        path_name = os.path.join('models', name + '-' + str(num_topic))\n",
    "        model.save(path_name) \n",
    "        print(temp_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the corpus of texts. First, we will need to preprocess it a bit. It will involve 3 steps (this is a very naive way of preprocessing the corpus but for our purposes, it is more than enough).\n",
    "\n",
    "1. Tokenize the text.\n",
    "2. Lemmatize the tokens.\n",
    "3. Compute the bag-of-words representation of the data.\n",
    "\n",
    "Afterward, we can compute the LDA Model that will reduce our data by grouping similar texts together. For example, instead of 184 articles this way we have 7 coherent groups. It allows our limited cognitive system to process, compare, or even draw conclusions about the main topics out there.\n",
    "\n",
    "4. Compute the LDA.\n",
    "5. Interpret the results.\n",
    "\n",
    "There is a very good tutorial on exactly this [here](https://radimrehurek.com/gensim/auto_examples/tutorials/run_lda.html#sphx-glr-auto-examples-tutorials-run-lda-py). However, its main issue is that it is kind of 'raw' and provides some details that can put you off because they require additional _Python_ knowledge. Therefore, in the code below I minimized the number of unnecessary details and focused on the most important parts. The code below uses the classes and functions defined in the chunk above. Don't be overwhelmed by it. For most of the uses, you can use it as a script that computes LDA for a corpus in a JSON line file.\n",
    "\n",
    "**IMPORTANT**: By default, the `content` field stores the text you would like to first tokenize and later lemmatize. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read the corpus from the file\n",
    "corpus = MyCorpus(path = 'LDA.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the dictionary\n",
    "dictionary = Dictionary( corpus.get_tokens() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Filter out words that occur less than in 5 documents, or more than 50% of the documents\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add the dictionary to the corpus\n",
    "corpus.set_dictionary(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute models and write them out to the files\n",
    "run_lda_models(corpus = corpus, dictionary = dictionary, min_topics=3, max_topics=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read in the model. It requires providing\n",
    "## the name of the model we want to load.\n",
    "model_name = input('Provide the name of the model you would like to load:\\n')\n",
    "model_path = os.path.join('models', model_name)\n",
    "model = LdaModel.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print out and write the figures with the most \n",
    "## probable tokens in each topic.\n",
    "list_top_tokens = model.get_top_tokens(corpus)\n",
    "for i in range(len(list_top_tokens)):\n",
    "    plt.barh(list(list_top_tokens[i].keys()), list(list_top_tokens[i].values()), align = 'center')\n",
    "    plt.xlim(0,.03)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.title('Topic' + ' ' + str(i + 1))\n",
    "    plt.xlabel('Probability')\n",
    "    plt.savefig('png/' + 'topic' + str(i + 1))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write out the results into a CSV file\n",
    "pd.DataFrame.from_records(line for line in corpus.get_topics(model = model)).to_excel('corpus.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('ids')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2f6b087e3237558552441618582c1cfff7dc162872ae65c00a668bb71df085d3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
