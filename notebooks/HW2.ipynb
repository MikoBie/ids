{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework (deadline 10.01.2025 11:59:59)\n",
    "\n",
    "Write solutions for the homework exercises in this notebook. Once the work is done download the notebook file (`File > Download .ipynb`) rename it properly so it follows a template `HW2_<SURNAME>_<NAME>.ipynb` and upload it to the [Google Classroom](https://classroom.google.com/c/NzIwNDg4NDAyMTA2/a/NzIwNDg4NDAyMTQz/details).\n",
    "\n",
    "Remember that you can contact me via email if you have any problems. Moreover, you can also visit me in the ISS on the fourth floor (room 415). Usually, I am there from 11ish but please let me know in advance if you are coming because I might be busy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 (15 points)\n",
    "You all should be able to get the file `links.jsonl` from the [Google Classrom](https://drive.google.com/file/d/1f7mPwNCuihV-sogjIdFO9U6BS_ZW6ioM/view). It contains 20 links to The Parliament Magazine and The Guardian articles. Your task is to:\n",
    "\n",
    "* Read `links.jsonl` to Python (Google Colab)\n",
    "* Scrap the articles from the given links\n",
    "* Write out the results into `HW2_<SURNAME>_<NAME>.jsonl`\n",
    "\n",
    "From every website you should get the following information:\n",
    "\n",
    "* Title of the article\n",
    "* Author of the text (if present, if not the value should be None)\n",
    "* Lead of the article (summary)\n",
    "* Date of publication\n",
    "* Text of the article\n",
    "\n",
    "Additionally, you should count the number of words in the given article and extract only relevant information from the `links.jsonl`.\n",
    "\n",
    "*Reminder:* A JSON line file contains multiple JSONs. As the name suggests each line contains one. Therefore, each line should look more or less like that:\n",
    "\n",
    "```python\n",
    "{ \n",
    "\t\"title\" : \"Why are Namedays better than Birthdays?\",\n",
    "    \"author\" : \"M. Biesaga\",\n",
    "    \"date\" : \"06.12.2019\",\n",
    "    \"lead\" : \"Scientists from one of the best Universities in the U.S. proved that the discussion about birthdays and namedays is finally over.\",\n",
    "    \"content\" : \"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer eget sapien nisi. In placerat nisl felis, vel porttitor odio aliquam quis. Nulla a facilisis arcu. Suspendisse potenti. Interdum et malesuada fames ac ante ipsum primis in faucibus. Duis et semper urna. Mauris sit amet enim ex. Integer eget ultricies enim, at tristique sem. In eu eros nisl. Nulla vitae pretium risus, quis vestibulum dui. Nullam vitae dapibus quam. Maecenas commodo dictum ex, id vestibulum ex volutpat interdum. Cras tempor diam non urna auctor, vitae dignissim tortor tristique. Nunc consectetur mauris non lorem luctus aliquam. Mauris vitae ligula orci.\",\n",
    "\t\"source\" : \"Journal of Scientific Science\",\n",
    "\t\"fb\" : { \n",
    "\t\t        \"likes\" : 112,\n",
    "\t            \"shares\" : 2,\n",
    "\t\t\t    \"comments\" : 43\n",
    "\t},\n",
    "\t\"length\" : 98 \n",
    "}\n",
    "```\n",
    "**IMPORTANT**: If you are going to send requests in a loop please don't send more than 1 request per second. You can use `time.sleep(1)` from the `time` module to halt your code for 1 second. For example, in a loop it would look something like that:\n",
    "```python\n",
    "for line in links:\n",
    "\t## Send request\n",
    "\ttime.sleep(1)\n",
    "```\n",
    "It will perform the code in a loop and at the end of each iteration, it will wait for a second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import modules\n",
    "import json\n",
    "import requests as rq\n",
    "from bs4 import BeautifulSoup\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read links.jsonl to Python\n",
    "with open(\"links.jsonl\", \"r\") as file:\n",
    "    links = [json.loads(line) for line in file.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create an empty list\n",
    "output = []\n",
    "\n",
    "## Iterate over all elements of the links list\n",
    "for line in links:\n",
    "    ## Create an empty dictionary\n",
    "    temp_dict = {}\n",
    "    temp_dict[\"source\"] = line[\"source_name\"]\n",
    "    temp_dict[\"fb\"] = {}\n",
    "    temp_dict[\"fb\"][\"likes\"] = line[\"fb\"][\"likes\"]\n",
    "    temp_dict[\"fb\"][\"comments\"] = line[\"fb\"][\"comments\"]\n",
    "    temp_dict[\"fb\"][\"shares\"] = line[\"fb\"][\"shares\"]\n",
    "\n",
    "    ## Send a request\n",
    "    response = rq.get(line[\"url\"])\n",
    "\n",
    "    ## Convert the response to an HTML object\n",
    "    html = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    if temp_dict[\"source\"] == \"The Parliament Magazine\":\n",
    "        temp_dict[\"title\"] = html.select_one(\"div.av-title h1\").text.strip()\n",
    "        ## I don't extract text from the HTML object because I wll do it later for\n",
    "        ## all sources.\n",
    "        temp_dict[\"author\"] = html.select_one(\"p.av-authName a\")\n",
    "        temp_dict[\"lead\"] = html.select_one(\"div.av-standFirst.playfair\").text.strip()\n",
    "        temp_dict[\"date\"] = html.select_one(\"p.av-date\").text.strip()\n",
    "        ## Extract the content of the article in the list comprehension.\n",
    "        temp_dict[\"content\"] = \" \".join(\n",
    "            p.text.strip() for p in html.select(\"div.av-main p\")\n",
    "        )\n",
    "    elif temp_dict[\"source\"] == \"The Guardian\":\n",
    "        temp_dict[\"title\"] = html.select_one(\"article div h1\").text.strip()\n",
    "        ## I don't extract text from the HTML object because I wll do it later for\n",
    "        ## all sources. In The Guardian, one article would return None because\n",
    "        ## apparently it does not have an author. If I used the strip() method on\n",
    "        ## the None value it would return an error.\n",
    "        temp_dict[\"author\"] = html.select_one(\"address div a\")\n",
    "        temp_dict[\"date\"] = html.select_one(\"details summary span\").text.strip()\n",
    "        temp_dict[\"lead\"] = html.select_one(\"div p\").text.strip()\n",
    "        temp_dict[\"content\"] = \" \".join(\n",
    "            p.text.strip() for p in html.select(\"article div#maincontent p\")\n",
    "        )\n",
    "    ## Here, I check whether the author field is None. If not\n",
    "    ## I extract the name of the author.\n",
    "    if temp_dict[\"author\"] is not None:\n",
    "        temp_dict[\"author\"] = temp_dict[\"author\"].text.strip()\n",
    "    ## Compute the length of the article.\n",
    "    temp_dict[\"length\"] = len(temp_dict[\"content\"].split())\n",
    "\n",
    "    output.append(temp_dict)\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"HW2_Biesaga_Miko≈Çaj.jsonl\", \"w\") as file:\n",
    "    for line in output:\n",
    "        file.write(json.dumps(line) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from newspaper import Article\n",
    "\n",
    "## Open connection with the output file.\n",
    "with open(\"HW2_Biesaga_Mikolaj.jsonl\", \"w\") as write_file:\n",
    "    ## Open connection with the file with links.\n",
    "    with open(\"links.jsonl\", \"r\") as read_file:\n",
    "        ## Iterate over every single line of the file.  # Note that this method\n",
    "        ## reads a single line at the time. So I never have more than one line\n",
    "        ## in the working memory. Unlike file.readlines() that stores all lines\n",
    "        ## in the working memory.\n",
    "        for line in read_file:\n",
    "            ## Convert a string into a dictionary.\n",
    "            line = json.loads(line)\n",
    "            ## Create an empty dictionary.\n",
    "            temp_dict = {}\n",
    "            ## Initialize Article object with the link.\n",
    "            art = Article(line[\"url\"])\n",
    "            ## Download the website's HTML.\n",
    "            art.download()\n",
    "            ## Parse HTML code.\n",
    "            art.parse()\n",
    "            ## Get the main text of the article.\n",
    "            temp_dict[\"text\"] = art.text_cleaned\n",
    "            ## Get the length of the article.\n",
    "            temp_dict[\"length\"] = len(art.text_cleaned.split())\n",
    "            ## Extract the publication date and convert it into a string.\n",
    "            temp_dict[\"date\"] = art.publish_date.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            ## Get the tile of the article.\n",
    "            temp_dict[\"title\"] = art.title\n",
    "            ## Get the source name.\n",
    "            temp_dict[\"source\"] = line[\"source_name\"]\n",
    "            ## Convert a string to an HTML object\n",
    "            html = BeautifulSoup(art.html, \"html.parser\")\n",
    "            ## Extract author and lead similarly to the first approach.\n",
    "            if line[\"source_name\"] == \"The Parliament Magazine\":\n",
    "                temp_dict[\"author\"] = html.select_one(\"p.av-authName a\")\n",
    "                temp_dict[\"lead\"] = html.select_one(\n",
    "                    \"div.av-standFirst.playfair\"\n",
    "                ).text.strip()\n",
    "            else:\n",
    "                temp_dict[\"author\"] = html.select_one(\"address div a\")\n",
    "                temp_dict[\"lead\"] = html.select_one(\"div p\").text.strip()\n",
    "            if temp_dict[\"author\"]:\n",
    "                temp_dict[\"author\"] = temp_dict[\"author\"].text.strip()\n",
    "            ## Extract data about Facebook likes\n",
    "            temp_dict[\"fb\"] = {\n",
    "                \"likes\": line[\"fb\"][\"likes\"],\n",
    "                \"shares\": line[\"fb\"][\"shares\"],\n",
    "                \"comments\": line[\"fb\"][\"comments\"],\n",
    "            }\n",
    "            ## Write out the data to the file\n",
    "            write_file.write(json.dumps(temp_dict) + \"\\n\")\n",
    "            ## Wait a second\n",
    "            time.sleep(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ids",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
