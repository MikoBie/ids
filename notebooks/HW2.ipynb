{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework (deadline 23.12.2022 23:59:59)\n",
    "\n",
    "Write solutions for the homework exercises in this notebook. Once the work is done download the notebook file (`File > Download .ipynb`) rename it properly so it follows a template `HW2_<SURNAME>_<NAME>.ipynb` and send the file to me. **Please also attach all files you will be asked to produce in this homework**. My email address is as follows: \n",
    "\n",
    "* <m.biesaga@uw.edu.pl>\n",
    "\n",
    "Remember that you can contact me via email if you have any problems. Moreover, you can also visit me in the ISS on the fourth floor (room 415). Usually, I am there from 11ish but please let me know in advance if you are coming because I might be busy. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 (15 points)\n",
    "You all should be able to get the file `links.jl` on our [Google Drive](https://drive.google.com/file/d/11m7TWX4w1T5UfhvPzTrLFlU-wOhWc5A0/view?usp=sharing). It contains 30 links to EURACTIV, The Guardian, and The Parliament Magazine articles. Your task is to:\n",
    "\n",
    "* Read `links.jl` to Python (Google Colab)\n",
    "* Scrap the articles from the given links\n",
    "* Write out the results into `HW2_<SURNAME>_<NAME>.jl`\n",
    "\n",
    "From every website you should get the following information:\n",
    "\n",
    "* Title of the article\n",
    "* Author of the text (if present, if not the value should be None)\n",
    "* Lead of the article (summary)\n",
    "* Date of publication\n",
    "* Text of the article\n",
    "\n",
    "Additionally, you should count the number of words in the given article and extract only relevant information from the `links.jl`.\n",
    "\n",
    "*Reminder:* A JSON line file contains multiple JSONs. As the name suggests each line contains one. Therefore, each line should look more or less like that:\n",
    "\n",
    "```python\n",
    "{ \n",
    "\t\"title\" : \"Why are Namedays better than Birthdays?\",\n",
    "    \"author\" : \"M. Biesaga\",\n",
    "    \"date\" : \"06.12.2019\",\n",
    "    \"lead\" : \"Scientists from one of the best Universities in the U.S. proved that the discussion about birthdays and namedays is finally over.\",\n",
    "    \"content\" : \"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer eget sapien nisi. In placerat nisl felis, vel porttitor odio aliquam quis. Nulla a facilisis arcu. Suspendisse potenti. Interdum et malesuada fames ac ante ipsum primis in faucibus. Duis et semper urna. Mauris sit amet enim ex. Integer eget ultricies enim, at tristique sem. In eu eros nisl. Nulla vitae pretium risus, quis vestibulum dui. Nullam vitae dapibus quam. Maecenas commodo dictum ex, id vestibulum ex volutpat interdum. Cras tempor diam non urna auctor, vitae dignissim tortor tristique. Nunc consectetur mauris non lorem luctus aliquam. Mauris vitae ligula orci.\",\n",
    "\t\"source\" : \"Journal of Scientific Science\",\n",
    "\t\"fb\" : { \n",
    "\t\t        \"likes\" : 112,\n",
    "\t            \"shares\" : 2,\n",
    "\t\t\t    \"comments\" : 43\n",
    "\t},\n",
    "\t\"length\" : 98 \n",
    "}\n",
    "```\n",
    "**IMPORTANT**: If you are going to send requests in a loop please don't send more than 1 request per second. You can use `time.sleep(1)` from the `time` module to halt your code for 1 second. For example, in a loop it would look something like that:\n",
    "```python\n",
    "for line in links:\n",
    "\t## Send request\n",
    "\ttime.sleep(1)\n",
    "```\n",
    "It will perform the code in a loop and at the end of each iteration, it will wait for a second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests as rq\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "## YOUR CODE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('links.jl', 'r') as file:\n",
    "    df = [ json.loads(line) for line in file.readlines() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = []\n",
    "\n",
    "for line in df:\n",
    "  temp_dict ={}\n",
    "  temp_dict['source_name'] = line['source_name']\n",
    "  temp_dict['fb'] = {}\n",
    "  temp_dict['fb']['likes'] = line['fb']['likes']\n",
    "  temp_dict['fb']['shares'] = line['fb']['shares']\n",
    "  temp_dict['fb']['comments'] = line['fb']['comments']\n",
    "  \n",
    "  response = rq.get(line['url'])\n",
    "  \n",
    "  html = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "\n",
    "  if temp_dict['source_name'] == 'EURACTIV':\n",
    "    temp_dict['title'] = html.select_one('section.ea-post-title').text.strip()\n",
    "\t  ## I don't extract text from the HTML object because I wll do it later for\n",
    "    ## all sources.\n",
    "    temp_dict['author'] = html.select_one('a.author')\n",
    "    temp_dict['lead'] = ''\n",
    "    temp_dict['date'] = html.select_one('span.ea-dateformat').text.strip()\n",
    "\t  ## Extract the content of the article in the list comprehension.\n",
    "    temp_dict['content'] = ' '.join( p.text.strip() for p in html.select('article div.ea-article-body-content p') )\n",
    "  elif temp_dict['source_name'] == 'The Guardian':\n",
    "    temp_dict['title'] = html.select_one('article div h1').text.strip()\n",
    "    ## I don't extract text from the HTML object because I wll do it later for\n",
    "    ## all sources. In The Guardian, one article would return None because\n",
    "    ## apparently it does not have an author. If I used the strip() method on\n",
    "    ## the None value it would return an error.\n",
    "    temp_dict['author'] = html.select_one('address div a')\n",
    "    temp_dict['date'] = html.select_one('details summary span').text.strip()\n",
    "    temp_dict['lead'] = html.select_one('div p').text.strip()\n",
    "    temp_dict['content'] = ' '.join( p.text.strip() for p in html.select('article div#maincontent p') )\n",
    "  else:\n",
    "    temp_dict['title'] = html.select_one('div.av-title h1').text.strip()\n",
    "    ## I don't extract text from the HTML object because I will do it later for\n",
    "    ## all sources.\n",
    "    temp_dict['author'] = html.select_one('main p.av-authName a')\n",
    "    temp_dict['date'] = html.select_one('div.av-authInfo p.av-date').text.strip()\n",
    "    temp_dict['lead'] = html.select_one('div.av-standFirst.playfair').text.strip()\n",
    "    temp_dict['content'] = ' '.join( p.text.strip() for p in html.select('div.container div.av-main p') )\n",
    "    ## Here, I check whether the author field is None. If not\n",
    "    ## I extract the name of the author.\n",
    "  if temp_dict['author'] is not None:\n",
    "    temp_dict['author'] = temp_dict['author'].text.strip()\n",
    "  ## Compute the length of the article.\n",
    "  temp_dict['length'] = len(temp_dict['content'].split())\n",
    "\n",
    "  output.append(temp_dict)\n",
    "  time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('HW2_Biesaga_Mikolaj.jl', 'w') as file:\n",
    "    for line in output:\n",
    "        file.write( json.dumps(line) + '\\n' )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Open connection with the output file.\n",
    "with open('HW2_Biesaga_Mikolaj.jl', 'w') as write_file:\n",
    "    ## Open connection with the file with links.\n",
    "    with open('links.jl', 'r') as read_file:\n",
    "        ## Iterate over every single line of the file.  # Note that this method\n",
    "        ## reads a single line at the time. So I never have more than one line\n",
    "        ## in the working memory. Unlike file.readlines() that stores all lines\n",
    "        ## in the working memory.\n",
    "        for line in read_file:\n",
    "            ## Convert a string into a dictionary.\n",
    "            line = json.loads(line)\n",
    "            ## Create an empty dictionary.\n",
    "            temp_dict = {}\n",
    "            ## Send a request and convert it to HTML code.\n",
    "            html = BeautifulSoup(rq.get(line['url']).content, 'html.parser')\n",
    "            ## Extract the title of the articles. I've noticed that all pages\n",
    "            ## have the titles in the head under meta tag. The only difference\n",
    "            ## between web pages is that the attribute sometimes is called name\n",
    "            ## other times property. Therefore I use this vertical line that\n",
    "            ## indicates or. In other words, in this line, I am looking for a tag\n",
    "            ## called meta with either name or property attribute equal to\n",
    "            ## \"og:title\". In both cases the title of the article is the value of\n",
    "            ## the content attribute.\n",
    "            temp_dict['title'] = html.select_one('meta[name|property=\"og:title\"]').get('content')\n",
    "            ## Extract the description of the article. It is a similar case to\n",
    "            ## extracting the title.\n",
    "            temp_dict['lead'] = html.select_one('meta[name|property=\"og:description\"]').get('content')\n",
    "            ## Extract the published date. This time it is easier because the\n",
    "            ## name of the attribute is always the same.\n",
    "            temp_dict['published_at'] = html.select_one('meta[property=\"article:published_time\"]').get('content')\n",
    "            ## Copy the name of the webpage from the links file.\n",
    "            temp_dict['source_name'] = line['source_name']\n",
    "            ## Both the author field and content field for these three sources\n",
    "            ## will have different tags. Therefore, I use the if-statement.\n",
    "            if temp_dict['source_name'] == 'The Guardian':\n",
    "                ## In that case, there is no point for me to extract from the\n",
    "                ## HTML code the text inside the if-statement because this field\n",
    "                ## might be empty. I will deal with it later.\n",
    "                temp_dict['author'] = html.select_one('main a[rel=\"author\"]')\n",
    "                ## Extract the content of the webpage and convert it into a string. \n",
    "                temp_dict['content'] = ' '.join( p.text.strip() for p in html.select('article div#maincontent p') )\n",
    "            elif temp_dict['source_name'] == 'EURACTIV':\n",
    "                ## In that case, there is no point for me to extract from the\n",
    "                ## HTML code the text inside the if-statement because this field\n",
    "                ## might be empty. I will deal with it later.\n",
    "                temp_dict['author'] = html.select_one('article a.author')\n",
    "                ## Extract the content of the webpage and convert it into a string. \n",
    "                temp_dict['content'] = ' '.join( p.text.strip() for p in html.select('article div.ea-article-body-content p') )\n",
    "            else:\n",
    "                ## In that case, there is no point for me to extract from the\n",
    "                ## HTML code the text inside the if-statement because this field\n",
    "                ## might be empty. I will deal with it later.\n",
    "                temp_dict['author'] = html.select_one('main p.av-authName a')\n",
    "                ## Extract the content of the webpage and convert it into a string. \n",
    "                temp_dict['content'] = ' '.join( p.text.strip() for p in html.select('div.container div.av-main p') )\n",
    "    \n",
    "    \t\t## Compute the length of the article. It is not a perfect solution\n",
    "    \t\t## but it is good enough for our case.\n",
    "            temp_dict['length'] = len(temp_dict['content'].split())\n",
    "            ## Check whether the author field is not None\n",
    "            if temp_dict['author'] is not None:\n",
    "                ## Extract from the HTML code the name of the author\n",
    "                temp_dict['author'] = temp_dict['author'].text.strip()  \n",
    "            ## Extract data about Facebook likes\n",
    "            temp_dict['fb'] = { 'likes' : line['fb']['likes'],\n",
    "                                 'shares' : line['fb']['shares'],\n",
    "                                 'comments' : line['fb']['comments']\n",
    "            }\n",
    "            ## Write out the data to the file\n",
    "            write_file.write(json.dumps(temp_dict) + '\\n')\n",
    "            ## Wait a second\n",
    "            time.sleep(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('ids')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2f6b087e3237558552441618582c1cfff7dc162872ae65c00a668bb71df085d3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
