{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LrC9U1ymAb5p"
   },
   "source": [
    "# Web APIs and HTTP requests\n",
    "\n",
    "In a nutshell, web APIs are publicly (usually; there is plenty of private APIs, but for obvious reasons, we do not care about them as we can not use them) available interfaces through which third parties (this is us!) can access some data resources in a remote, reliable and programmable manner.\n",
    "\n",
    "What does it mean in practice?\n",
    "\n",
    "* **Remote.** Users can access the resource from anywhere, provided they have an internet connection.\n",
    "* **Reliable.** The interface exposed to users is independent of the internal details of the system that produces the data. In other words, the way a user communicates with the API is independent of the way the system works. In practice it means that a user does not have to know anything about the system, it is enough to know the API interface.\n",
    "* **Programmable.** API can be interacted with based on a predefined set of commands/methods (an interface) in a way that can be expressed with a programming language. This is usually achieved by using HTTP protocol which is a standard communication protocol in the Web and for which utilities are available in any major programming language.\n",
    "\n",
    "## Practical example -- Wikipedia\n",
    "\n",
    "<center><img src=\"png/wiki_logo.png\"/></center>\n",
    "\n",
    "But what does that all really mean? Let's talk about [practice](https://youtu.be/eGDBR2L5kzI). We will use the public API of Wikipedia (we all know what it is, right?). Public Wikipedia API can be used for many purposes, but it also makes publicly available a lot (in fact almost all) of data that is stored within Wikipedia, such as page statistics, registered users, etc. We mentioned that in some sense an API is an interface that allows third parties to communicate with and requests various thing from some platform in an orderly and programmable manner. Let us now see a real example of such an interface.\n",
    "\n",
    "Wikipedia API (for English Wikipedia) lives at this URL:\n",
    "\n",
    "* [https://en.wikipedia.org/w/api.php](https://en.wikipedia.org/w/api.php)\n",
    "\n",
    "The URL takes us to an ugly webpage that contains documentation on all so-called API endpoints exposed by the Wikipedia API. What are they? Endpoints are *commands/requests* that the API understands and that can be used to extract some data from it. They define exactly the interface through which one can communicate with some external system via API. To sum up, an API understood as an interface is:\n",
    "\n",
    "* a publicly available *place* on the internet (associated with a particular URL)\n",
    "* a set of endpoints (commands) that define possible interactions with the API.\n",
    "\n",
    "Ok, so we have seen that the Wikipedia API lives at a particular URL. However, the URL by itself just leads us to documentation describing all the endpoints. So how can we use a particular endpoint to actually do something? Let us inspect endpoint called [query](https://en.wikipedia.org/w/api.php?action=help&modules=query)\n",
    "\n",
    "`https://en.wikipedia.org/w/api.php?action=help&modules=query`\n",
    "\n",
    "Now we see the documentation for the endpoint `query`. It is quite complex as it kind of defines another nested API within the top API. From now on we will work exclusively with this part of the Wikipedia API since this is the one we have to use to extract data from Wikipedia. Let us now note that the URL has already a particular form:\n",
    "\n",
    "`<URL>?<key-value pair>&<key-value pair> ...`\n",
    "\n",
    "The part after the `?` sign is crucial here as it defines a so-called query string that can be passed with an URL. A query string does not specify a different location (like a URL does), instead, it attaches some additional data to a request sent to a location specified in the standard `<URL>` part. This is additional data is crucial here since it allows us to communicate with APIs through the HTTP protocol. Now it is clear that `https://en.wikipedia.org/w/api.php?action=help&modules=query` is still the same address as `https://en.wikipedia.org/w/api.php` but enhanced with additional data that told the Wikipedia API to take as to the help page of the module (endpoint) `query`. So let us now try to finally do something useful.\n",
    "\n",
    "### Extracting list of Wikiprojects from Wikipedia API\n",
    "\n",
    "Now from the docs of the `query` endpoint, we select the `projects` [(sub)endpoint](https://en.wikipedia.org/w/api.php?action=help&modules=query%2Bprojects). The documentation gives us instructions on how to use the endpoint as well as some usage examples. When we click the link from the first example we see a long list of project names. These are so-called *Wikiprojects* which are registered semi-official groups of editors dedicated to working on a specific topic/theme. They can give us some basic insight into what kinds of topics are of most interest to Wikipedia editors (but do not base any claims solely on this simple information!)\n",
    "\n",
    "The URL from the first example looks like this:\n",
    "\n",
    "`api.php?action=query&list=projects`\n",
    "\n",
    "Again, it has the URL part (some of it omitted) and the query string part that specify that we use the `query` endpoint and ask it to list all the projects. This is great! We can look at the list in our browsers. However, even this list is somewhat too long to deal with it like this, so we would like to process it in Python.\n",
    "\n",
    "### Talking to API from Python\n",
    "\n",
    "Fortunately, Python makes it very easy to build HTTP requests and talk to an API. Utilities for this kind of work can be found in the `requests` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p8g4YVY5R7Ue"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "## From now on we can refer to the `requests` module\n",
    "## by its name (it is saved as a variable!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0FRVoz4jdWYv"
   },
   "source": [
    "How can we use it to get some data from an API? Let us decompose this problem into several steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GOCUEfBGc0Fi"
   },
   "outputs": [],
   "source": [
    "## First define the base API url\n",
    "URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "## Then define the query string parameters we want to pass with our request\n",
    "## It is often called the 'payload'\n",
    "payload = {\n",
    "    'action': 'query',\n",
    "    'list': 'projects',\n",
    "    'format': 'json'\n",
    "}\n",
    "## The 'requests' package wants us to define the payload as a dict\n",
    "## since this makes it easy to build GET requests dynamically\n",
    "## in a program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "sC1g5TOQeqtP",
    "outputId": "3f45a357-7680-45a8-ad9b-4aa903c6f7ab"
   },
   "outputs": [],
   "source": [
    "## Now with the URL and the payload ready we can send a request\n",
    "## to the Wikipedia API to kindly ask for the list of projects\n",
    "## It is as simple as this\n",
    "response = requests.get(URL, params=payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's see the results\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a bit underwhelming because apart from the status code we did not get anything. Therefore, we need to examine the object we got. For that, we will use the `dir()` function. In simple terms, it returns a scope of an object (what is inside it). In the terms of a module, it would return all methods (functions) included in the module. In our case, it will tell us what we can get from the response object apart from the status code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "iwgmEk4ke4oI",
    "outputId": "f3af5744-7b8e-48c1-8ccc-9d4bfc55ce88"
   },
   "outputs": [],
   "source": [
    "## By a time-honored tradition of a countless generation of computer\n",
    "## scientists we call the result of a web request a 'response'\n",
    "## Now we would like to extract the actual data from it\n",
    "data = response.json()\n",
    "## What do we have in the response?\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "h5anqKZEfQOh",
    "outputId": "ce8609f5-c977-429c-9243-049cab59c17d"
   },
   "outputs": [],
   "source": [
    "## Probably we want to focus on the 'query' part\n",
    "data['query'].keys()\n",
    "## And here are the projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "VSKp6E9SgJMV",
    "outputId": "d89b1798-d4a4-4ad1-bde8-8513a220817b"
   },
   "outputs": [],
   "source": [
    "## Now let us save the projects in a variable\n",
    "## to save us some typing\n",
    "projects = data['query']['projects']\n",
    "## Now we can easily count the projects\n",
    "len(projects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 201
    },
    "colab_type": "code",
    "id": "IAFDoUWLgd07",
    "outputId": "754f7d6b-223f-4d4e-e952-5c32cb27f777"
   },
   "outputs": [],
   "source": [
    "# and see them if we like ...\n",
    "# ... but maybe not all of them at one\n",
    "# maybe just first ten\n",
    "projects[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 201
    },
    "colab_type": "code",
    "id": "b0-ssAjvgk1a",
    "outputId": "9a0b9fca-b0e7-41d9-862e-ebc393a3cdcc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# and last ten\n",
    "projects[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Wikipedia API: part II\n",
    "\n",
    "Above we learnt how to extract names of the projects from the Wikipedia API. This time we will try to do something a little bit more involved.\n",
    "\n",
    "1. We gonna take a random sample of 10 Wikipedia articles. There is an endpoint in Wikipedia for doing just that. However, note because of the sampling each of you will get different results.\n",
    "2. The first step will give us only id numbers and the title of the pages. We will use them to extract the full text of the pages via a different endpoint of the Wikipedia API.\n",
    "3. We will compute the word length distributions of the pages.\n",
    "\n",
    "## Step 1.\n",
    "\n",
    "First, we have to sample 10 random Wikipedia articles. This should not be too hard since we have a special method for this, so it should be just one simple API call. The method we are looking for is `list=random` and it is defined within the `query` endpoint (`action=query`). We can read more about it [here](https://en.wikipedia.org/w/api.php?action=help&modules=query%2Brandom). **HINT.** Remember that you can view the results of your queries directly in the browser.\n",
    "\n",
    "A quick read of the doc page and we can decide that we need only two query parameters:\n",
    "\n",
    "1. `rnamespace=0` (which limits the results to the namespace `0` which is the part of Wikipedia where actual encyclopedic articles live).\n",
    "2. `rnlimit=10` (because we want to extract only 10 random articles)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The above considerations lead us to the following payload\n",
    "## we will want to attach to out query URL.\n",
    "\n",
    "payload = {\n",
    "    'action': 'query',  ## since we want to use the `query` endpoint\n",
    "    'list': 'random',   ## because we want to use the `random` method\n",
    "    ## But we also need to add arguments for the `random` method\n",
    "    'rnnamespace': 0,\n",
    "    'rnlimit': 10,\n",
    "    'format': 'json'    ## we need to add it so the data can be read by Python\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now we are ready to make the GET request\n",
    "## But first we need to import the requests package\n",
    "import requests as rq\n",
    "## And define our base URL\n",
    "BASE_URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "response = rq.get(BASE_URL, params=payload)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We see that our response is OK (HTTP response code 200 means 'OK')\n",
    "## So we can extract the data from the response object with `.json()`\n",
    "## method defined on it.\n",
    "data = response.json()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so now we have titles of articles and their unique ids. As you can probably imagine we are only interested in the unique ids at this point. Because we will need them to in different endpoint to extract texts of the articles. But how exactly we are going to access them? It is a dictionary so it should not be a major problem to access a single value, right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['query']['random'][0]['id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what exactly happened there? First, we got a mapping with three keys: `batchcomplete`, `continue`, and `query`. We were only interested in the `query` field. Therefore, we typed as follows:\n",
    "```python\n",
    "data['query']\n",
    "```\n",
    "However, it was again a mapping inside a mapping with only one field: `random`. Therefore:\n",
    "```python\n",
    "data['query']['random']\n",
    "```\n",
    "Inside this mapping, we had a ten-element list. So to access the first element we typed:\n",
    "```python\n",
    "data['query']['random'][0]\n",
    "```\n",
    "Every element of that list was also a mapping again with three keys: `id`, `ns`, and `title`. We were only interested in the `id` field. So, we just typed:\n",
    "```python\n",
    "data['query']['random'][0]['id']\n",
    "```\n",
    "However, again we could access all the `ids` manually but it would be easier just to use a `for-loop`. As you probably can imagine we are going to loop over that list because the rest of the fields are going to be the same.\n",
    "\n",
    "```python\n",
    "for page in data['query']['random']:\n",
    "    print(page['id'])\n",
    "```\n",
    "So a loop like this would work fine if we only wanted to print the `ids`. We could even modify it a bit to store the `ids` in the list (it is what we want to do), for example:\n",
    "```python\n",
    "list_ids = []\n",
    "for page in data['query']['random']:\n",
    "    list_ids.append(page['id'])\n",
    "```\n",
    "So first, we would create a list outside of the loop and then use a method `append` to add each value of `page['id']` as the last element of the list. It is doable. But Python offers a smarter way of saving results of the loop in a list. It is called **list comprehension** and in this particular example looks like this:\n",
    "```python\n",
    "page_ids = [ page['id'] for page in data['query']['random'] ]\n",
    "```\n",
    "It does exactly the same as the previous example but in a neater way. The difference is that first you write what is happening in the loop `page['id']` and afterward you define the loop `for page in data['query']['random']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## From the obtained relatively simply dictionary\n",
    "## We can extract the list of page ids as follows:\n",
    "page_ids = [ page['id'] for page in data['query']['random'] ]\n",
    "page_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.\n",
    "\n",
    "Now we have a nice list of page ids, so we can use it to extract the content of the pages using a different method defined on the `query` endpoint. We will use a so-called _cirrus doc_ endpoint. _Cirrus_ is a system for organizing and storing text documents used by Wikipedia. It does not really matter to us. What matters is the fact that an endpoint like this exists and that it has a particular format. As we said _cirrus doc_ is a method on the `query` endpoint and we can call it with `prop=cirrusdoc`. However, to obtain any data we have also to pass a list of page ids in a proper format. Remember every piece of data that we provide through URL parameters (query string) is always treated as a string. Thanks to this every API can use some convention for defining lists of values. The Wikipedia API uses `|` as the separator, so it uses the following convention:\n",
    "\n",
    "* `<item 1>|<item 2>| ... |<item n>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Thus we have to join our page ids to form a single string\n",
    "page_ids_string = \"|\".join(str(p) for p in page_ids) ## this for loop is written similarly as the previous one\n",
    "page_ids_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now, the above considerations already enforce a particular form of a payload\n",
    "## that we will have to attach to the request URL.\n",
    "\n",
    "payload = {\n",
    "    'action': 'query',\n",
    "    'prop': 'cirrusdoc',\n",
    "    'pageids': page_ids_string,\n",
    "    'format': 'json'\n",
    "}\n",
    "payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## And now we are ready to make a request\n",
    "response = rq.get(BASE_URL, params=payload)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## And parse the response to a json dictionary\n",
    "data = response.json()\n",
    "## We can look and the top-level keys of the dict\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We should be interested in the query field, since judging by the name\n",
    "## it should contain the results of our query\n",
    "data['query'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Great, now we have only one key on the lower level, so it has to store the data\n",
    "pages = data['query']['pages']\n",
    "pages.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We see that the pages dictionary store all the pages we requested identified with their ids\n",
    "## Let us look at the inner keys of sub-dict with data of a single page\n",
    "key = list(pages)[0]\n",
    "pages[key].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## It seems that the main data is stored under the `cirrusdoc` key.\n",
    "type(pages[key]['cirrusdoc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hmm, the cirrusdoc property is a list.\n",
    "## So we have to extract data from it.\n",
    "pages[key]['cirrusdoc'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Okay, finally we see the source key, that must store the actual article content\n",
    "pages[key]['cirrusdoc'][0]['source'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bingo!! We see the `text` field. It contains the article text.\n",
    "## This is exactly what we want to extract.\n",
    "pages[key]['cirrusdoc'][0]['source']['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We examined the anatomy of the response of the _cirrus doc_ method in the Wikipedia API. So now we understand it and we can use this new knowledge to automatically extract the content of all the articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's use a for loop to get all the content of all pages\n",
    "articles = [ p['cirrusdoc'][0]['source']['text'] for p in pages.values() ]\n",
    "articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE THAT THE PREVIOUS EXPRESSION\n",
    "## DOES THE SAME AS THE FOLLOWING MORE VERBOSE EXPRESSION\n",
    "articles = []\n",
    "for page_id in pages.keys():\n",
    "  page = pages[page_id]\n",
    "  cirrus = page['cirrusdoc']\n",
    "  page_data = cirrus[0]\n",
    "  source = page_data['source']\n",
    "  text = source['text']\n",
    "  articles.append(text)\n",
    "\n",
    "articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great!!! We finally extracted the data we want. Now we can focus on computing words' length distributions of this data. So let's try to write a function that will return a dictionary with the distribution of words' lengths. It will be somehow similar to the function we wrote for the [N6](https://github.com/MikoBie/ppss/blob/main/notebooks/N6.ipynb) last year. It looked like that:\n",
    "\n",
    "```python\n",
    "def dict_count(L1):\n",
    "\t\"\"\"\n",
    "\tIt returns a dictionary with the frequencies of elements of L1.\n",
    "\n",
    "\tArgs:\n",
    "\t\tL1 (list): a list of values\n",
    "\n",
    "\tReturns:\n",
    "\t\tdict: dictionary with frequencies of elements of L1\n",
    "\t\"\"\"\n",
    "\toutput = {}\n",
    "\tfor item in L1:\n",
    "\t\tif item in output:\n",
    "\t\t\toutput[item] += 1\n",
    "\t\telse:\n",
    "\t\t\toutput[item] = 1\n",
    "\treturn output\n",
    "```\n",
    "And it more or less worked as follows. If we input the following list:\n",
    "```python\n",
    "input_list = [1, 2, 3, 3, 2, 4]\n",
    "```\n",
    "It returned the following dictionary:\n",
    "```python\n",
    "output_dict = {1 : 1, 2 : 2, 3 : 2, 4 : 1}\n",
    "```\n",
    "Now we need to figure out how from a list of strings get a list of words' lengths, right? Any ideas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE.\n",
    "def counter(l):\n",
    "    \"\"\"\n",
    "    It takes a list of strings and returns a dictionary with the distribution of words\n",
    "    length. Keys are the lengths and values are the frequencies.\n",
    "    Args:\n",
    "        l (str): a list of strings\n",
    "\n",
    "    Returns:\n",
    "        dict : a dictionary with the distribution of words length. \n",
    "    \"\"\"\n",
    "    ## BODY OF THE FUNCTION\n",
    "    \n",
    "dist =  counter(articles)           \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the results because they are very characteristic. They follow the [Zipf's distribution](https://en.wikipedia.org/wiki/Zipf%27s_law#Related_laws). In simple terms, the shortest words occur the most frequently and their frequency is inversely proportional to their rank in the frequency table. Thus the most frequent word will occur approximately twice as often as the second most frequent word, three times as often as the third most frequent word, etc. Let's see the results on a plot. We are not going to go too deep into plotting in _Python_ because there is no need for that. You can always plot something in _R_. Most of cases it will be enough to extract some data in _Python_ and later plot in _R_. That is because the syntax for plotting in _Python_ is at first not that straightforward, it is a bit similar to base plotting in R. So you have to imagine a blank canvas on which you put layer after layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's import the matplot lib module for plotting\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "## Sort the dictionary in descending order by keys\n",
    "## Instead of length of the word compute the rank\n",
    "## in the frequency table\n",
    "lists = { key + 1 : value[1] for key, value in enumerate(sorted(dist.items())) }\n",
    "## Create tuplets for x and y axis\n",
    "x, y = zip(*lists.items())\n",
    "## Create a plot\n",
    "plt.plot(x, y)\n",
    "## Add name of the x ax\n",
    "plt.xlabel('Rank')\n",
    "## Add name of the y ax\n",
    "plt.ylabel('Frequency')\n",
    "## Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Read about the `pageviews` method (`prop=pageviews`) in the `query endpoint` ([docpage](https://en.wikipedia.org/w/api.php?action=help&modules=query%2Bpageviews)). Use this method to extract page views data for the pages from the previous exercise (if you want you can sample 10 new pages with the `list=random` method) for the last 60 days. The results will be broken down by single days, so you have to aggregate the results (sum) so they give the total page views count for the entire period of 60 days. Remember that to select pages by page ids you pass `pageids=<id 1>|<id 2>|...|<id n>`. We did a very similar thing when we extracted article content through the `cirrusdoc` method in the Wikipedia API in the previous part of this notebook. Your final output should be a `dict` object that maps page ids to pageviews (total number of pageviews over 60 days). It should look something like this:\n",
    "\n",
    "```python\n",
    "results = {\n",
    "    # page_id: pageviews\n",
    "    153253: 10204,\n",
    "    423423: 101,\n",
    "    11012:  12,\n",
    "    42435:  546,\n",
    "    # and so on\n",
    "}\n",
    "```\n",
    "\n",
    "If you want you can sample 10 pages yourself. Otherwise, you may use the following list of page ids that we prepared for you. Sampling pages yourself will give you extra credit (but it is possible to get maximum points without it as well)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import module requests\n",
    "import requests\n",
    "\n",
    "## Some page ids\n",
    "page_ids = [\n",
    "    19969580,\n",
    "    39982842,\n",
    "    25699035,\n",
    "    52642931,\n",
    "    53055349,\n",
    "    24133565,\n",
    "    1164662,\n",
    "    40656459,\n",
    "    12533026,\n",
    "    47110862\n",
    "]\n",
    "\n",
    "## API URL\n",
    "BASE_URL = 'https://en.wikipedia.org/w/api.php'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE\n",
    "payload = {}\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "N1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10.6 ('ids')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "2f6b087e3237558552441618582c1cfff7dc162872ae65c00a668bb71df085d3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
