{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter\n",
    "\n",
    "<center><img src=\"png/twitter_logo.png\" width = 150/></center>\n",
    "\n",
    "The third API we are going to explore is Twitter API. On the one hand, it will be the easiest one to access, on the other hand, it will be the hardest one because unlike Wikipedia and Reddit it requires having not only a Twitter account but also a Developer Account. Moreover, the version we are going to use has very strict limits on how much data we can actually get. However, enough downsides. The good thing is that there is a great resource provided by Twitter itself with a very comprehensive [tutorial](https://github.com/twitterdev/getting-started-with-the-twitter-api-v2-for-academic-research). This notebook contains a small extract from it. So if you feel you need more information or something is unclear I would recommend seeing the tutorial.\n",
    "\n",
    "## What is Twitter?\n",
    "\n",
    "I would assume that most of you know what Twitter is, what Tweets look like, and what kind of interactions are possible there. Just in the case below there is a very brief definition from the above-mentioned course.\n",
    "\n",
    "> Twitter is a platform that is used by people across the world to exchange thoughts, ideas, and information with one another, using Tweets. Each Tweet consists of up to 280 characters and may include media such as links, images, and videos. In the context of research, Twitter data refers to the public information that is provided via Twitter’s application programming interface (API). The API supports various endpoints such as recent search, filtered steam, etc. that let developers and researchers connect to the API and request Twitter data.\n",
    "\n",
    "In general, if you ever saw a tweet. You probably saw it in the format in the picture below.\n",
    "\n",
    "<center><img src=\"png/greta.png\"/></center>\n",
    "\n",
    "However, under the hood, there is much more information (metadata) that characterizes every single tweet. With luck we can get some of this information using Twitter API, for example:\n",
    "\n",
    " * Tweet text\n",
    " * Tweet ID (that uniquely identifies a Tweet)\n",
    " * The time at which the Tweet was created\n",
    " * Public metrics associated with the Tweet such as the number of retweets, number of likes, etc.\n",
    " * Public user information such as username, user ID, user bio, profile image URL, etc.\n",
    " * Tweet Annotations - some Tweets are annotated based on the topic that they are about and the named entities present in the Tweets, i.e. COVID-19 stream.\n",
    "\n",
    "Complete information on the data we can get from each tweet might be found in the documentation under the following [link](https://developer.twitter.com/en/docs/twitter-api/data-dictionary/object-model/tweet).\n",
    "\n",
    "## Developer Account\n",
    "\n",
    "As I mentioned in the beginning, to access Twitter API we need to establish a [Developer Account](https://developer.twitter.com/en) (it requires having a regular account first). It requires answering a few fairly easy questions and confirming your email. By default, you will create a Developer Account with Essential Access to Twitter API. For the purpose of this class, it will be enough but if you plan on doing real research it is worth applying for Academic Research Access. \n",
    "\n",
    "After creating an account you should be able to log into [Developer Platform](https://developer.twitter.com/en) and create a project. In general, projects serve for organizing your access to Twitter API. Each project might contain multiple Apps (in our case it will be only one app) that serve for generating credentials for authentication. In other words, you need to create a project and later an app so Twitter can recognize that it is you who try to access the API. Therefore, you are never anonymous when you are getting data from Twitter. More or less, they know what you are doing. [Let that sink in](https://edition.cnn.com/videos/business/2022/10/28/late-night-elon-musk-sink-pun-twitter-orig-cprog-fj.cnn-business).\n",
    "\n",
    "When creating the App and project you simply need to answer a few simple questions. The answer should be straightforward. Until you get to the screen looking like that.\n",
    "\n",
    "<center><img src=\"png/keys.png\" /></center>\n",
    "\n",
    " It is the most important moment because those are the credentials (authorization details you need to store somewhere safe). In this particular case, although it is not the best idea ever just copy and paste them in the following chunk under relevant names. You should, however, try not to share them with anyone. That is because as I mentioned before they serve to identify you (it is more or less your ID for Twitter). So if someone maluses them it will be on your account. You should never share them on public repositories. Probably the best practice is to add them as environmental variables but it is far beyond this class. Therefore, for now, you will store them in this notebook (you can always access them on the Twitter Developer Platform)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For the extraction of the enviornmental variable\n",
    "## import os\n",
    "## BEARER_TOKEN = os.getenv('Bearer_Token')\n",
    "## Install the the Twitter module\n",
    "!pip install twarc\n",
    "## Define authorization keys\n",
    "## API_KEY = ''\n",
    "## API_SECRET_KEY = ''\n",
    "BEARER_TOKEN = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The just-established account gives us access to the so-called standard product track. In general, it allows to:\n",
    "\n",
    "* Search for Tweets from the last 7 days by specifying queries using supported operators (more on building queries in later sections)\n",
    "* Stream Tweets in real-time as they are happening by specifying rules to filter for Tweets that you are interested in.\n",
    "* Get Tweets from a user’s timeline (up to 3200 most recent Tweets)\n",
    "* Build the full Tweet objects from a Tweet ID or a set of Tweet IDs\n",
    "* Look up follower relationships\n",
    "\n",
    "As you can see it has the limitation of how far we can move back. Moreover, there is a total limit of 500,000 tweets we can get over the month. This restriction, however, does not apply to streaming tweets. We can stream as many tweets as we want. But it is important to acknowledge that the Stream Tweets endpoint gives access to only a sample of tweets (around 1% of all tweets).\n",
    "\n",
    "### Academic Research Product Track\n",
    "\n",
    "It is possible and rather straightforward to get access to the whole archive of tweets, dating back to 2006 (using the full-archive search endpoint). It requires just to apply for it and the benefits include:\n",
    "\n",
    "* Ability to get historical Tweets from the entire archive of public conversation on Twitter, dating back to 2006 (using the full-archive search endpoint)\n",
    "* Higher monthly Tweet volume cap of 10 million Tweets per month\n",
    "* More advanced filter options to return relevant data, including a longer query length, support for more concurrent rules (for filtered stream endpoint), and additional operators that are only supported in this product track (more on this later)\n",
    "\n",
    "As Twitter states: \"The Academic Research product track is reserved for those conducting professional academic researchers who have a specific research purpose with Twitter data.\" In order to get access to the academic research product track, these are the requirements:\n",
    "\n",
    "* You are a graduate student, doctoral candidate, post-doc, faculty, or research-focused employee at an academic institution or university.\n",
    "* You have a clearly defined research objective, and you have specific plans for how you intend to use, analyze, and share Twitter data from your research.\n",
    "* You will use this product track for non-commercial purposes.\n",
    "\n",
    "From what I know it is not very hard to get such access to Twitter API, however, it requires writing quite a few sentences about the purpose.\n",
    "\n",
    "## Endpoints\n",
    "\n",
    "The Twitter API provides different endpoints to get Tweets, based on your use case. It is important to know which endpoint you should use, in order to get the right data. For example, if you want to get historical Tweets, you have the choice of using the [recent search endpoint](https://developer.twitter.com/en/docs/twitter-api/tweets/search/introduction) (if the Tweets are from the last 7 days) or the [full-archive search endpoint](https://developer.twitter.com/en/docs/twitter-api/tweets/search/quick-start/full-archive-search) (if the Tweets are older than that). You can not get this historical data using a streaming endpoint such as [filtered stream endpoint](https://developer.twitter.com/en/docs/twitter-api/tweets/filtered-stream/introduction), because that endpoint only provides Tweets in real-time, as they happen. Similarly, if you want to build your Tweet dataset from a list of Tweet IDs, you can use the [Tweet lookup endpoint](https://developer.twitter.com/en/docs/twitter-api/tweets/lookup/introduction). A good summary of the most popular endpoints with the questions they might help to answer might be found [here](https://github.com/twitterdev/getting-started-with-the-twitter-api-v2-for-academic-research/blob/main/modules/3-deciding-which-endpoints-to-use.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load modules\n",
    "from twarc import Twarc2, expansions ## Twitter Wrapper\n",
    "import datetime ## Date management\n",
    "\n",
    "## Create a connection with Twitter using your BEARER TOKEN\n",
    "client = Twarc2(bearer_token=BEARER_TOKEN)\n",
    "\n",
    "\n",
    "## Specify the start time in UTC for the time period you want Tweets from\n",
    "## It must be within last 7 days\n",
    "start_time = datetime.datetime(2022, 11, 11, 0, 0, 0, 0, datetime.timezone.utc)\n",
    "\n",
    "## Specify the end time in UTC for the time period you want Tweets from\n",
    "end_time = datetime.datetime(2022, 11, 12, 0, 0, 0, 0, datetime.timezone.utc)\n",
    "\n",
    "## This is where we specify our query \n",
    "query = \"from:elonmusk -is:retweet\"\n",
    "\n",
    "## The search_recent method call the recent search endpoint to get Tweets based on the query, start and end times\n",
    "search_results = client.search_recent(query=query, start_time=start_time, end_time=end_time, max_results=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's unpack what we got. We accessed the Twitter API using this module `Twarc2` and probably we would expect any kind of information that we succeed. Status code? At least something similar to Wikipedia's response, right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We would expect a status code or a at least a list of JSONs?\n",
    "search_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, it is neither. It is a generator object (as it says above). It means that it is the output of a special kind of function that is called a generator. In nutshell, a generator is a function that returns a lazy iterator. It means that its values are generated when we iterate over it. It might be confusing (and for me still is) but we don't have to worry about it right now too much. That is because the good news is that we can use a `for-loop` to extract the values of a generator object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Assign elements of the generator to the \n",
    "## object calle results\n",
    "results = [ item for item in search_results ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand a bad news is that since generator generates its content when it is interated over we can do it only once. Therefore, you can only iterate over them once -- they are single use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's try to re-use our generator to extract its\n",
    "## values once again.\n",
    "[ item for item in search_results ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I mentioned before, we should not be very surprised that we got an empty list knowing what a generator is. Good, we assigned the values of the generator to the name `results`. Let's examine it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's see what we got from Twitter\n",
    "print(type(results))\n",
    "print(len(results))\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0]['meta']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a list of a length 1. And as we see under the index `0` we have a JSON that contains four keys. While we probably will be interested in the `data` others might also draw our attention, for example `meta` and `includes`. They both contain some additional information about tweets (`includes` -- information about media attached to the tweet and `meta` -- simply meta information about the tweet we gathered). Therefore, both fields might be of some interest to us. We could try to add them to the relevant tweets in the `data` field but fortunately, we don't have to do it manually. We can use the `expansions.flatten()` function. It will do the dirty work for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expansions.flatten(results[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know the structure that we get from Twitter API, we can do all the above-mentioned steps in a single line using a list comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's do everything in one line\n",
    "results_lc = [ tweet  for item in search_results for tweet in expansions.flatten(item) ]\n",
    "results_lc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yyyyy, why it did not work? It did not work because we tried to iterate over `search_results` which is a generator object. Therefore, they are only single-iterable.\n",
    "\n",
    "Ok, let's now see what kind of data we get when we examine a single tweet. It will be probably not a big surprise if I tell you that it is a dictionary. Therefore, let's look at its keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_lc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_lc[0]['in_reply_to_user']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's unpack them a bit the fields and recognize what we can expect them to contain.\n",
    "\n",
    "* `source` (string) -- it indicates the software used to create a tweet, i.e. Twitter for iPhone\n",
    "* `conversation_id` (string) -- the Tweet ID of the original Tweet of the conversation (which includes direct replies, replies of replies).\n",
    "* `possibly sensitive` (boolean) -- this field indicates content may be recognized as sensitive. The Tweet author can select within their own account preferences and choose “Mark media you tweet as having material that may be sensitive” so each Tweet created after has this flag set. This may also be judged and labeled by an internal Twitter support agent.\n",
    "* `reply_settings` (string) -- shows you who can reply to a given Tweet. Fields returned are \"everyone\", \"mentioned_users\", and \"followers\".\n",
    "* `context_annotations` (list) -- contains context annotations for the Tweet. This is a bit of magic.\n",
    "* `lang` (string) -- Language of the Tweet, if detected by Twitter.\n",
    "* `edit_history_tweet_ids` (list) -- unique identifiers indicating all versions of a Tweet. For Tweets with no edits, there will be one ID. For Tweets with an edit history, there will be multiple IDs, arranged in ascending order reflecting the order of edits. The most recent version is the last position of the array.\n",
    "* `id` (string) -- the unique identifier of the requested Tweet.\n",
    "* `entities` (dict) -- entities that have been parsed out of the text of the tweet.\n",
    "* `text` (string) -- the content of the tweet (encoded in UTF-8).\n",
    "* `edit_controls` (dict) -- when present, this indicates how much longer the Tweet can be edited and the number of remaining edits. Tweets are only editable for the first 30 minutes after creation and can be edited up to five times.\n",
    "* `in_reply_to_user_id` (string) -- if the represented tweet is a reply, this field will contain the original Tweet’s author ID. This will not necessarily always be the user directly mentioned in the Tweet.\n",
    "* `created_at` (string) -- creation time of the Tweet (in the following format YYYY-MM-DDThh:mm:ss.<time>Z, i.e. 2022-11-01T23:30:51.000Z).\n",
    "* `public_metrics` (dict) --  public engagement metrics for the Tweet at the time of the request (retweet count, like count, quote count, reply count).\n",
    "* `referenced_tweets` (list) --  list of Tweets this Tweet refers to. For example, if the parent Tweet is a Retweet, a Retweet with a comment (also known as Quoted Tweet) or a Reply, it will include the related Tweet referenced to by its parent.\n",
    "* `author_id` (string) -- the unique identifier of the User who posted this tweet.\n",
    "* `author` (dict) -- information about the author of the tweet.\n",
    "* `in_reply_to_user` -- information about the user to which this tweet replies.\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Now, that we know what tweets look like. Let's examine the tweets we collected. Please extract from the collected tweets a tweet with the biggest number of likes and return its text (maybe it will be something funny...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Queries\n",
    "\n",
    "In order to get Tweets for your research using the Twitter API, you need to specify what Tweets you are looking for. To do so, you need to write a search query (when using search endpoints) or set rules (when using the filtered stream endpoint). Both the search query and rules serve the same purpose of giving you the ability to describe the keywords and conditions for which you want tweets.\n",
    "\n",
    "A search query or rule can consist of a combination of standalone operators such as keywords and conjunction-required operators such as `is:retweet`. Standalone operators can be used by themselves or with other operators, whereas conjunction-related operators can not be used alone.\n",
    "\n",
    "Below are some examples of search queries based on certain use-cases:\n",
    "\n",
    "* `from:elonmusk` -- gives all tweets from the Elon Musk account.\n",
    "* `Elon Musk` -- gives all tweets that contain the words `\"Elon Musk\"`. The space between the terms indicates logical AND operator. Therefore, both of the words must appear in the tweet.\n",
    "* `Elon Musk OR unions` -- gives all tweets that contain the words Elon Musk or unions. The logical AND between `Elon` and `Musk` will be applied first followed by the logical OR of that with `union`.\n",
    "* `from:TwitterDev -is:retweet` -- gives all tweets from the TwitterDev account that are not retweets. The is:retweet operator filters for only those tweets that are retweets and the ‘-’ indicates negation of this condition.\n",
    "* `covid-19 has:geo` -- gives all Tweets that contain the word covid-19 and have geo data associated with them. However, it only works for the Academic Research product track.\n",
    "* `from:elonmusk has:images` -- gives all tweets that are from Twitterdev and have images.\n",
    "* `conversation_id:1394699198382043136` -- gives all tweets in the conversation thread with conversation_id 1394699198382043136. The conversation ID is the tweet ID of the main Tweet for which you want all the replies. Because this Tweet is older than the last 30 days, you can only obtain it using the full-archive search endpoint. Additionally, you will also have to specify the start_time parameter otherwise you will not get any results back.\n",
    "* `covid-19 lang:pl` -- gives all tweets that contain the word covid-19 that are in Polish.\n",
    "\n",
    "When you use the recent search and full-archive endpoints as part of the Academic Research product track, your query length can be up to 1024 characters. In the Standard product track, you have access to the recent search endpoint and your query length can be up to 512 characters.\n",
    "\n",
    "When you use the filtered stream endpoint as part of the Academic Research product track , you can set up to 1000 concurrent rules and each rule can be 1024 characters long. In the Standard product track, you can set 25 concurrent rules and each rule is 512 characters long.\n",
    "\n",
    "See the complete list of operators supported by endpoints:\n",
    "\n",
    "1. [Recent search and Full-archive search](https://developer.twitter.com/en/docs/twitter-api/tweets/search/integrate/build-a-query#availability)\n",
    "2. [Filtered stream](https://developer.twitter.com/en/docs/twitter-api/tweets/filtered-stream/integrate/build-a-rule#availability)\n",
    "\n",
    "There is also bunch of operators that are exclusive for the Acadmic Research track product but we are going to skip them for now. Their description might be found for example [here](https://github.com/twitterdev/getting-started-with-the-twitter-api-v2-for-academic-research/blob/main/modules/5-how-to-write-search-queries.md).\n",
    "\n",
    "### Getting a random 1% sample of Tweets in real-time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load modules\n",
    "from twarc import Twarc2, expansions ## Twitter Wrapper\n",
    "\n",
    "## Create a connection with Twitter using BEARER TOKEN\n",
    "client = Twarc2(bearer_token=BEARER_TOKEN)\n",
    "\n",
    "## Create a list in which we will store the tweets\n",
    "list_tweets = []\n",
    "\n",
    "## The sample method gives a 1% random sample of all Tweets\n",
    "## We use enumerate to return both a number and the tweet\n",
    "for count, result in enumerate(client.sample()):\n",
    "    ## We use expansions.flatten() to merge the data about a single\n",
    "    ## tweet\n",
    "    tweet = expansions.flatten(result)\n",
    "    ## Append the tweet to the list\n",
    "    list_tweets.append(tweet[0])\n",
    "\n",
    "    ## Break the look after reaching 102 tweets\n",
    "    if count > 100:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT**: The sample method connects us to the Twitter stream. Therefore, unless we disconnect using the above method (or for example while loop) the for-loop will never end.\n",
    "\n",
    "*Finger Exercise*: Why do we get 102 tweets, not 100? How to amend the code above to get exactly 100 tweets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering for Tweets on a topic using filtered-stream\n",
    "\n",
    "The filtered stream endpoints deliver filtered Tweets to you in real-time that match on a set of rules that are applied to the stream. Rules are made up of operators that are used to match on a variety of Tweet attributes.\n",
    "Multiple rules can be applied to a stream using the method `Twarc2.add_stream_rules()`. Once you’ve added rules and connect to your stream using the `stream` endpoint, only those tweets that match your rules will be delivered in real-time through a persistent streaming connection. You do not need to disconnect from your stream to add or remove rules. \n",
    "\n",
    "More details on defining rules might be found [here](https://developer.twitter.com/en/docs/twitter-api/tweets/filtered-stream/integrate/build-a-rule)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load modules\n",
    "from twarc import Twarc2, expansions ## Twitter Wrapper\n",
    "\n",
    "## Create a connection with Twitter using BEARER TOKEN\n",
    "client = Twarc2(bearer_token=BEARER_TOKEN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we connect to the Twitter stream it is a good practice to check for the existing rules and delete them. Just to make sure that you are really filtering what you think you are filtering. To do so we will use the `Twarc2.get_stream_rules()` method that will return all exsiting rules and the `Twarc2.delete_stream_rule_ids()` method that takes an id of the rule and deletes it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check the existing rules. \n",
    "existing_rules = client.get_stream_rules()\n",
    "print(existing_rules)\n",
    "\n",
    "## Check whether there are any rules.\n",
    "if 'data' in existing_rules and len(existing_rules['data']) > 0:\n",
    "\t## Return a list of rules ids\n",
    "    rule_ids = [ rule['id'] for rule in existing_rules['data'] ]\n",
    "\t## Remove existing rules\n",
    "    client.delete_stream_rule_ids(rule_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding new rules is fairly easy. We just use the syntax from the queries listed above and create a list of dictionaries. If you have the Academic Research product track, you can set up to 1000 concurrent rules and each rule can be 1024 characters long. In the Standard product track, you can set 25 concurrent rules and each rule is 512 characters long. In other words, in our case, we can create a list of 25 dictionaries. In each dictionary, the most important is the `value` key. That is because we enter our rules as its value. We can add also the `tag` key which we can describe in more human-readable language the given rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add new rules\n",
    "new_rules = [\n",
    "    { \"value\": \"cat has:media\", \"tag\": \"cats with media\" }\n",
    "]\n",
    "added_rules = client.add_stream_rules(rules=new_rules)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT**: The rules are concurrent. That means that there is `AND` between them. In other words, they should narrow down your search not look for different topics.\n",
    "\n",
    "Streaming tweets is very similar to getting a sample of 1% of tweets. You just connect to the stream and need to specify when you want to disconnect (or maybe never...). The only difference is that instead of the `Twarc2.smaple()` method you use `Twarc2.stream()`. The rest is more or less the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create an empty list\n",
    "list_tweets = []\n",
    "\n",
    "## Connect to the filtered stream\n",
    "for count, result in enumerate(client.stream()):\n",
    "    ## Use expansions.flatten() to merge the data about a single\n",
    "    ## tweet\n",
    "    tweet = expansions.flatten(result)\n",
    "    ## Append the tweet to the list\n",
    "    list_tweets.append(tweet[0])\n",
    "    ## Break the look after reaching 102 tweets\n",
    "    if count > 100:\n",
    "        break\n",
    "\n",
    "## Delete the rules once you have collected the desired number of Tweets\n",
    "rule_ids = [rule['id'] for rule in added_rules['data']]\n",
    "client.delete_stream_rule_ids(rule_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Write out to a `JSON` file only tweets that are in English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweet lookup endpoint\n",
    "\n",
    "Although the main two endpoints we can access through Twitter API are [recent search endpoint](https://developer.twitter.com/en/docs/twitter-api/tweets/search/introduction) (or [full-archive search endpoint](https://developer.twitter.com/en/docs/twitter-api/tweets/search/quick-start/full-archive-search) if you have the academic access) and [filtered stream endpoint](https://developer.twitter.com/en/docs/twitter-api/tweets/filtered-stream/introduction) there is also a third endpoint that allows to look up timelines, users, followers, tweets, conversations, etc. -- so-called [tweet lookup endpoint](https://developer.twitter.com/en/docs/twitter-api/tweets/lookup/introduction). The `Twarc2` module provides simple methods that allow accessing it. The only caveat is that you will get the tweets (or whatever you are looking for) in batches (pages). Each one of them will contain 100 tweets (or whatever you are looking for). Therefore, we will have to first loop over each page (batch) and within each page over tweets to be able to access them. Don't worry because it might sound complicated but in reality is not. The code below should walk you through the process.\n",
    "\n",
    "#### User's timeline\n",
    "\n",
    "The maximum number of tweets you can get for a user is `3200`. Within `15` minutes you can send a maximum of `1500` requests. In other words, you can check the timelines of 1500 users in 15 minutes and for each user, you will get only 3200 tweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load modules\n",
    "from twarc import Twarc2, expansions ## Twitter Wrapper\n",
    "\n",
    "## Create a connection to Twitter using BEARER TOKEN\n",
    "client = Twarc2(bearer_token=BEARER_TOKEN)\n",
    "\n",
    "\n",
    "## Create an empty list\n",
    "tweet_list = []\n",
    "\n",
    "## Connect to the timeline of Nature \n",
    "user_timeline = client.timeline(user=\"nature\")\n",
    "\n",
    "## Twitter API will return the data paginated. It means\n",
    "## that it will be grouped in batches of 100 tweets.\n",
    "## Therefore, we first iterate over pages (batches of\n",
    "## tweets).\n",
    "for page in user_timeline:\n",
    "    ## Use expansions.flatten() to merge information \n",
    "    ## about tweets\n",
    "    result = expansions.flatten(page)\n",
    "    ## Iterate over this 100 tweets\n",
    "    for tweet in result:\n",
    "        ## Append tweets to the list.\n",
    "        tweet_list.append(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a very similar way using method `Twarc2.mentions()` you can get tweets in which a given user was mentioned. However, the limits are a bit different. You can seach for only 800 users within 15 minutes and you will get only 800 last mentions.\n",
    "\n",
    "#### Users\n",
    "\n",
    "You can either look up users by their ids or user names. Unfortunetly in the same request you can't mix ids and usernames. If you decide to users by their usernames you must pass the `True` value to usernames. You can send up to `900` requests within `15` minutes. Each request might contain no more than `100` users. Therefore, again you will get information about users in batches of 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load modules\n",
    "from twarc import Twarc2, expansions\n",
    "\n",
    "## Create a connection to Twitter using BEARER TOKEN\n",
    "client = Twarc2(bearer_token=BEARER_TOKEN)\n",
    "\n",
    "\n",
    "## List of user IDs to lookup\n",
    "users = ['AlojzyZNowak', 'whataweekhuh', 'iga_swiatek']\n",
    "\n",
    "## Create a list of users\n",
    "users_list = []\n",
    "\n",
    "## Connect to lookup endpoint\n",
    "lookup = client.user_lookup(users=users, usernames = True)\n",
    "\n",
    "## Twitter API will return the data paginated. It means\n",
    "## that it will be grouped in batches of 100 tweets.\n",
    "## Therefore, we first iterate over pages (batches of\n",
    "## tweets).\n",
    "for page in lookup:\n",
    "\t## Use expansions.flatten() to merge information\n",
    "    ## about users\n",
    "    result = expansions.flatten(page)\n",
    "\t## Iterate over this 100 users (maximum in our case 3)\n",
    "    for user in result:\n",
    "        users_list.append(user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get user's followers\n",
    "\n",
    "You can get a given user's followers by either providing user's id or username. However, there are quite strict rate limits. You can only send `15` requests per `15` minutes and in each request, you can gather only ~~`5000`~~ `100` ids. ~~That means that for example for Elon Musk you would have to wait more or less 17 days to get all the followers.~~ The good news though is that `Twarc` will handle the so-called sleeping time in which you can't send requests because of the rate limits. However, you need to pass the argument `user_fields=['id']` otherwise the function will get also information about the user which will limit the number of ids you will get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load modules\n",
    "from twarc import Twarc2, expansions\n",
    "\n",
    "## Create a connection to Twitter using BEARER TOKEN\n",
    "client = Twarc2(bearer_token=BEARER_TOKEN)\n",
    "\n",
    "\n",
    "## List of the user's followers\n",
    "followers_list = [] \n",
    "\n",
    "## Connect to lookup endpoint\n",
    "followers = client.followers(user=\"JuiceKowalczyk\")\n",
    "\n",
    "## Twitter API will return the data paginated. It means\n",
    "## that it will be grouped in batches of 5000 users.\n",
    "## Therefore, we first itearte over pages (batches of\n",
    "## tweets.\n",
    "for page in followers:\n",
    "    ## Use expansions.flatten() to merge information\n",
    "    ## about users\n",
    "    result = expansions.flatten(page)\n",
    "    ## Iterate over this 5000 ids\n",
    "    for user in result:\n",
    "        ## Append to the list\n",
    "        followers_list.append(user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a very similar way using the method `Twarc2.following()` you can get information on given user friends (people the user is following not real friends). The limits are exactly the same as in the case of followers. However, usually, after a certain point in fame people have more followers than friends (Twitter limits people to follow no more than 5000 accounts). It sounds worse than it actually is, for example, Elon Musk follows only 132 people.\n",
    "\n",
    "#### Lookup conversation\n",
    "\n",
    "You can also look up a certain conversation. It means gathering tweets that are a reply to a certain tweet. However, with Standard access, you will have access to only tweets posted within the last 7 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load module\n",
    "from twarc import Twarc2, expansions\n",
    "\n",
    "## Create a connection to Twitter using BEARER TOKEN\n",
    "client = Twarc2(bearer_token=BEARER_TOKEN)\n",
    "\n",
    "\n",
    "## Specify the Tweet ID for which you want the conversation thread\n",
    "query = \"conversation_id:1591121142961799168\"\n",
    "\n",
    "## Connect to recent tweets endpoint\n",
    "search_results = client.search_recent(query=query, max_results=100)\n",
    "\n",
    "## Create an empty list\n",
    "tweet_list = []\n",
    "\n",
    "## Twitter API will return the data paginated. It means that it\n",
    "## will be grouped in batches of 100 tweets. Therefore, we first\n",
    "## iterate over pages (batches of tweets)\n",
    "for page in search_results:\n",
    "    ## Use expansions.flatten() to merge information\n",
    "    ## about users\n",
    "    result = expansions.flatten(page)\n",
    "    ## Iterate over this 100 tweets\n",
    "    for tweet in result:\n",
    "        # Here we are printing the full Tweet object JSON to the console\n",
    "        tweet_list.append(tweet)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('ids')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2f6b087e3237558552441618582c1cfff7dc162872ae65c00a668bb71df085d3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
