{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling\n",
    "\n",
    "In general, topic modeling will serve us to cluster texts that are similar to one another. Why might we want this? That is because when getting data from the Internet, we will usually have quite a big number of texts. It will be very ineffective to read all of them. Instead, we might want to read just a few from each group to understand what are the different types of texts (narratives). For example, it is probably relatively easy to gather texts about the climate emergency. However, the question is in what context it is described, and whether this context has changed over the years or not. Or maybe we want to see whether there are different narratives present in the gathered corpus.\n",
    "\n",
    "<center><img src=\"png/topic_modeling.png\" width = 600/></center>\n",
    "\n",
    "If everything goes according to the plan, under topic modeling we will discuss three models:\n",
    "\n",
    "1. `tf-idf` (term frequency - inverse document frequency) -- a very simple model that will allow us to find similarities between texts.\n",
    "2. `LDA` (Latent Dirichlet Allocation) -- model that explains a set of observations through unobserved groups, and each group explains why some parts of the data are similar.\n",
    "3. ~~`BERTopic` -- a new approach to topic modeling that uses the latest advances in transformers-based machine learning for Natural Language Processing.~~\n",
    "\n",
    "However, before we start with anything fancy let's start with the basics and define some key concepts that will serve us in our [voyage](https://www.youtube.com/watch?v=NlgmH5q9uNk) with topic modeling.\n",
    "\n",
    "1. `Document` -- some text.\n",
    "2. `Corpus` -- a collection of documents (texts).\n",
    "3. `Vector` -- a mathematical representation of a document (text).\n",
    "4. `Model` -- an algorithm for transforming vectors from one representation to another.\n",
    "\n",
    "## Document\n",
    "\n",
    "A document is basically a string that might be the length of a single tweet or [A Litle Life](https://en.wikipedia.org/wiki/A_Little_Life). In _Python_ it will look more or less like what we could have expected. Below, we have the text of a tweet by [Paulo Coelho Quotes](https://www.youtube.com/watch?v=dQw4w9WgXcQ) account:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "document = \"Fear of failure leads to failure.\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus\n",
    "\n",
    "On the other hand, a corpus is a collection of documents (texts). For us, they will play two roles:\n",
    "\n",
    "1. They are the input for training a model. This is going to be our main use case because among others it will allow for separating common themes and topics.\n",
    "2. They organize documents. After training, a topic model can be used to extract topics from new documents (documents not seen in the training corpus). In other words, using the trained model we will be able to say how similar is a new document to a group of documents from our corpus.\n",
    "\n",
    "At this point, we can think about a corpus as a list of strings. Consider the following. These are 20 tweets by [Paulo Coelho Quotes](https://twitter.com/PauloCoelho_DQ). We could have imported them to _Python_ in a more efficient way than this but at this point, this is not important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_corpus = [\n",
    "    \"Fear of failure leads to failure.\",\n",
    "    \"If we do not face our fears, our fears will chase us forever.\",\n",
    "    \"Be brave. Take risks. Nothing can substitute experience.\",\n",
    "    \"Rumors are created by haters, spread by fools, and accepted by idiots.\",\n",
    "    \"Love yourself first because this is the person you are going to spend the rest of your life with.\",\n",
    "    \"The distance between dreams and reality is called discipline.\",\n",
    "    \"Never underestimate the power of stupid people.\",\n",
    "    \"If you are never alone, you cannot know yourself.\",\n",
    "    \"Whatever you decide to do, make sure it makes you happy.\",\n",
    "    \"The more violent the storm, the quicker it passes.\",\n",
    "    \"If you are never alone, you cannot know yourself.\",\n",
    "    \"If you want to be successful, you must respect one rule â€“ Never lie to yourself.\",\n",
    "    \"First it hurts, then it changes us.\",\n",
    "    \"No matter how you feel today, get up, dress up and show up.\",\n",
    "    \"Listen to your heart. It knows all things.\",\n",
    "    \"Remember that wherever your heart is, there you will find your treasure.\",\n",
    "    \"Never explain. Your friends will not need it. And haters will continue to hate.\",\n",
    "    \"The secret of life, though, is to fall seven times and to get up eight times.\",\n",
    "    \"We do not need to explain our love. We only need to show it.\",\n",
    "    \"Better to be crazy and happy than normal and bitter.\"\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important**: The above example loads the entire corpus into memory. In practice, corpora may be very large, so loading them into memory may be inconvenient (to say at least). However, for now, we are leaving it as it is.\n",
    "\n",
    "Before we move any further, we need to tokenize our corpus and also remove stop words. We may just use the same code we discussed when we computed the sentiment.\n",
    "\n",
    "Therefore, let's start with loading relevant modules and downloading stop words for English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import NLTK module\n",
    "import nltk\n",
    "## Import stop words\n",
    "from nltk.corpus import stopwords\n",
    "## Import function to tokenize text\n",
    "from nltk.tokenize import word_tokenize\n",
    "## Import function to lemmatize text\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "## Download stopwords list\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "## Assign the list of English stop words to stop_words\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "## Assign WordNetLemmatizer to lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to preprocess each document from the corpus. We are going to apply the same methods as when we were talking about sentiment analysis. In other words, we are going to have the following pipleline for each document:\n",
    "\n",
    "$$\\text{document} \\rightarrow \\text{tokenization} \\rightarrow \\text{lemmatization} \\rightarrow \\text{stop words removal}$$\n",
    "\n",
    "We are going to do it in one step using list comprehension because it is just the most convenient solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenize every single text. Meanwhile lemmatize and remove tokens\n",
    "## that are shorter than 2 and are in stop_words list\n",
    "texts = [ [ lemmatizer.lemmatize(token) for token in word_tokenize(doc.lower()) if lemmatizer.lemmatize(token) not in stop_words and len(lemmatizer.lemmatize(token)) > 1 ] \n",
    "          for doc in text_corpus ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a mapping with the frequency of all words in the corpus. We know how to do it but let's discover a new way of doing it. Instead of using a built-in mapping, we are going to use a more fancy one. The difference is that it does not return an error when a given key does not exist. Instead, it creates a new pairing with the given key and default value. In other words, if we define new mapping as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A new type of dictionary\n",
    "from collections import defaultdict\n",
    "\n",
    "new_mapping = defaultdict(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and try to access its non-existing key, let's say `'name'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_mapping['name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it returned `0` because we initialized it with a default value to `int` which is understood as `0`. What's more, when we inspect the mapping, we see that a new pairing appeared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it not only did not return an error. It also created a new pairing. This is pretty cool, right? Mostly because we don't have to write any more code like the following to compute the frequency of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create an empty mapping.\n",
    "frequency = {}\n",
    "\n",
    "## Compute the frequency of tokens in the whole\n",
    "## corpus.\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        if token in frequency:\n",
    "            frequency[token] += 1\n",
    "        else:\n",
    "            frequency[token] = 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create an empty fancy mapping.\n",
    "frequency = defaultdict(int)\n",
    "\n",
    "## Count the number of words all texts\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, both methods return the same results.\n",
    "\n",
    "Ok, now let's use the frequency mapping to remove from our corpus words that appear only once. For our purposes, they are not going to be very useful because we will not be able to infer from them whether the two documents are similar. Again, we are going to use list comprehension for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove tokens that appear only once in the whole corpus\n",
    "processed_corpus = [ [ token for token in text if frequency[token] > 1 ] \n",
    "                     for text in texts ]\n",
    "\n",
    "processed_corpus"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding, we will associate each word in the corpus with a unique integer ID. We want to do this because for a computer it will be just easier to use integers than strings (basically a string consumes more space than an integer, therefore, the computer can operate on integers faster than on string). We will use for that a new module `gensim`. It will allow for the creation of a dictionary that will store unique tokens paired with a unique integer id. In other words, we will create a kind of mapping. We can think about it a bit like a dictionary with integers as keys and tokens as values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "## Create a new object called Dictionary\n",
    "dictionary = corpora.Dictionary(processed_corpus)\n",
    "\n",
    "## Let's print it out\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see it is not a mapping per se. But we can easily convert it to an ordinary mapping by iteraiting over it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{ key : value for key, value in dictionary.items() }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: We have here a relatively small corpus, therefore, there are only 17 different tokens. For a larger corpus, dictionaries can become massive, like over a thousand tokens. Among others, this is why it is not an ordinary dictionary but stores the data in a more efficient and memory-friendly way. The other property of this Dictionary is that we don't have to have all documents in the working memory at the same time to create it. There is an easy way to create it by processing one document at a time. However, we are not going to discuss it right now because right now we focus on the general idea.\n",
    "\n",
    "## Vector\n",
    "\n",
    "What happened so far was relatively easy. More or less we converted a set of texts into a new object which is called Dictionary (it is not the same as `dict` or a mapping). Now, we would like to convert the tokens we have into vectors. We need that because to infer the latent structure in our corpus we need a way to represent documents in a way that allows for mathematical manipulation. In other words, we need to transform strings that are understandable for us into something which will be understandable for a computer. We can infer the latent structure (find similarities between texts) by understanding the meaning of the texts but computers can not really do that (don't believe people who state otherwise). Instead, they can quite well and fast perform mathematical operations. Therefore, we need a way to convert a text into a mathematical object (vector) and ask the computer to find similarities between a set of vectors using math (you probably computed the distance between two vectors in high school).\n",
    "\n",
    "In other words, we need to find a way to represent a document (text) as a vector of features. For example, a single feature may be thought of as a question-answer pair:\n",
    "\n",
    "1. How many times does the word *sponge* appear in the document? Zero.\n",
    "2. How many paragraphs does the document consist of? Two.\n",
    "3. How many fonts does the document use? Five.\n",
    "\n",
    "If we are going to apply the same questions to every single text we can skip their text and only refer to their integer ids (such as `1`, `2`, and `3`). Therefore, the representation of a document becomes a series of pairs like `(1, 0), (2, 2), (3, 5)`. This is called a **dense vector**, because it contains an explicit answer to each of the above questions.\n",
    "\n",
    "In practice, we can omit `0` values. We do that to save memory (usually we have much more features than just `3`). Therefore, actually, our vector would look like `(2,2), (3,5)`. This is known as a **sparse vector** or **bag-of-words vector**.\n",
    "\n",
    "Assuming the questions are the same we can compare two texts, for example, texts with the following **bag-of-words** vectors `(2, 2), (3, 5)` and `(1, 1), (2, 2), (3, 5)`. Based on the answers to our questions we can infer that the two texts must be similar. The only difference between these two texts is the appearance of [sponge](https://www.youtube.com/watch?v=-SuSWX8h-hg). They must be similar, at least on the examined features. We don't have to do any heavy math to examine it.\n",
    "\n",
    "Ok, so let's have something more involving than our silly questions. Usually, under the **bag-of-words** models, each document is represented by a vector containing the frequency count of each word in the dictionary. For example, let's assume we have a dictionary containing the following words `['sponge', 'square', 'pants', 'bob']`. A document consisting of the string `\"sponge pants sponge\"` would then be represented by the vector `(1, 2), (2, 0), (3, 1), (4, 0)`, where the entries of the vector are (in order) the occurrences of \"sponge\", \"square\", \"pants\", and \"bob\" in the document. The only issue here is that we kind of ignored the order of words. But let's don't worry about it right now. Also, as the name of this method suggests it is a **bag-of-words** approach so it is understandable that the order does not matter too much.\n",
    "\n",
    "Ok, it is all very good but how it connects with the creation of this Dictionary object? This is somehow exactly this. Our processed corpus has `17` unique tokens in it, which means that each document will be represented by a 17-dimensional vector under the **bag-of-words model**. We can use the dictionary to turn tokenized documents into these 17-dimensional vectors. Let's see these ids again using the `token2id` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.token2id"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now, when we want to vectorize a phrase we will just use the above dictionary as a kind of cipher. For example, suppose we want to vectorize the phrase [`\"If you do not fight for your love what kind of love do you have?\"`](https://youtu.be/jFrGYB8inoY) (note that this phrase was not in our original corpus, however, is equally cheesy). We can create the **bag-of-word** representation for a document using the `doc2bow()` method of the dictionary, which returns a sparse representation of the word counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_doc = \"If you do not fight for your love what kind of love do you have?\"\n",
    "new_tokens = [ lemmatizer.lemmatize(token).lower() for token in word_tokenize(new_doc) ]\n",
    "new_vec = dictionary.doc2bow(new_tokens)\n",
    "new_vec"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first entry in each tuple corresponds to the ID of the token in the dictionary, the second corresponds to the count of this token.\n",
    "\n",
    "Note that this vector only contains entries for words that actually appeared in the document. Because any given document will only contain a few words out of the many words in the dictionary, words that do not appear in the vectorization are represented as implicitly zero as a space-saving measure.\n",
    "\n",
    "We can convert our entire original corpus to a list of vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [ dictionary.doc2bow(text) for text in processed_corpus ]\n",
    "bow_corpus"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: In our specific case we can have the whole list in the working memory but with more (and longer) documents we will not want to load all the elements at the same time. Luckily, `gensim` allows you to use any iterator that returns a single document vector at a time but we are not going to talk about it right now.\n",
    "\n",
    "**IMPORTANT**: The distinction between a document and a vector is that the former is text, and the latter is a mathematically convenient representation of the text. Moreover, depending on how the representation was obtained, two different documents may have the same vector representations.\n",
    "\n",
    "\n",
    "## Model\n",
    "\n",
    "Before, we talk about more involving models. Let's start with the simplest transformation. So far, we have vectorized our corpus. Now, we can transform the **bag-of-words** representation into something more interesting.\n",
    "\n",
    "One simple example of a model (transformation) is `tf-idf` (term frequency - inverse document frequency). In simple words, it returns a statistic that allows saying how important a given term is to a document in the corpus, for example how important Sirius Black is to the Harry Potter book in the collection of Harry Potter books. Therefore, this model allows for transforming vectors from the **bag-of-words** representation to a vector space where the frequency counts are weighted according to the relative rarity of each word in the corpus. Why it is useful? That is because this statistic lays the foundations for the relevance of the search.\n",
    "\n",
    "Imagine that you have a set of Harry Potter books and you would like to order them by the relevance to the following query: \"Sirius Black\". A simple way to start out is by eliminating books that do not contain all two words \"Sirius Black\", but this still leaves many books (probably 7). To further distinguish them, we might count the number of times each term occurs in each document; the number of times a term occurs in a document is called its term frequency. However, the problem often arises when the texts differ in terms of length (and if you read Harry Potter books you probably noticed that each book was longer than the previous one). Therefore, we can simply divide the number of occurence of a given word ($i$) in a given document ($j$) by the number of all words:\n",
    "\n",
    "$$frequency_{i,j} = \\frac{i_j}{n_j}$$\n",
    "\n",
    "where, $i$ is a given word (token); $j$ is a given document; and $n_j$ is the number of words (tokens) in the document $j$.\n",
    "\n",
    "Because the term \"Black\" is so common, term frequency will tend to incorrectly emphasize documents that happen to use the word \"Black\" more frequently, without giving enough weight to the more meaningful term \"Sirius\". The term \"Black\" is not a good keyword to distinguish relevant and non-relevant documents and terms, unlike the less-common word \"Sirius\". Hence, an inverse document frequency factor is incorporated which diminishes the weight of terms that occur very frequently in the document set and increases the weight of terms that occur rarely. In other words, \"Black\" is not a very specific word for our corpus. It appears in all texts very often. On the other hand, \"Sirius\" is more unique because it appears less often in texts, therefore, we can think about it as more important for our corpus. In other words, if the term \"Sirius\" appears in our text than the probability that it is a text we have been looking for is higher.\n",
    "\n",
    "In our case, we will compute tf-idf model simply according to the following equation:\n",
    "\n",
    "$$tf{-}idf_{i,j} = frequency_{i,j} \\times log_2(\\frac{D}{document\\_freq_i})$$\n",
    "\n",
    "where, $frequency_{i,j}$ is frequency of a term $i$ in document $j$; $D$ is a number of documents in the corpus; and $document\\_freq_i$ is the number of documents with the term $i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Import models from gensim\n",
    "from gensim import models\n",
    "\n",
    "## Train the model. In other words, compute\n",
    "## the tf-idf statistic for each term.\n",
    "tfidf = models.TfidfModel(bow_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, this is pretty cool but how exactly check what are the tf-idf scores for each term? Let's start by reminding ourselves of all the tokens we have and their frequency in the whole corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We simply look into frequency mapping. \n",
    "## The only thing is that we remove the elements\n",
    "## that appeared only once. \n",
    "{ key : value for key, value in frequency.items() if value > 1 }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's examine the documents' frequency. In other words, in how many documents a given term appeared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We can extract it from our model\n",
    "tfidf.dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This does not help much because the keys here are these unique ids. Let's do something more useful and convert the ids back to words. Unlike computers, it matters to us whether we have a number or a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We iterate over dictionary and take id and token and convert\n",
    "## tfidf.dfs into human-readable format.\n",
    "{ token : tfidf.dfs[id] for id, token in dictionary.items() }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we can see is that for example word \"failure\" appears 2 times in the whole corpus but only in 1 document while for example \"love\" appears 2 times but each time in a different document. In other words, the word \"love\" is not a very specific word for a document while \"failure\" is.\n",
    "\n",
    "So far so good. Let's now see idf for each word. In other words:\n",
    "\n",
    "$$idf_i = log_2(\\frac{D}{document\\_freq_i})$$\n",
    "\n",
    "**NOTE**: the lower the `idf` score the less specific the word. For example, according to the above-mentioned formula $idf_{never} = 2.0$ while $idf_{failure} = 4.32$ (it is the highest possible `idf` in this corpus). This statistic itself can be quite useful because it could allow us to find for example unique words for each document (sometimes it is very useful for example if you want to create a list of keywords)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We iterate over dictionary and take id and token and convert\n",
    "## tfidf.dfs into human-readable format.\n",
    "{ token : tfidf.idfs[id] for id, token in dictionary.items() }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's see the `tf-idf` for given terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## transform the \"first love\" string\n",
    "words = \"First times\".lower().split()\n",
    "\n",
    "print(tfidf[dictionary.doc2bow(words)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``tfidf`` model again returns a list of tuples, where the first entry is the token ID and the second entry is the tf-idf weighting. Note that the ID corresponding to \"first\" (which occurred 2 times in the original corpus) has been weighted lower than the ID corresponding to \"times\" (which also appeared twice but in the same document). In other words, `tf-idf` provides the inromation whether a given term allows to identify a document or is somehow spread across all documents. \n",
    "\n",
    "Once you've created the model, you can do all sorts of cool stuff with it. For example, to transform the whole corpus via TfIdf and index it, in preparation for similarity queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Import a similarities measure\n",
    "from gensim import similarities\n",
    "\n",
    "## Compute cosine similarity measure. Baisically, it is a cosine between\n",
    "## two vectors.\n",
    "index = similarities.SparseMatrixSimilarity(tfidf[bow_corpus], num_features=17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and to query the similarity of our query document ``query_document`` against every document in the corpus:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Provide a document to chekc the similarity\n",
    "query_document = [ lemmatizer.lemmatize(token).lower() for token in word_tokenize('Do not fear to love for the first time.') ]\n",
    "\n",
    "## Convert the query document to a Bag of words representation\n",
    "query_bow = dictionary.doc2bow(query_document)\n",
    "\n",
    "## Compute the similarities between a query_document and corpus documents\n",
    "sims = index[tfidf[query_bow]]\n",
    "\n",
    "## Print out the results\n",
    "print(list(enumerate(sims)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to read this output? Document 4 has a similarity score of $0.533039=53\\%$, document 12 has a similarity score of $46\\%$ etc. We can make this slightly more readable by sorting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Print out the results in the descending order\n",
    "for document_number, score in sorted(enumerate(sims), key=lambda x: x[1], reverse=True):\n",
    "    print(text_corpus[document_number], score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('ids')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2f6b087e3237558552441618582c1cfff7dc162872ae65c00a668bb71df085d3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
