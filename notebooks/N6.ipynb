{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling\n",
    "\n",
    "Before we start with anything fancy let's define some key concepts that will serve us in our journey with topic modelling.\n",
    "\n",
    "1. `Document` -- some text.\n",
    "2. `Corpus` -- a collection of documents (texts).\n",
    "3. `Vector` -- a mathematical representation of a document (text).\n",
    "4. `Model` -- an algorithm for transforming vectors from one representation to another.\n",
    "\n",
    "## Document\n",
    "\n",
    "A document is basically a string that might be of the length of a single tweet or [A Litle Life](https://en.wikipedia.org/wiki/A_Little_Life). In _Python_ it will look more or less to what we could have expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "document = \"Human machine interface for lab abc computer applications\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Corpus\n",
    "\n",
    "On the other hand corpus is a collection of documetns (texts). They have two roles:\n",
    "\n",
    "1. They are the input for trainig a model. This is going to be our main use case. It allows for separating common themes and topics.\n",
    "2. Documents to orgnize. After training, a topic model can be used to extract topics from new documents (documents not seen in the training corpus).\n",
    "\n",
    "At this point we can think about a corpus as a list of strings. Consider the following.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_corpus = [\n",
    "    \"Human machine interface for lab abc computer applications\",\n",
    "    \"A survey of user opinion of computer system response time\",\n",
    "    \"The EPS user interface management system\",\n",
    "    \"System and human system engineering testing of EPS\",\n",
    "    \"Relation of user perceived response time to error measurement\",\n",
    "    \"The generation of random binary unordered trees\",\n",
    "    \"The intersection graph of paths in trees\",\n",
    "    \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "    \"Graph minors A survey\",\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Important**: The above example loads the entire corpus into memory. In practice, corpora may be very large, so loading them into memory may be inconvinient (to say at least). However, for now we are leaving it as it is.\n",
    "\n",
    "Before, we move any further, we need to tokenize our corpus and also remove stop words. We may just use the same code we discussed last week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/mikolaj/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['human', 'interface', 'computer'],\n",
       " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
       " ['eps', 'user', 'interface', 'system'],\n",
       " ['system', 'human', 'system', 'eps'],\n",
       " ['user', 'response', 'time'],\n",
       " ['trees'],\n",
       " ['graph', 'trees'],\n",
       " ['graph', 'minors', 'trees'],\n",
       " ['graph', 'minors', 'survey']]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Import NLTK module\n",
    "import nltk\n",
    "## Import stop words\n",
    "from nltk.corpus import stopwords\n",
    "## Import function to tokenize text\n",
    "from nltk.tokenize import word_tokenize\n",
    "## A new type of dictionary\n",
    "from collections import defaultdict\n",
    "\n",
    "## Download stopwords list\n",
    "nltk.download('stopwords')\n",
    "\n",
    "## Assign the list of English stop words to stop_words\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "## Tokenize every single text. Meanwhile remove tokens\n",
    "## that are shorter than 2 and are in stop_words list\n",
    "texts = [ [ token for token in word_tokenize(doc.lower()) if token not in stop_words and len(token) > 1 ] \n",
    "          for doc in text_corpus ]\n",
    "\n",
    "## We define the dictionary which will not raise an error\n",
    "## when the key is not present. Instead it will create a \n",
    "## new pairing with default value of 0\n",
    "frequency = defaultdict(int)\n",
    "\n",
    "## Count the number of words all texts\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "## With a normal dictionary our code would look like \n",
    "## the following\n",
    "## frequency = {}\n",
    "## for text in texts:\n",
    "##     for token in text:\n",
    "##         if token in frequency:\n",
    "##             frequency[token] += 1\n",
    "##         else:\n",
    "##             frequency[token] = 1\n",
    "\n",
    "            \n",
    "## Remove tokens that appear only once in the whole corpus\n",
    "processed_corpus = [ [ token for token in text if frequency[token] > 1 ] \n",
    "                     for text in texts ]\n",
    "\n",
    "processed_corpus"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding, we want to associate each wrod in the corpus with a unique integer ID. We will use for that a new module `gensim`. It will allow for creating an object that will store out texts as representation of unique tokens. In other words, to save space our we will not store in the list token but rather reference to a unique token. It sounds more complicated than it actulaly is. Please consider the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary<12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...>\n"
     ]
    }
   ],
   "source": [
    "## Import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "## Create a new object called Dictionary\n",
    "dictionary = corpora.Dictionary(processed_corpus)\n",
    "\n",
    "## Let's print it out\n",
    "print(dictionary)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have here a relatively small corpus, therefore, there is only 12 different tokens. For larger corpuses, dictionaries can become massive, like over thousand of tokens.\n",
    "\n",
    "## Vector\n",
    "\n",
    "What happened so far was relatively easy. More or less we converted a set of texts into a new object which is called Dictionary (it is not the same as `dict`). Now, we would like to convert tokens we have into vectors. We need that because to ifer the latent structure in our corpus we need a way to represent documents that we cana mnaipulate mathematically. In other words, we need to transform strings that are understandable for us into something which will be understandable for a comupter. We can infer the latent structure (find similarities between texts) by understanding the meaning of the texts but computers can not really do that (don't believe people who state otherwise). Instead they can quite well and fast perform mathematical operations. Therefore, we need a way to convert a text into a mathematical object (vector) and ask computer find similiarties between set of vectors using math (you probably computed the distance between two vectors in high school).\n",
    "\n",
    "In other words, we need to find a way to represent a document (text) as a vector of features. For example, a single feature may be thought of as a question-answer pair:\n",
    "\n",
    "1. How many times does the word *splonge* appear in the document? Zero.\n",
    "2. How many paragraphs does the document consist of? Two.\n",
    "3. How many fonts does the document use? Five.\n",
    "\n",
    "If we are going to apply the same questions to every single text we can skip their text and only refer to their integer ids (syc as 1, 2, and 3). Therefore, the representation of a document becomes a series of pairs like `(1, 0), (2, 2), (3, 5)`. This is called a **dense vector**, because it contains an explicit answer to each of the above questions.\n",
    "\n",
    "In practice, we can ommit `0` values. We do that to save memory (usually we have much more features than just 3). Therefore, actually our vector would look like `(2,2), (3,5)`. This is known as a **sparse vector** or **bag-of-words vector**.\n",
    "\n",
    "Assuming the questions are the same we can compare two texts, for example texts with  the following **bag-of-words** vectors `(2, 2), (3, 5)` and `(1, .1), (2, 1.9), (3, 4.9)` must be similar, at least on the examined features. We don't have to do any heavy math to examine it.\n",
    "\n",
    "Ok, so let's have something more involving than our silly questions. Under the **bag-of-words** models, each document is represented by a vector containing the frequency count of each word in the dictionary. For example, let's assume we have a dictionary containing the following words `['coffee', 'milk', 'sugar', 'spoon']`. A document consisting of the string `\"coffee milk coffee\"` would then be represented by the vector `(1, 2), (2, 1), (3, 0), (4, 0)`, where the entries of the vector are (in order) the occurrences of \"coffee\", \"milk\", \"sugar\", and \"spoon\" in the document. The only issue here is that we kind of ignore the order of words using this approach. But is is understandable since the appraoch is called **bag-of-words**.\n",
    "\n",
    "Ok, it is all very good but how it connects with the creation of this Dictionary object? This is somehow exactly this. Our processed corpus has `12` unique tokens in it, which means that each document will be represented by a 12-dimensional vector under the bag-of-words model. We can use the dictionary to turn tokenized documents into these 12-dimensional vectors. Let's first see tehse IDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'computer': 0, 'human': 1, 'interface': 2, 'response': 3, 'survey': 4, 'system': 5, 'time': 6, 'user': 7, 'eps': 8, 'trees': 9, 'graph': 10, 'minors': 11}\n"
     ]
    }
   ],
   "source": [
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('ids')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2f6b087e3237558552441618582c1cfff7dc162872ae65c00a668bb71df085d3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
