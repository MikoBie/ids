{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling\n",
    "\n",
    "In general, topic modeling will serve us to cluster texts that are similar to one another. Why might we want this? That is because when getting data from the Internet, we will usually have quite a big number of texts. It will be very ineffective to read all of them. Instead, we might want to read just a few from each group to understand what are the different types of texts (narratives). For example, it is probably relatively easy to gather texts about the climate emergency. However, the question is in what context it is described, and whether this context has changed over the years or not.\n",
    "\n",
    "<center><img src=\"png/topic_modeling.png\" width = 600/></center>\n",
    "\n",
    "If everything goes according to the plan, under topic modeling we will discuss three models:\n",
    "\n",
    "1. `tf-idf` (term frequency - inverse document frequency) -- a very simple model that will allow us to find similarities between texts.\n",
    "2. `LDA` (Latent Dirichlet Allocation) -- model that explains a set of observations through unobserved groups, and each group explains why some parts of the data are similar.\n",
    "3. `BERTopic` -- a new approach to topic modeling that uses the latest advances in transformers-based machine learning for Natural Language Processing.\n",
    "\n",
    "However, before we start with anything fancy let's start with the basics and define some key concepts that will serve us in our journey with topic modeling.\n",
    "\n",
    "1. `Document` -- some text.\n",
    "2. `Corpus` -- a collection of documents (texts).\n",
    "3. `Vector` -- a mathematical representation of a document (text).\n",
    "4. `Model` -- an algorithm for transforming vectors from one representation to another.\n",
    "\n",
    "## Document\n",
    "\n",
    "A document is basically a string that might be the length of a single tweet or [A Litle Life](https://en.wikipedia.org/wiki/A_Little_Life). In _Python_ it will look more or less like what we could have expected. Below, we have the text of the last tweet by [The Last Week Tonight with John Oliver](https://www.youtube.com/watch?v=dQw4w9WgXcQ) account:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "document = \"Human machine interface for lab abc computer applications\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus\n",
    "\n",
    "On the other hand, a corpus is a collection of documents (texts). For us, they will play two roles:\n",
    "\n",
    "1. They are the input for training a model. This is going to be our main use case because among others it will allow for separating common themes and topics.\n",
    "2. They organize documents. After training, a topic model can be used to extract topics from new documents (documents not seen in the training corpus). In other words, using the trained model we will be able to say how similar is a new document to a group of documents from our corpus.\n",
    "\n",
    "At this point, we can think about a corpus as a list of strings. Consider the following. These are the 20 last tweets by [The Last Week Tonight with John Oliver](https://twitter.com/lastweektonight). We could have imported them to _Python_ in a more efficient way than this but at this point, this is not important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_corpus = [\n",
    "    \"Human machine interface for lab abc computer applications\",\n",
    "    \"A survey of user opinion of computer system response time\",\n",
    "    \"The EPS user interface management system\",\n",
    "    \"System and human system engineering testing of EPS\",\n",
    "    \"Relation of user perceived response time to error measurement\",\n",
    "    \"The generation of random binary unordered trees\",\n",
    "    \"The intersection graph of paths in trees\",\n",
    "    \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "    \"Graph minors A survey\",\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important**: The above example loads the entire corpus into memory. In practice, corpora may be very large, so loading them into memory may be inconvenient (to say at least). However, for now, we are leaving it as it is.\n",
    "\n",
    "Before we move any further, we need to tokenize our corpus and also remove stop words. We may just use the same code we discussed last week.\n",
    "\n",
    "Therefore, let's start with loading relevant modules and downloading stop words for English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/mikolaj/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "## Import NLTK module\n",
    "import nltk\n",
    "## Import stop words\n",
    "from nltk.corpus import stopwords\n",
    "## Import function to tokenize text\n",
    "from nltk.tokenize import word_tokenize\n",
    "## A new type of dictionary\n",
    "from collections import defaultdict\n",
    "\n",
    "## Download stopwords list\n",
    "nltk.download('stopwords')\n",
    "\n",
    "## Assign the list of English stop words to stop_words\n",
    "stop_words = stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to tokenize the texts and remove stop words. We are going to do it in one step using list comprehension because it is just the most convenient solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenize every single text. Meanwhile remove tokens\n",
    "## that are shorter than 2 and are in stop_words list\n",
    "texts = [ [ token for token in word_tokenize(doc.lower()) if token not in stop_words and len(token) > 1 ] \n",
    "          for doc in text_corpus ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a mapping with the frequency of all words in the corpus. We know how to do it but let's discover a new way of doing it. Instead of using a built-in mapping, we are going to use a more fancy one. The difference is that it does not return an error when a given key does not exist. Instead, it creates a new pairing with the given key and default value. In other words, if we define new mapping as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_mapping = defaultdict(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and try to access its non-existing key, let's say `'name'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_mapping['name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it returned `0` because we initialized it with a default value to `int` which is understood as `0`. What's more, when we inspect the mapping, we see that a new pairing appeared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {'name': 0})"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it not only did not return an error. It also created a new pairing. This is pretty cool, right? Mostly because we don't have to write any more code like the following to compute the frequency of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create an empty mapping.\n",
    "frequency = {}\n",
    "\n",
    "## Compute the frequency of tokens in the whole\n",
    "## corpus.\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        if token in frequency:\n",
    "            frequency[token] += 1\n",
    "        else:\n",
    "            frequency[token] = 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create an empty fancy mapping.\n",
    "frequency = defaultdict(int)\n",
    "\n",
    "## Count the number of words all texts\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, both methods return the same results.\n",
    "\n",
    "Ok, now let's use the frequency mapping to remove from our corpus words that appear only once. For our purposes, they are not going to be very useful because we will not be able to infer from them whether the two documents are similar. Again, we are going to use list comprehension for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['human', 'interface', 'computer'],\n",
       " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
       " ['eps', 'user', 'interface', 'system'],\n",
       " ['system', 'human', 'system', 'eps'],\n",
       " ['user', 'response', 'time'],\n",
       " ['trees'],\n",
       " ['graph', 'trees'],\n",
       " ['graph', 'minors', 'trees'],\n",
       " ['graph', 'minors', 'survey']]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Remove tokens that appear only once in the whole corpus\n",
    "processed_corpus = [ [ token for token in text if frequency[token] > 1 ] \n",
    "                     for text in texts ]\n",
    "\n",
    "processed_corpus"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding, we will associate each word in the corpus with a unique integer ID. We want to do this because for a computer it will be just easier to use integers than strings (basically a string consumes more space than an integer, therefore, the computer can operate on integers faster than on string). We will use for that a new module `gensim`. It will allow for the creation of a dictionary that will store unique tokens paired with a unique integer id. In other words, we will create a kind of mapping. We can think about it a bit like a dictionary with integers as keys and tokens as values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary<12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...>\n"
     ]
    }
   ],
   "source": [
    "## Import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "## Create a new object called Dictionary\n",
    "dictionary = corpora.Dictionary(processed_corpus)\n",
    "\n",
    "## Let's print it out\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see it is not a mapping per se. But we can easily convert it to an ordinary mapping by iteraiting over it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'computer',\n",
       " 1: 'human',\n",
       " 2: 'interface',\n",
       " 3: 'response',\n",
       " 4: 'survey',\n",
       " 5: 'system',\n",
       " 6: 'time',\n",
       " 7: 'user',\n",
       " 8: 'eps',\n",
       " 9: 'trees',\n",
       " 10: 'graph',\n",
       " 11: 'minors'}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{ key : value for key, value in dictionary.items() }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: We have here a relatively small corpus, therefore, there are only 12 different tokens. For a larger corpus, dictionaries can become massive, like over a thousand tokens. Among others, this is why it is not an ordinary dictionary but stores the data in a more efficient and memory-friendly way. The other property of this Dictionary is that we don't have to have all documents in the working memory at the same time to create it. There is an easy way to create it by processing one document at a time. However, we are not going to discuss it right now because right now we focus on the general idea.\n",
    "\n",
    "## Vector\n",
    "\n",
    "What happened so far was relatively easy. More or less we converted a set of texts into a new object which is called Dictionary (it is not the same as `dict` or a mapping). Now, we would like to convert the tokens we have into vectors. We need that because to infer the latent structure in our corpus we need a way to represent documents in a way that allows for mathematical manipulation. In other words, we need to transform strings that are understandable for us into something which will be understandable for a computer. We can infer the latent structure (find similarities between texts) by understanding the meaning of the texts but computers can not really do that (don't believe people who state otherwise). Instead, they can quite well and fast perform mathematical operations. Therefore, we need a way to convert a text into a mathematical object (vector) and ask the computer to find similarities between a set of vectors using math (you probably computed the distance between two vectors in high school).\n",
    "\n",
    "In other words, we need to find a way to represent a document (text) as a vector of features. For example, a single feature may be thought of as a question-answer pair:\n",
    "\n",
    "1. How many times does the word *sponge* appear in the document? Zero.\n",
    "2. How many paragraphs does the document consist of? Two.\n",
    "3. How many fonts does the document use? Five.\n",
    "\n",
    "If we are going to apply the same questions to every single text we can skip their text and only refer to their integer ids (such as `1`, `2`, and `3`). Therefore, the representation of a document becomes a series of pairs like `(1, 0), (2, 2), (3, 5)`. This is called a **dense vector**, because it contains an explicit answer to each of the above questions.\n",
    "\n",
    "In practice, we can omit `0` values. We do that to save memory (usually we have much more features than just `3`). Therefore, actually, our vector would look like `(2,2), (3,5)`. This is known as a **sparse vector** or **bag-of-words vector**.\n",
    "\n",
    "Assuming the questions are the same we can compare two texts, for example, texts with the following **bag-of-words** vectors `(2, 2), (3, 5)` and `(1, 1), (2, 2), (3, 5)`. Based on the answers to our questions we can infer that the two texts must be similar. We don't really have to employ heavy maths to say that. The only difference between these two texts is the appearance of [sponge](https://www.youtube.com/watch?v=-SuSWX8h-hg). They must be similar, at least on the examined features. We don't have to do any heavy math to examine it.\n",
    "\n",
    "Ok, so let's have something more involving than our silly questions. Usually, under the **bag-of-words** models, each document is represented by a vector containing the frequency count of each word in the dictionary. For example, let's assume we have a dictionary containing the following words `['sponge', 'square', 'pants', 'bob']`. A document consisting of the string `\"sponge pants sponge\"` would then be represented by the vector `(1, 2), (2, 0), (3, 1), (4, 0)`, where the entries of the vector are (in order) the occurrences of \"sponge\", \"square\", \"pants\", and \"bob\" in the document. The only issue here is that we kind of ignored the order of words. But let's don't worry about it right now. Also, as the name of this method suggests it is a **bag-of-words** approach so it is understandable that the order does not matter too much.\n",
    "\n",
    "Ok, it is all very good but how it connects with the creation of this Dictionary object? This is somehow exactly this. Our processed corpus has `12` unique tokens in it, which means that each document will be represented by a 12-dimensional vector under the **bag-of-words model**. We can use the dictionary to turn tokenized documents into these 12-dimensional vectors. Let's see these ids again using the `token2id` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'computer': 0,\n",
       " 'human': 1,\n",
       " 'interface': 2,\n",
       " 'response': 3,\n",
       " 'survey': 4,\n",
       " 'system': 5,\n",
       " 'time': 6,\n",
       " 'user': 7,\n",
       " 'eps': 8,\n",
       " 'trees': 9,\n",
       " 'graph': 10,\n",
       " 'minors': 11}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.token2id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now, when we want to vectorize a phrase we will just use the above dictionary as a kind of cipher. For example, suppose we want to vectorize the phrase \"Human computer interaction\" (note that this phrase was not in our original corpus). We can create the **bag-of-word** representation for a document using the `doc2bow()` method of the dictionary, which returns a sparse representation of the word counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1), (1, 1)]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_doc = \"Human computer interaction\"\n",
    "new_vec = dictionary.doc2bow(word_tokenize(new_doc.lower()))\n",
    "new_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first entry in each tuple corresponds to the ID of the token in the dictionary, the second corresponds to the count of this token.\n",
    "\n",
    "Note that \"interaction\" did not occur in the original corpus and so it was not included in the vectorization. Also, note that this vector only contains entries for words that actually appeared in the document. Because any given document will only contain a few words out of the many words in the dictionary, words that do not appear in the vectorization are represented as implicitly zero as a space-saving measure.\n",
    "\n",
    "We can convert our entire original corpus to a list of vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1)],\n",
       " [(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)],\n",
       " [(2, 1), (5, 1), (7, 1), (8, 1)],\n",
       " [(1, 1), (5, 2), (8, 1)],\n",
       " [(3, 1), (6, 1), (7, 1)],\n",
       " [(9, 1)],\n",
       " [(9, 1), (10, 1)],\n",
       " [(9, 1), (10, 1), (11, 1)],\n",
       " [(4, 1), (10, 1), (11, 1)]]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus = [ dictionary.doc2bow(text) for text in processed_corpus ]\n",
    "bow_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: In our specific case we can have the whole list in the working memory but with more (and longer) documents we will not want to load all the elements at the same time. Luckily, `gensim` allows you to use any iterator that returns a single document vector at a time but we are not going to talk about it right now.\n",
    "\n",
    "**IMPORTANT**: The distinction between a document and a vector is that the former is text, and the latter is a mathematically convenient representation of the text. Moreover, depending on how the representation was obtained, two different documents may have the same vector representations.\n",
    "\n",
    "\n",
    "## Model\n",
    "\n",
    "Before, we talk about more involving models. Let's start with the simplest transformation. So far, we have vectorized our corpus. Now, we can transform the **bag-of-words** representation into something more interesting.\n",
    "\n",
    "One simple example of a model (transformation) is `tf-idf` (term frequency - inverse document frequency). In simple words, it returns a statistic that allows saying how important a given term is to a document in the corpus, for example how important Sirius Black is to the Harry Potter books in the collection of Harry Potter books. Therefore, this model allows for transforming vectors from the **bag-of-words** representation to a vector space where the frequency counts are weighted according to the relative rarity of each word in the corpus. Why it is useful? That is because this statistic lays the foundations for the relevance of the search.\n",
    "\n",
    "Imagine that you have a set of Harry Potter books and you would like to order them by the relevance to the following query: \"Sirius Black\". A simple way to start out is by eliminating books that do not contain all two words \"Sirius Black\", but this still leaves many books (probably 7). To further distinguish them, we might count the number of times each term occurs in each document; the number of times a term occurs in a document is called its term frequency. However, the problem often arises when the texts differ in terms of length (and if you read Harry Potter books you probably noticed that each book was longer than the previous one). Therefore, the term frequency is simply the number of occurence of a given word ($i$) in a given document ($j$):\n",
    "\n",
    "$$frequency_{i,j}$$\n",
    "\n",
    "Because the term \"Black\" is so common, term frequency will tend to incorrectly emphasize documents that happen to use the word \"Black\" more frequently, without giving enough weight to the more meaningful term \"Sirius\". The term \"Black\" is not a good keyword to distinguish relevant and non-relevant documents and terms, unlike the less-common word \"Sirius\". Hence, an inverse document frequency factor is incorporated which diminishes the weight of terms that occur very frequently in the document set and increases the weight of terms that occur rarely. In other words, \"Black\" is not a very specific word for our corpus. It appears in all texts very often. On the other hand, \"Sirius\" is more unique because it appears less often in texts, therefore, we can think about it as more important for our corpus. In other words, if the term \"Sirius\" appears in our text than the probability that it is a text we have been looking for is higher.\n",
    "\n",
    "In our case, we will compute tf-idf model simply according to the following equation:\n",
    "\n",
    "$$tf{-}idf_{i,j} = frequency_{i,j} \\times log_2(\\frac{D}{document\\_freq_i})$$\n",
    "\n",
    "where, $frequency_{i,j}$ is frequency of a term $i$ in document $j$; $D$ is a number of documents in the corpus; and $document\\_freq_i$ is the number of documents with the term $i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Import models from gensim\n",
    "from gensim import models\n",
    "\n",
    "## Train the model. In other words, compute\n",
    "## the tf-idf statistic for each term.\n",
    "tfidf = models.TfidfModel(bow_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, this is pretty cool but how exactly check what are the tf-idf scores for each term? Let's start by reminding ourselves of all the tokens we have and their frequency in the whole corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'human': 2,\n",
       " 'interface': 2,\n",
       " 'computer': 2,\n",
       " 'survey': 2,\n",
       " 'user': 3,\n",
       " 'system': 4,\n",
       " 'response': 2,\n",
       " 'time': 2,\n",
       " 'eps': 2,\n",
       " 'trees': 3,\n",
       " 'graph': 3,\n",
       " 'minors': 2}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## We simply look into frequency mapping. \n",
    "## The only thing is that we remove the elements\n",
    "## that appeared only once. \n",
    "{ key : value for key, value in frequency.items() if value > 1 }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's examine the documents' frequency. In other words, in how many documents a given term appeared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 2, 1: 2, 2: 2, 3: 2, 4: 2, 5: 3, 6: 2, 7: 3, 8: 2, 9: 3, 10: 3, 11: 2}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## We can extract it form our model\n",
    "tfidf.dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This does not help much because the keys here are these unique ids. Let's do something more useful and convert the ids back to words. Unlike computers, it matters to us whether we have a number or a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'computer': 2,\n",
       " 'human': 2,\n",
       " 'interface': 2,\n",
       " 'response': 2,\n",
       " 'survey': 2,\n",
       " 'system': 3,\n",
       " 'time': 2,\n",
       " 'user': 3,\n",
       " 'eps': 2,\n",
       " 'trees': 3,\n",
       " 'graph': 3,\n",
       " 'minors': 2}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## We iterate over dictionary and take id and token and convert\n",
    "## tfidf.dfs into human-readable format.\n",
    "{ token : tfidf.dfs[id] for id, token in dictionary.items() }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we can see is that for example word \"system\" appears 4 times in the whole corpus but only in 3 documents. Other words appear in every document they appear only once.\n",
    "\n",
    "So far so good. Let's now see idf for each word. In other words:\n",
    "\n",
    "$$idf_i = log_2(\\frac{D}{document\\_freq_i})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'computer': 2.1699250014423126,\n",
       " 'human': 2.1699250014423126,\n",
       " 'interface': 2.1699250014423126,\n",
       " 'response': 2.1699250014423126,\n",
       " 'survey': 2.1699250014423126,\n",
       " 'system': 1.5849625007211563,\n",
       " 'time': 2.1699250014423126,\n",
       " 'user': 1.5849625007211563,\n",
       " 'eps': 2.1699250014423126,\n",
       " 'trees': 1.5849625007211563,\n",
       " 'graph': 1.5849625007211563,\n",
       " 'minors': 2.1699250014423126}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## We iterate over dictionary and take id and token and convert\n",
    "## tfidf.dfs into human-readable format.\n",
    "{ token : tfidf.idfs[id] for id, token in dictionary.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__abstractmethods__', '__class__', '__class_getitem__', '__contains__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__reversed__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_adapt_by_suffix', '_load_specials', '_save_specials', '_smart_save', 'add_documents', 'add_lifecycle_event', 'cfs', 'compactify', 'dfs', 'doc2bow', 'doc2idx', 'filter_extremes', 'filter_n_most_frequent', 'filter_tokens', 'from_corpus', 'from_documents', 'get', 'id2token', 'items', 'iteritems', 'iterkeys', 'itervalues', 'keys', 'lifecycle_events', 'load', 'load_from_text', 'merge_with', 'most_common', 'num_docs', 'num_nnz', 'num_pos', 'patch_with_special_tokens', 'save', 'save_as_text', 'token2id', 'values']\n"
     ]
    }
   ],
   "source": [
    "print(dir(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 2, 2: 2, 0: 2, 4: 2, 7: 3, 5: 4, 3: 2, 6: 2, 8: 2, 9: 3, 10: 3, 11: 2}"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.cfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/mikolaj/Desktop/ids/notebooks/N6.ipynb Komórka 40\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mikolaj/Desktop/ids/notebooks/N6.ipynb#Y125sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m## We iterate over dictionary and take id and token and convert\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mikolaj/Desktop/ids/notebooks/N6.ipynb#Y125sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m## tfidf.dfs into human-readable format.\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/mikolaj/Desktop/ids/notebooks/N6.ipynb#Y125sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m { token : tfidf[\u001b[39mid\u001b[39m] \u001b[39mfor\u001b[39;00m \u001b[39mid\u001b[39m, token \u001b[39min\u001b[39;00m dictionary\u001b[39m.\u001b[39mitems() }\n",
      "\u001b[1;32m/Users/mikolaj/Desktop/ids/notebooks/N6.ipynb Komórka 40\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mikolaj/Desktop/ids/notebooks/N6.ipynb#Y125sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m## We iterate over dictionary and take id and token and convert\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mikolaj/Desktop/ids/notebooks/N6.ipynb#Y125sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m## tfidf.dfs into human-readable format.\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/mikolaj/Desktop/ids/notebooks/N6.ipynb#Y125sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m { token : tfidf[\u001b[39mid\u001b[39;49m] \u001b[39mfor\u001b[39;00m \u001b[39mid\u001b[39m, token \u001b[39min\u001b[39;00m dictionary\u001b[39m.\u001b[39mitems() }\n",
      "File \u001b[0;32m~/anaconda3/envs/ids/lib/python3.10/site-packages/gensim/models/tfidfmodel.py:495\u001b[0m, in \u001b[0;36mTfidfModel.__getitem__\u001b[0;34m(self, bow, eps)\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[39m# unknown (new) terms will be given zero weight (NOT infinity/huge weight,\u001b[39;00m\n\u001b[1;32m    492\u001b[0m \u001b[39m# as strict application of the IDF formula would dictate)\u001b[39;00m\n\u001b[1;32m    494\u001b[0m termid_array, tf_array \u001b[39m=\u001b[39m [], []\n\u001b[0;32m--> 495\u001b[0m \u001b[39mfor\u001b[39;00m termid, tf \u001b[39min\u001b[39;00m bow:\n\u001b[1;32m    496\u001b[0m     termid_array\u001b[39m.\u001b[39mappend(termid)\n\u001b[1;32m    497\u001b[0m     tf_array\u001b[39m.\u001b[39mappend(tf)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "## We iterate over dictionary and take id and token and convert\n",
    "## tfidf.dfs into human-readable format.\n",
    "{ token : tfidf[(] for id, token in dictionary.items() }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's see tf-idf statistics for all the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1)],\n",
       " [(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)],\n",
       " [(2, 1), (5, 1), (7, 1), (8, 1)],\n",
       " [(1, 1), (5, 2), (8, 1)],\n",
       " [(3, 1), (6, 1), (7, 1)],\n",
       " [(9, 1)],\n",
       " [(9, 1), (10, 1)],\n",
       " [(9, 1), (10, 1), (11, 1)],\n",
       " [(4, 1), (10, 1), (11, 1)]]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'human': 2,\n",
       " 'interface': 2,\n",
       " 'computer': 2,\n",
       " 'survey': 2,\n",
       " 'user': 3,\n",
       " 'system': 4,\n",
       " 'response': 2,\n",
       " 'time': 2,\n",
       " 'eps': 2,\n",
       " 'trees': 3,\n",
       " 'graph': 3,\n",
       " 'minors': 2}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{ key : value for key, value in frequency.items() if value > 1 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5, 1), (11, 1)]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = word_tokenize(\"system minors\".lower())\n",
    "\n",
    "words_bow = dictionary.doc2bow(words)\n",
    "\n",
    "words_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on dict object:\n",
      "\n",
      "class dict(object)\n",
      " |  dict() -> new empty dictionary\n",
      " |  dict(mapping) -> new dictionary initialized from a mapping object's\n",
      " |      (key, value) pairs\n",
      " |  dict(iterable) -> new dictionary initialized as if via:\n",
      " |      d = {}\n",
      " |      for k, v in iterable:\n",
      " |          d[k] = v\n",
      " |  dict(**kwargs) -> new dictionary initialized with the name=value pairs\n",
      " |      in the keyword argument list.  For example:  dict(one=1, two=2)\n",
      " |  \n",
      " |  Built-in subclasses:\n",
      " |      StgDict\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __contains__(self, key, /)\n",
      " |      True if the dictionary has the specified key, else False.\n",
      " |  \n",
      " |  __delitem__(self, key, /)\n",
      " |      Delete self[key].\n",
      " |  \n",
      " |  __eq__(self, value, /)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __ge__(self, value, /)\n",
      " |      Return self>=value.\n",
      " |  \n",
      " |  __getattribute__(self, name, /)\n",
      " |      Return getattr(self, name).\n",
      " |  \n",
      " |  __getitem__(...)\n",
      " |      x.__getitem__(y) <==> x[y]\n",
      " |  \n",
      " |  __gt__(self, value, /)\n",
      " |      Return self>value.\n",
      " |  \n",
      " |  __init__(self, /, *args, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __ior__(self, value, /)\n",
      " |      Return self|=value.\n",
      " |  \n",
      " |  __iter__(self, /)\n",
      " |      Implement iter(self).\n",
      " |  \n",
      " |  __le__(self, value, /)\n",
      " |      Return self<=value.\n",
      " |  \n",
      " |  __len__(self, /)\n",
      " |      Return len(self).\n",
      " |  \n",
      " |  __lt__(self, value, /)\n",
      " |      Return self<value.\n",
      " |  \n",
      " |  __ne__(self, value, /)\n",
      " |      Return self!=value.\n",
      " |  \n",
      " |  __or__(self, value, /)\n",
      " |      Return self|value.\n",
      " |  \n",
      " |  __repr__(self, /)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __reversed__(self, /)\n",
      " |      Return a reverse iterator over the dict keys.\n",
      " |  \n",
      " |  __ror__(self, value, /)\n",
      " |      Return value|self.\n",
      " |  \n",
      " |  __setitem__(self, key, value, /)\n",
      " |      Set self[key] to value.\n",
      " |  \n",
      " |  __sizeof__(...)\n",
      " |      D.__sizeof__() -> size of D in memory, in bytes\n",
      " |  \n",
      " |  clear(...)\n",
      " |      D.clear() -> None.  Remove all items from D.\n",
      " |  \n",
      " |  copy(...)\n",
      " |      D.copy() -> a shallow copy of D\n",
      " |  \n",
      " |  get(self, key, default=None, /)\n",
      " |      Return the value for key if key is in the dictionary, else default.\n",
      " |  \n",
      " |  items(...)\n",
      " |      D.items() -> a set-like object providing a view on D's items\n",
      " |  \n",
      " |  keys(...)\n",
      " |      D.keys() -> a set-like object providing a view on D's keys\n",
      " |  \n",
      " |  pop(...)\n",
      " |      D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n",
      " |      \n",
      " |      If the key is not found, return the default if given; otherwise,\n",
      " |      raise a KeyError.\n",
      " |  \n",
      " |  popitem(self, /)\n",
      " |      Remove and return a (key, value) pair as a 2-tuple.\n",
      " |      \n",
      " |      Pairs are returned in LIFO (last-in, first-out) order.\n",
      " |      Raises KeyError if the dict is empty.\n",
      " |  \n",
      " |  setdefault(self, key, default=None, /)\n",
      " |      Insert key with a value of default if key is not in the dictionary.\n",
      " |      \n",
      " |      Return the value for key if key is in the dictionary, else default.\n",
      " |  \n",
      " |  update(...)\n",
      " |      D.update([E, ]**F) -> None.  Update D from dict/iterable E and F.\n",
      " |      If E is present and has a .keys() method, then does:  for k in E: D[k] = E[k]\n",
      " |      If E is present and lacks a .keys() method, then does:  for k, v in E: D[k] = v\n",
      " |      In either case, this is followed by: for k in F:  D[k] = F[k]\n",
      " |  \n",
      " |  values(...)\n",
      " |      D.values() -> an object providing a view on D's values\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  __class_getitem__(...) from builtins.type\n",
      " |      See PEP 585\n",
      " |  \n",
      " |  fromkeys(iterable, value=None, /) from builtins.type\n",
      " |      Create a new dictionary with keys from iterable and values set to value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  __new__(*args, **kwargs) from builtins.type\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __hash__ = None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tfidf.dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5, 1), (11, 1)]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('ids')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2f6b087e3237558552441618582c1cfff7dc162872ae65c00a668bb71df085d3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
